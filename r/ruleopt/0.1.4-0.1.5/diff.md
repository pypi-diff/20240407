# Comparing `tmp/ruleopt-0.1.4-cp39-cp39-win_amd64.whl.zip` & `tmp/ruleopt-0.1.5-cp39-cp39-macosx_11_0_arm64.whl.zip`

## zipinfo {}

```diff
@@ -1,31 +1,40 @@
-Zip file size: 310472 bytes, number of entries: 29
--rw-rw-rw-  2.0 fat      257 b- defN 24-Apr-07 11:33 ruleopt/__init__.py
--rw-rw-rw-  2.0 fat       96 b- defN 24-Apr-07 11:33 ruleopt/aux_classes/__init__.py
--rw-rw-rw-  2.0 fat  1401393 b- defN 24-Apr-07 11:34 ruleopt/aux_classes/aux_classes.c
--rw-rw-rw-  2.0 fat   211968 b- defN 24-Apr-07 11:34 ruleopt/aux_classes/aux_classes.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat      280 b- defN 24-Apr-07 11:33 ruleopt/estimator/__init__.py
--rw-rw-rw-  2.0 fat    26086 b- defN 24-Apr-07 11:33 ruleopt/estimator/base.py
--rw-rw-rw-  2.0 fat    12714 b- defN 24-Apr-07 11:33 ruleopt/estimator/lightgbm_.py
--rw-rw-rw-  2.0 fat    11558 b- defN 24-Apr-07 11:33 ruleopt/estimator/xgboost_.py
--rw-rw-rw-  2.0 fat      126 b- defN 24-Apr-07 11:33 ruleopt/estimator/sklearn_/__init__.py
--rw-rw-rw-  2.0 fat     7001 b- defN 24-Apr-07 11:33 ruleopt/estimator/sklearn_/base_sklearn.py
--rw-rw-rw-  2.0 fat    18249 b- defN 24-Apr-07 11:33 ruleopt/estimator/sklearn_/rug.py
--rw-rw-rw-  2.0 fat     5662 b- defN 24-Apr-07 11:33 ruleopt/estimator/sklearn_/rux.py
--rw-rw-rw-  2.0 fat       68 b- defN 24-Apr-07 11:33 ruleopt/explainer/__init__.py
--rw-rw-rw-  2.0 fat     9014 b- defN 24-Apr-07 11:33 ruleopt/explainer/explainer.py
--rw-rw-rw-  2.0 fat      137 b- defN 24-Apr-07 11:33 ruleopt/rule_cost/__init__.py
--rw-rw-rw-  2.0 fat     5252 b- defN 24-Apr-07 11:33 ruleopt/rule_cost/rule_cost.py
--rw-rw-rw-  2.0 fat      293 b- defN 24-Apr-07 11:33 ruleopt/solver/__init__.py
--rw-rw-rw-  2.0 fat     3208 b- defN 24-Apr-07 11:33 ruleopt/solver/base.py
--rw-rw-rw-  2.0 fat     4086 b- defN 24-Apr-07 11:33 ruleopt/solver/cplex_solver.py
--rw-rw-rw-  2.0 fat     3875 b- defN 24-Apr-07 11:33 ruleopt/solver/gurobi_solver.py
--rw-rw-rw-  2.0 fat     5120 b- defN 24-Apr-07 11:33 ruleopt/solver/ortools_solver.py
--rw-rw-rw-  2.0 fat    10425 b- defN 24-Apr-07 11:33 ruleopt/solver/unc_solver.py
--rw-rw-rw-  2.0 fat      124 b- defN 24-Apr-07 11:33 ruleopt/utils/__init__.py
--rw-rw-rw-  2.0 fat     1086 b- defN 24-Apr-07 11:33 ruleopt/utils/utils.py
--rw-rw-rw-  2.0 fat     1135 b- defN 24-Apr-07 11:34 ruleopt-0.1.4.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     4153 b- defN 24-Apr-07 11:34 ruleopt-0.1.4.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 24-Apr-07 11:34 ruleopt-0.1.4.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 24-Apr-07 11:34 ruleopt-0.1.4.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2480 b- defN 24-Apr-07 11:34 ruleopt-0.1.4.dist-info/RECORD
-29 files, 1745954 bytes uncompressed, 306476 bytes compressed:  82.4%
+Zip file size: 317428 bytes, number of entries: 38
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-07 15:56 ruleopt/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-07 15:56 ruleopt-0.1.5.dist-info/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-07 15:56 ruleopt/estimator/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-07 15:56 ruleopt/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-07 15:56 ruleopt/solver/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-07 15:56 ruleopt/rule_cost/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-07 15:56 ruleopt/explainer/
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-07 15:56 ruleopt/aux_classes/
+-rw-r--r--  2.0 unx      247 b- defN 24-Apr-07 15:55 ruleopt/__init__.py
+drwxr-xr-x  2.0 unx        0 b- stor 24-Apr-07 15:56 ruleopt/estimator/sklearn_/
+-rw-r--r--  2.0 unx      271 b- defN 24-Apr-07 15:55 ruleopt/estimator/__init__.py
+-rw-r--r--  2.0 unx    12388 b- defN 24-Apr-07 15:55 ruleopt/estimator/lightgbm_.py
+-rw-r--r--  2.0 unx    11253 b- defN 24-Apr-07 15:55 ruleopt/estimator/xgboost_.py
+-rw-r--r--  2.0 unx    23998 b- defN 24-Apr-07 15:55 ruleopt/estimator/base.py
+-rw-r--r--  2.0 unx     6805 b- defN 24-Apr-07 15:55 ruleopt/estimator/sklearn_/base_sklearn.py
+-rw-r--r--  2.0 unx     5514 b- defN 24-Apr-07 15:55 ruleopt/estimator/sklearn_/rux.py
+-rw-r--r--  2.0 unx      119 b- defN 24-Apr-07 15:55 ruleopt/estimator/sklearn_/__init__.py
+-rw-r--r--  2.0 unx    17777 b- defN 24-Apr-07 15:55 ruleopt/estimator/sklearn_/rug.py
+-rw-r--r--  2.0 unx      120 b- defN 24-Apr-07 15:55 ruleopt/utils/__init__.py
+-rw-r--r--  2.0 unx     1053 b- defN 24-Apr-07 15:55 ruleopt/utils/utils.py
+-rw-r--r--  2.0 unx    10141 b- defN 24-Apr-07 15:55 ruleopt/solver/unc_solver.py
+-rw-r--r--  2.0 unx     4975 b- defN 24-Apr-07 15:55 ruleopt/solver/ortools_solver.py
+-rw-r--r--  2.0 unx      286 b- defN 24-Apr-07 15:55 ruleopt/solver/__init__.py
+-rw-r--r--  2.0 unx     3756 b- defN 24-Apr-07 15:55 ruleopt/solver/gurobi_solver.py
+-rw-r--r--  2.0 unx     3957 b- defN 24-Apr-07 15:55 ruleopt/solver/cplex_solver.py
+-rw-r--r--  2.0 unx     3126 b- defN 24-Apr-07 15:55 ruleopt/solver/base.py
+-rw-r--r--  2.0 unx     5079 b- defN 24-Apr-07 15:55 ruleopt/rule_cost/rule_cost.py
+-rw-r--r--  2.0 unx      134 b- defN 24-Apr-07 15:55 ruleopt/rule_cost/__init__.py
+-rw-r--r--  2.0 unx     8771 b- defN 24-Apr-07 15:55 ruleopt/explainer/explainer.py
+-rw-r--r--  2.0 unx       65 b- defN 24-Apr-07 15:55 ruleopt/explainer/__init__.py
+-rw-r--r--  2.0 unx       91 b- defN 24-Apr-07 15:55 ruleopt/aux_classes/__init__.py
+-rwxr-xr-x  2.0 unx   266544 b- defN 24-Apr-07 15:56 ruleopt/aux_classes/aux_classes.cpython-39-darwin.so
+-rw-r--r--  2.0 unx  1424395 b- defN 24-Apr-07 15:56 ruleopt/aux_classes/aux_classes.c
+-rw-rw-r--  2.0 unx     2511 b- defN 24-Apr-07 15:56 ruleopt-0.1.5.dist-info/RECORD
+-rw-r--r--  2.0 unx     1112 b- defN 24-Apr-07 15:56 ruleopt-0.1.5.dist-info/LICENSE
+-rw-r--r--  2.0 unx      108 b- defN 24-Apr-07 15:56 ruleopt-0.1.5.dist-info/WHEEL
+-rw-r--r--  2.0 unx        8 b- defN 24-Apr-07 15:56 ruleopt-0.1.5.dist-info/top_level.txt
+-rw-r--r--  2.0 unx     4060 b- defN 24-Apr-07 15:56 ruleopt-0.1.5.dist-info/METADATA
+38 files, 1818664 bytes uncompressed, 312420 bytes compressed:  82.8%
```

## zipnote {}

```diff
@@ -1,88 +1,115 @@
-Filename: ruleopt/__init__.py
+Filename: ruleopt/
 Comment: 
 
-Filename: ruleopt/aux_classes/__init__.py
+Filename: ruleopt-0.1.5.dist-info/
 Comment: 
 
-Filename: ruleopt/aux_classes/aux_classes.c
+Filename: ruleopt/estimator/
 Comment: 
 
-Filename: ruleopt/aux_classes/aux_classes.cp39-win_amd64.pyd
+Filename: ruleopt/utils/
 Comment: 
 
-Filename: ruleopt/estimator/__init__.py
+Filename: ruleopt/solver/
 Comment: 
 
-Filename: ruleopt/estimator/base.py
+Filename: ruleopt/rule_cost/
+Comment: 
+
+Filename: ruleopt/explainer/
+Comment: 
+
+Filename: ruleopt/aux_classes/
+Comment: 
+
+Filename: ruleopt/__init__.py
+Comment: 
+
+Filename: ruleopt/estimator/sklearn_/
+Comment: 
+
+Filename: ruleopt/estimator/__init__.py
 Comment: 
 
 Filename: ruleopt/estimator/lightgbm_.py
 Comment: 
 
 Filename: ruleopt/estimator/xgboost_.py
 Comment: 
 
-Filename: ruleopt/estimator/sklearn_/__init__.py
+Filename: ruleopt/estimator/base.py
 Comment: 
 
 Filename: ruleopt/estimator/sklearn_/base_sklearn.py
 Comment: 
 
-Filename: ruleopt/estimator/sklearn_/rug.py
+Filename: ruleopt/estimator/sklearn_/rux.py
 Comment: 
 
-Filename: ruleopt/estimator/sklearn_/rux.py
+Filename: ruleopt/estimator/sklearn_/__init__.py
 Comment: 
 
-Filename: ruleopt/explainer/__init__.py
+Filename: ruleopt/estimator/sklearn_/rug.py
 Comment: 
 
-Filename: ruleopt/explainer/explainer.py
+Filename: ruleopt/utils/__init__.py
 Comment: 
 
-Filename: ruleopt/rule_cost/__init__.py
+Filename: ruleopt/utils/utils.py
 Comment: 
 
-Filename: ruleopt/rule_cost/rule_cost.py
+Filename: ruleopt/solver/unc_solver.py
+Comment: 
+
+Filename: ruleopt/solver/ortools_solver.py
 Comment: 
 
 Filename: ruleopt/solver/__init__.py
 Comment: 
 
-Filename: ruleopt/solver/base.py
+Filename: ruleopt/solver/gurobi_solver.py
 Comment: 
 
 Filename: ruleopt/solver/cplex_solver.py
 Comment: 
 
-Filename: ruleopt/solver/gurobi_solver.py
+Filename: ruleopt/solver/base.py
 Comment: 
 
-Filename: ruleopt/solver/ortools_solver.py
+Filename: ruleopt/rule_cost/rule_cost.py
 Comment: 
 
-Filename: ruleopt/solver/unc_solver.py
+Filename: ruleopt/rule_cost/__init__.py
 Comment: 
 
-Filename: ruleopt/utils/__init__.py
+Filename: ruleopt/explainer/explainer.py
 Comment: 
 
-Filename: ruleopt/utils/utils.py
+Filename: ruleopt/explainer/__init__.py
+Comment: 
+
+Filename: ruleopt/aux_classes/__init__.py
+Comment: 
+
+Filename: ruleopt/aux_classes/aux_classes.cpython-39-darwin.so
+Comment: 
+
+Filename: ruleopt/aux_classes/aux_classes.c
 Comment: 
 
-Filename: ruleopt-0.1.4.dist-info/LICENSE
+Filename: ruleopt-0.1.5.dist-info/RECORD
 Comment: 
 
-Filename: ruleopt-0.1.4.dist-info/METADATA
+Filename: ruleopt-0.1.5.dist-info/LICENSE
 Comment: 
 
-Filename: ruleopt-0.1.4.dist-info/WHEEL
+Filename: ruleopt-0.1.5.dist-info/WHEEL
 Comment: 
 
-Filename: ruleopt-0.1.4.dist-info/top_level.txt
+Filename: ruleopt-0.1.5.dist-info/top_level.txt
 Comment: 
 
-Filename: ruleopt-0.1.4.dist-info/RECORD
+Filename: ruleopt-0.1.5.dist-info/METADATA
 Comment: 
 
 Zip file comment:
```

## filetype from file(1)

```diff
@@ -1 +1 @@
-Zip archive data, at least v2.0 to extract, compression method=deflate
+Zip archive data, at least v2.0 to extract, compression method=store
```

## ruleopt/__init__.py

 * *Ordering differences only*

```diff
@@ -1,10 +1,10 @@
-from .estimator import (RUGClassifier, RUXClassifier, RUXLGBMClassifier, RUXXGBClassifier)
-from .explainer import Explainer
-
-__all__ = [
-    "RUGClassifier",
-    "RUXClassifier",
-    "RUXLGBMClassifier",
-    "RUXXGBClassifier",
-    "Explainer",]
-
+from .estimator import (RUGClassifier, RUXClassifier, RUXLGBMClassifier, RUXXGBClassifier)
+from .explainer import Explainer
+
+__all__ = [
+    "RUGClassifier",
+    "RUXClassifier",
+    "RUXLGBMClassifier",
+    "RUXXGBClassifier",
+    "Explainer",]
+
```

## ruleopt/aux_classes/__init__.py

 * *Ordering differences only*

```diff
@@ -1,6 +1,6 @@
-from .aux_classes import (Rule, Coefficients)
-
-
-__all__ = [
-    "Rule",
+from .aux_classes import (Rule, Coefficients)
+
+
+__all__ = [
+    "Rule",
     "Coefficients"]
```

## ruleopt/aux_classes/aux_classes.c

```diff
@@ -1,27 +1,26 @@
 /* Generated by Cython 3.0.10 */
 
 /* BEGIN: Cython Metadata
 {
     "distutils": {
         "depends": [
-            "C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-fqu8v5xm\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h",
-            "C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-fqu8v5xm\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h",
-            "C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-fqu8v5xm\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h",
-            "C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-fqu8v5xm\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h",
-            "C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-fqu8v5xm\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h"
+            "/private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/core/include/numpy/arrayobject.h",
+            "/private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/core/include/numpy/arrayscalars.h",
+            "/private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/core/include/numpy/ndarrayobject.h",
+            "/private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/core/include/numpy/ndarraytypes.h",
+            "/private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/core/include/numpy/ufuncobject.h"
         ],
         "extra_compile_args": [
-            "/O2",
-            "/favor:ATOM",
-            "/fp:fast",
+            "-Ofast",
+            "-funroll-loops",
             "-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION"
         ],
         "include_dirs": [
-            "C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-build-env-fqu8v5xm\\overlay\\Lib\\site-packages\\numpy\\core\\include"
+            "/private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/core/include"
         ],
         "language": "c",
         "name": "ruleopt.aux_classes.aux_classes",
         "sources": [
             "ruleopt/aux_classes/aux_classes.pyx"
         ]
     },
@@ -1526,18 +1525,18 @@
   #undef _Complex_I
   #define _Complex_I 1.0fj
 #endif
 
 /* #### Code section: filename_table ### */
 
 static const char *__pyx_f[] = {
-  "ruleopt\\\\aux_classes\\\\aux_classes.pyx",
+  "ruleopt/aux_classes/aux_classes.pyx",
   "<stringsource>",
   "__init__.cython-30.pxd",
-  "ruleopt\\\\aux_classes\\\\aux_classes.pxd",
+  "ruleopt/aux_classes/aux_classes.pxd",
   "type.pxd",
 };
 /* #### Code section: utility_code_proto_before_types ### */
 /* ForceInitThreads.proto */
 #ifndef __PYX_FORCE_INIT_THREADS
   #define __PYX_FORCE_INIT_THREADS 0
 #endif
@@ -1676,177 +1675,177 @@
   Py_ssize_t strides[8];
   Py_ssize_t suboffsets[8];
 } __Pyx_memviewslice;
 #define __Pyx_MemoryView_Len(m)  (m.shape[0])
 
 /* #### Code section: numeric_typedefs ### */
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":730
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":730
  * # in Cython to enable them only on the right systems.
  * 
  * ctypedef npy_int8       int8_t             # <<<<<<<<<<<<<<
  * ctypedef npy_int16      int16_t
  * ctypedef npy_int32      int32_t
  */
 typedef npy_int8 __pyx_t_5numpy_int8_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":731
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":731
  * 
  * ctypedef npy_int8       int8_t
  * ctypedef npy_int16      int16_t             # <<<<<<<<<<<<<<
  * ctypedef npy_int32      int32_t
  * ctypedef npy_int64      int64_t
  */
 typedef npy_int16 __pyx_t_5numpy_int16_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":732
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":732
  * ctypedef npy_int8       int8_t
  * ctypedef npy_int16      int16_t
  * ctypedef npy_int32      int32_t             # <<<<<<<<<<<<<<
  * ctypedef npy_int64      int64_t
  * #ctypedef npy_int96      int96_t
  */
 typedef npy_int32 __pyx_t_5numpy_int32_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":733
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":733
  * ctypedef npy_int16      int16_t
  * ctypedef npy_int32      int32_t
  * ctypedef npy_int64      int64_t             # <<<<<<<<<<<<<<
  * #ctypedef npy_int96      int96_t
  * #ctypedef npy_int128     int128_t
  */
 typedef npy_int64 __pyx_t_5numpy_int64_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":737
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":737
  * #ctypedef npy_int128     int128_t
  * 
  * ctypedef npy_uint8      uint8_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uint16     uint16_t
  * ctypedef npy_uint32     uint32_t
  */
 typedef npy_uint8 __pyx_t_5numpy_uint8_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":738
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":738
  * 
  * ctypedef npy_uint8      uint8_t
  * ctypedef npy_uint16     uint16_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uint32     uint32_t
  * ctypedef npy_uint64     uint64_t
  */
 typedef npy_uint16 __pyx_t_5numpy_uint16_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":739
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":739
  * ctypedef npy_uint8      uint8_t
  * ctypedef npy_uint16     uint16_t
  * ctypedef npy_uint32     uint32_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uint64     uint64_t
  * #ctypedef npy_uint96     uint96_t
  */
 typedef npy_uint32 __pyx_t_5numpy_uint32_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":740
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":740
  * ctypedef npy_uint16     uint16_t
  * ctypedef npy_uint32     uint32_t
  * ctypedef npy_uint64     uint64_t             # <<<<<<<<<<<<<<
  * #ctypedef npy_uint96     uint96_t
  * #ctypedef npy_uint128    uint128_t
  */
 typedef npy_uint64 __pyx_t_5numpy_uint64_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":744
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":744
  * #ctypedef npy_uint128    uint128_t
  * 
  * ctypedef npy_float32    float32_t             # <<<<<<<<<<<<<<
  * ctypedef npy_float64    float64_t
  * #ctypedef npy_float80    float80_t
  */
 typedef npy_float32 __pyx_t_5numpy_float32_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":745
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":745
  * 
  * ctypedef npy_float32    float32_t
  * ctypedef npy_float64    float64_t             # <<<<<<<<<<<<<<
  * #ctypedef npy_float80    float80_t
  * #ctypedef npy_float128   float128_t
  */
 typedef npy_float64 __pyx_t_5numpy_float64_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":754
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":754
  * # The int types are mapped a bit surprising --
  * # numpy.int corresponds to 'l' and numpy.long to 'q'
  * ctypedef npy_long       int_t             # <<<<<<<<<<<<<<
  * ctypedef npy_longlong   longlong_t
  * 
  */
 typedef npy_long __pyx_t_5numpy_int_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":755
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":755
  * # numpy.int corresponds to 'l' and numpy.long to 'q'
  * ctypedef npy_long       int_t
  * ctypedef npy_longlong   longlong_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_ulong      uint_t
  */
 typedef npy_longlong __pyx_t_5numpy_longlong_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":757
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":757
  * ctypedef npy_longlong   longlong_t
  * 
  * ctypedef npy_ulong      uint_t             # <<<<<<<<<<<<<<
  * ctypedef npy_ulonglong  ulonglong_t
  * 
  */
 typedef npy_ulong __pyx_t_5numpy_uint_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":758
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":758
  * 
  * ctypedef npy_ulong      uint_t
  * ctypedef npy_ulonglong  ulonglong_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_intp       intp_t
  */
 typedef npy_ulonglong __pyx_t_5numpy_ulonglong_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":760
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":760
  * ctypedef npy_ulonglong  ulonglong_t
  * 
  * ctypedef npy_intp       intp_t             # <<<<<<<<<<<<<<
  * ctypedef npy_uintp      uintp_t
  * 
  */
 typedef npy_intp __pyx_t_5numpy_intp_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":761
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":761
  * 
  * ctypedef npy_intp       intp_t
  * ctypedef npy_uintp      uintp_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_double     float_t
  */
 typedef npy_uintp __pyx_t_5numpy_uintp_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":763
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":763
  * ctypedef npy_uintp      uintp_t
  * 
  * ctypedef npy_double     float_t             # <<<<<<<<<<<<<<
  * ctypedef npy_double     double_t
  * ctypedef npy_longdouble longdouble_t
  */
 typedef npy_double __pyx_t_5numpy_float_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":764
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":764
  * 
  * ctypedef npy_double     float_t
  * ctypedef npy_double     double_t             # <<<<<<<<<<<<<<
  * ctypedef npy_longdouble longdouble_t
  * 
  */
 typedef npy_double __pyx_t_5numpy_double_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":765
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":765
  * ctypedef npy_double     float_t
  * ctypedef npy_double     double_t
  * ctypedef npy_longdouble longdouble_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_cfloat      cfloat_t
  */
 typedef npy_longdouble __pyx_t_5numpy_longdouble_t;
@@ -1908,42 +1907,42 @@
 struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Coefficients;
 struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule;
 struct __pyx_array_obj;
 struct __pyx_MemviewEnum_obj;
 struct __pyx_memoryview_obj;
 struct __pyx_memoryviewslice_obj;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":767
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":767
  * ctypedef npy_longdouble longdouble_t
  * 
  * ctypedef npy_cfloat      cfloat_t             # <<<<<<<<<<<<<<
  * ctypedef npy_cdouble     cdouble_t
  * ctypedef npy_clongdouble clongdouble_t
  */
 typedef npy_cfloat __pyx_t_5numpy_cfloat_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":768
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":768
  * 
  * ctypedef npy_cfloat      cfloat_t
  * ctypedef npy_cdouble     cdouble_t             # <<<<<<<<<<<<<<
  * ctypedef npy_clongdouble clongdouble_t
  * 
  */
 typedef npy_cdouble __pyx_t_5numpy_cdouble_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":769
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":769
  * ctypedef npy_cfloat      cfloat_t
  * ctypedef npy_cdouble     cdouble_t
  * ctypedef npy_clongdouble clongdouble_t             # <<<<<<<<<<<<<<
  * 
  * ctypedef npy_cdouble     complex_t
  */
 typedef npy_clongdouble __pyx_t_5numpy_clongdouble_t;
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":771
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":771
  * ctypedef npy_clongdouble clongdouble_t
  * 
  * ctypedef npy_cdouble     complex_t             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew1(a):
  */
 typedef npy_cdouble __pyx_t_5numpy_complex_t;
@@ -2081,17 +2080,17 @@
  *     def __cinit__(self):
  *         """
  */
 
 struct __pyx_vtabstruct_7ruleopt_11aux_classes_11aux_classes_Rule {
   PyObject *(*add_clause)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, int, double, double, int, int __pyx_skip_dispatch);
   PyObject *(*_get_clause)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, int, int __pyx_skip_dispatch);
-  int (*_check_rule_nogil)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, __Pyx_memviewslice);
+  __Pyx_memviewslice (*_check_rule_nogil)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, __Pyx_memviewslice);
   int (*_check_clause_nogil)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, __Pyx_memviewslice, int);
-  int (*check_rule)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, __Pyx_memviewslice, int __pyx_skip_dispatch);
+  PyArrayObject *(*check_rule)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, __Pyx_memviewslice, int __pyx_skip_dispatch);
 };
 static struct __pyx_vtabstruct_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_vtabptr_7ruleopt_11aux_classes_11aux_classes_Rule;
 
 
 /* "View.MemoryView":114
  * @cython.collection_type("sequence")
  * @cname("__pyx_array")
@@ -2725,14 +2724,25 @@
 /* HasAttr.proto */
 #if __PYX_LIMITED_VERSION_HEX >= 0x030d00A1
 #define __Pyx_HasAttr(o, n)  PyObject_HasAttrWithError(o, n)
 #else
 static CYTHON_INLINE int __Pyx_HasAttr(PyObject *, PyObject *);
 #endif
 
+/* BufferIndexError.proto */
+static void __Pyx_RaiseBufferIndexError(int axis);
+
+/* BufferIndexErrorNogil.proto */
+static void __Pyx_RaiseBufferIndexErrorNogil(int axis);
+
+/* WriteUnraisableException.proto */
+static void __Pyx_WriteUnraisable(const char *name, int clineno,
+                                  int lineno, const char *filename,
+                                  int full_traceback, int nogil);
+
 /* PyObjectFormat.proto */
 #if CYTHON_USE_UNICODE_WRITER
 static PyObject* __Pyx_PyObject_Format(PyObject* s, PyObject* f);
 #else
 #define __Pyx_PyObject_Format(s, f) PyObject_Format(s, f)
 #endif
 
@@ -3081,20 +3091,27 @@
 static CYTHON_INLINE PyObject *__pyx_memview_get_nn___pyx_t_7ruleopt_11aux_classes_11aux_classes_DOUBLE_t(const char *itemp);
 static CYTHON_INLINE int __pyx_memview_set_nn___pyx_t_7ruleopt_11aux_classes_11aux_classes_DOUBLE_t(const char *itemp, PyObject *obj);
 
 /* ObjectToMemviewSlice.proto */
 static CYTHON_INLINE __Pyx_memviewslice __Pyx_PyObject_to_MemoryviewSlice_ds_nn___pyx_t_7ruleopt_11aux_classes_11aux_classes_DOUBLE_t(PyObject *, int writable_flag);
 
 /* ObjectToMemviewSlice.proto */
-static CYTHON_INLINE __Pyx_memviewslice __Pyx_PyObject_to_MemoryviewSlice_ds_float(PyObject *, int writable_flag);
+static CYTHON_INLINE __Pyx_memviewslice __Pyx_PyObject_to_MemoryviewSlice_dsds_float(PyObject *, int writable_flag);
+
+/* ObjectToMemviewSlice.proto */
+static CYTHON_INLINE __Pyx_memviewslice __Pyx_PyObject_to_MemoryviewSlice_ds_char(PyObject *, int writable_flag);
 
 /* MemviewDtypeToObject.proto */
 static CYTHON_INLINE PyObject *__pyx_memview_get_float(const char *itemp);
 static CYTHON_INLINE int __pyx_memview_set_float(const char *itemp, PyObject *obj);
 
+/* MemviewDtypeToObject.proto */
+static CYTHON_INLINE PyObject *__pyx_memview_get_char(const char *itemp);
+static CYTHON_INLINE int __pyx_memview_set_char(const char *itemp, PyObject *obj);
+
 /* RealImag.proto */
 #if CYTHON_CCOMPLEX
   #ifdef __cplusplus
     #define __Pyx_CREAL(z) ((z).real())
     #define __Pyx_CIMAG(z) ((z).imag())
   #else
     #define __Pyx_CREAL(z) (__real__(z))
@@ -3233,19 +3250,22 @@
 /* CIntFromPy.proto */
 static CYTHON_INLINE long __Pyx_PyInt_As_long(PyObject *);
 
 /* CIntToPy.proto */
 static CYTHON_INLINE PyObject* __Pyx_PyInt_From_int(int value);
 
 /* CIntToPy.proto */
-static CYTHON_INLINE PyObject* __Pyx_PyInt_From_long(long value);
+static CYTHON_INLINE PyObject* __Pyx_PyInt_From_char(char value);
 
 /* CIntFromPy.proto */
 static CYTHON_INLINE char __Pyx_PyInt_As_char(PyObject *);
 
+/* CIntToPy.proto */
+static CYTHON_INLINE PyObject* __Pyx_PyInt_From_long(long value);
+
 /* FormatTypeName.proto */
 #if CYTHON_COMPILING_IN_LIMITED_API
 typedef PyObject *__Pyx_TypeName;
 #define __Pyx_FMT_TYPENAME "%U"
 static __Pyx_TypeName __Pyx_PyType_GetName(PyTypeObject* tp);
 #define __Pyx_DECREF_TypeName(obj) Py_XDECREF(obj)
 #else
@@ -3280,17 +3300,17 @@
 static CYTHON_INLINE int __pyx_f_5numpy_7ndarray_4ndim_ndim(PyArrayObject *__pyx_v_self); /* proto*/
 static CYTHON_INLINE npy_intp *__pyx_f_5numpy_7ndarray_5shape_shape(PyArrayObject *__pyx_v_self); /* proto*/
 static CYTHON_INLINE npy_intp *__pyx_f_5numpy_7ndarray_7strides_strides(PyArrayObject *__pyx_v_self); /* proto*/
 static CYTHON_INLINE npy_intp __pyx_f_5numpy_7ndarray_4size_size(PyArrayObject *__pyx_v_self); /* proto*/
 static CYTHON_INLINE char *__pyx_f_5numpy_7ndarray_4data_data(PyArrayObject *__pyx_v_self); /* proto*/
 static PyObject *__pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule__get_clause(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_v_self, int __pyx_v_index, int __pyx_skip_dispatch); /* proto*/
 static PyObject *__pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule_add_clause(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_v_self, int __pyx_v_feature, double __pyx_v_ub, double __pyx_v_lb, int __pyx_v_na, int __pyx_skip_dispatch); /* proto*/
-static int __pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule__check_rule_nogil(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_v_self, __Pyx_memviewslice __pyx_v_X); /* proto*/
+static __Pyx_memviewslice __pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule__check_rule_nogil(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_v_self, __Pyx_memviewslice __pyx_v_X); /* proto*/
 static int __pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule__check_clause_nogil(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_v_self, __Pyx_memviewslice __pyx_v_X, int __pyx_v_idx); /* proto*/
-static int __pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule_check_rule(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_v_self, __Pyx_memviewslice __pyx_v_X, int __pyx_skip_dispatch); /* proto*/
+static PyArrayObject *__pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule_check_rule(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_v_self, __Pyx_memviewslice __pyx_v_X, int __pyx_skip_dispatch); /* proto*/
 
 /* Module declarations from "libc.string" */
 
 /* Module declarations from "libc.stdio" */
 
 /* Module declarations from "__builtin__" */
 
@@ -3352,14 +3372,15 @@
 static void __pyx_memoryview_slice_assign_scalar(__Pyx_memviewslice *, int, size_t, void *, int); /*proto*/
 static void __pyx_memoryview__slice_assign_scalar(char *, Py_ssize_t *, Py_ssize_t *, int, size_t, void *); /*proto*/
 static PyObject *__pyx_unpickle_Enum__set_state(struct __pyx_MemviewEnum_obj *, PyObject *); /*proto*/
 /* #### Code section: typeinfo ### */
 static __Pyx_TypeInfo __Pyx_TypeInfo_nn___pyx_t_7ruleopt_11aux_classes_11aux_classes_SIZE_t = { "SIZE_t", NULL, sizeof(__pyx_t_7ruleopt_11aux_classes_11aux_classes_SIZE_t), { 0 }, 0, __PYX_IS_UNSIGNED(__pyx_t_7ruleopt_11aux_classes_11aux_classes_SIZE_t) ? 'U' : 'I', __PYX_IS_UNSIGNED(__pyx_t_7ruleopt_11aux_classes_11aux_classes_SIZE_t), 0 };
 static __Pyx_TypeInfo __Pyx_TypeInfo_nn___pyx_t_7ruleopt_11aux_classes_11aux_classes_DOUBLE_t = { "DOUBLE_t", NULL, sizeof(__pyx_t_7ruleopt_11aux_classes_11aux_classes_DOUBLE_t), { 0 }, 0, 'R', 0, 0 };
 static __Pyx_TypeInfo __Pyx_TypeInfo_float = { "float", NULL, sizeof(float), { 0 }, 0, 'R', 0, 0 };
+static __Pyx_TypeInfo __Pyx_TypeInfo_char = { "char", NULL, sizeof(char), { 0 }, 0, 'H', __PYX_IS_UNSIGNED(char), 0 };
 /* #### Code section: before_global_var ### */
 #define __Pyx_MODULE_NAME "ruleopt.aux_classes.aux_classes"
 extern int __pyx_module_is_main_ruleopt__aux_classes__aux_classes;
 int __pyx_module_is_main_ruleopt__aux_classes__aux_classes = 0;
 
 /* Implementation of "ruleopt.aux_classes.aux_classes" */
 /* #### Code section: global_var ### */
@@ -3406,14 +3427,15 @@
 static const char __pyx_k_sys[] = "sys";
 static const char __pyx_k_9_2f[] = "<9.2f";
 static const char __pyx_k_None[] = "None";
 static const char __pyx_k_Rule[] = "Rule";
 static const char __pyx_k_base[] = "base";
 static const char __pyx_k_cols[] = "cols";
 static const char __pyx_k_dict[] = "__dict__";
+static const char __pyx_k_int8[] = "int8";
 static const char __pyx_k_intp[] = "intp";
 static const char __pyx_k_main[] = "__main__";
 static const char __pyx_k_mode[] = "mode";
 static const char __pyx_k_name[] = "name";
 static const char __pyx_k_ndim[] = "ndim";
 static const char __pyx_k_pack[] = "pack";
 static const char __pyx_k_rows[] = "rows";
@@ -3433,28 +3455,30 @@
 static const char __pyx_k_flags[] = "flags";
 static const char __pyx_k_index[] = "index";
 static const char __pyx_k_numpy[] = "numpy";
 static const char __pyx_k_range[] = "range";
 static const char __pyx_k_shape[] = "shape";
 static const char __pyx_k_start[] = "start";
 static const char __pyx_k_state[] = "state";
+static const char __pyx_k_uint8[] = "uint8";
 static const char __pyx_k_yvals[] = "yvals";
 static const char __pyx_k_DOUBLE[] = "DOUBLE";
 static const char __pyx_k_dict_2[] = "_dict";
 static const char __pyx_k_enable[] = "enable";
 static const char __pyx_k_encode[] = "encode";
 static const char __pyx_k_format[] = "format";
 static const char __pyx_k_import[] = "__import__";
 static const char __pyx_k_name_2[] = "__name__";
 static const char __pyx_k_pickle[] = "pickle";
 static const char __pyx_k_reduce[] = "__reduce__";
 static const char __pyx_k_rstrip[] = "rstrip";
 static const char __pyx_k_struct[] = "struct";
 static const char __pyx_k_unpack[] = "unpack";
 static const char __pyx_k_update[] = "update";
+static const char __pyx_k_asarray[] = "asarray";
 static const char __pyx_k_cleanup[] = "cleanup";
 static const char __pyx_k_disable[] = "disable";
 static const char __pyx_k_feature[] = "feature";
 static const char __pyx_k_float64[] = "float64";
 static const char __pyx_k_fortran[] = "fortran";
 static const char __pyx_k_memview[] = "memview";
 static const char __pyx_k_or_null[] = " or null";
@@ -3530,15 +3554,15 @@
 static const char __pyx_k_Step_may_not_be_zero_axis_d[] = "Step may not be zero (axis %d)";
 static const char __pyx_k_itemsize_0_for_cython_array[] = "itemsize <= 0 for cython.array";
 static const char __pyx_k_Coefficients___reduce_cython[] = "Coefficients.__reduce_cython__";
 static const char __pyx_k_unable_to_allocate_array_data[] = "unable to allocate array data.";
 static const char __pyx_k_Coefficients___setstate_cython[] = "Coefficients.__setstate_cython__";
 static const char __pyx_k_strided_and_direct_or_indirect[] = "<strided and direct or indirect>";
 static const char __pyx_k_numpy_core_multiarray_failed_to[] = "numpy.core.multiarray failed to import";
-static const char __pyx_k_ruleopt_aux_classes_aux_classes[] = "ruleopt\\aux_classes\\aux_classes.pyx";
+static const char __pyx_k_ruleopt_aux_classes_aux_classes[] = "ruleopt/aux_classes/aux_classes.pyx";
 static const char __pyx_k_All_dimensions_preceding_dimensi[] = "All dimensions preceding dimension %d must be indexed and not sliced";
 static const char __pyx_k_Buffer_view_does_not_expose_stri[] = "Buffer view does not expose strides";
 static const char __pyx_k_Can_only_create_a_buffer_that_is[] = "Can only create a buffer that is contiguous in memory.";
 static const char __pyx_k_Cannot_assign_to_read_only_memor[] = "Cannot assign to read-only memoryview";
 static const char __pyx_k_Cannot_create_writable_memory_vi[] = "Cannot create writable memory view from read-only memoryview";
 static const char __pyx_k_Cannot_transpose_memoryview_with[] = "Cannot transpose memoryview with indirect dimensions";
 static const char __pyx_k_Empty_shape_tuple_for_cython_arr[] = "Empty shape tuple for cython.array";
@@ -3775,14 +3799,15 @@
   PyObject *__pyx_kp_u__6;
   PyObject *__pyx_kp_u__7;
   PyObject *__pyx_n_s_abc;
   PyObject *__pyx_n_s_add_clause;
   PyObject *__pyx_n_s_allocate_buffer;
   PyObject *__pyx_kp_u_and;
   PyObject *__pyx_kp_u_and_not_null;
+  PyObject *__pyx_n_s_asarray;
   PyObject *__pyx_n_s_asyncio_coroutines;
   PyObject *__pyx_n_s_base;
   PyObject *__pyx_n_s_c;
   PyObject *__pyx_n_u_c;
   PyObject *__pyx_n_s_check_rule;
   PyObject *__pyx_n_s_class;
   PyObject *__pyx_n_s_class_getitem;
@@ -3819,14 +3844,15 @@
   PyObject *__pyx_kp_u_got;
   PyObject *__pyx_kp_u_got_differing_extents_in_dimensi;
   PyObject *__pyx_n_s_i;
   PyObject *__pyx_n_s_id;
   PyObject *__pyx_n_s_import;
   PyObject *__pyx_n_s_index;
   PyObject *__pyx_n_s_initializing;
+  PyObject *__pyx_n_s_int8;
   PyObject *__pyx_n_s_intp;
   PyObject *__pyx_n_s_is_coroutine;
   PyObject *__pyx_kp_u_isenabled;
   PyObject *__pyx_n_s_itemsize;
   PyObject *__pyx_kp_s_itemsize_0_for_cython_array;
   PyObject *__pyx_n_s_lb;
   PyObject *__pyx_n_u_lb;
@@ -3885,14 +3911,15 @@
   PyObject *__pyx_n_s_struct;
   PyObject *__pyx_n_s_sys;
   PyObject *__pyx_n_s_test;
   PyObject *__pyx_n_s_to_dict;
   PyObject *__pyx_n_s_to_text;
   PyObject *__pyx_n_s_ub;
   PyObject *__pyx_n_u_ub;
+  PyObject *__pyx_n_s_uint8;
   PyObject *__pyx_kp_s_unable_to_allocate_array_data;
   PyObject *__pyx_kp_s_unable_to_allocate_shape_and_str;
   PyObject *__pyx_n_s_unpack;
   PyObject *__pyx_n_s_update;
   PyObject *__pyx_n_s_use_setstate;
   PyObject *__pyx_n_s_version_info;
   PyObject *__pyx_kp_u_x;
@@ -4079,14 +4106,15 @@
   Py_CLEAR(clear_module_state->__pyx_kp_u__6);
   Py_CLEAR(clear_module_state->__pyx_kp_u__7);
   Py_CLEAR(clear_module_state->__pyx_n_s_abc);
   Py_CLEAR(clear_module_state->__pyx_n_s_add_clause);
   Py_CLEAR(clear_module_state->__pyx_n_s_allocate_buffer);
   Py_CLEAR(clear_module_state->__pyx_kp_u_and);
   Py_CLEAR(clear_module_state->__pyx_kp_u_and_not_null);
+  Py_CLEAR(clear_module_state->__pyx_n_s_asarray);
   Py_CLEAR(clear_module_state->__pyx_n_s_asyncio_coroutines);
   Py_CLEAR(clear_module_state->__pyx_n_s_base);
   Py_CLEAR(clear_module_state->__pyx_n_s_c);
   Py_CLEAR(clear_module_state->__pyx_n_u_c);
   Py_CLEAR(clear_module_state->__pyx_n_s_check_rule);
   Py_CLEAR(clear_module_state->__pyx_n_s_class);
   Py_CLEAR(clear_module_state->__pyx_n_s_class_getitem);
@@ -4123,14 +4151,15 @@
   Py_CLEAR(clear_module_state->__pyx_kp_u_got);
   Py_CLEAR(clear_module_state->__pyx_kp_u_got_differing_extents_in_dimensi);
   Py_CLEAR(clear_module_state->__pyx_n_s_i);
   Py_CLEAR(clear_module_state->__pyx_n_s_id);
   Py_CLEAR(clear_module_state->__pyx_n_s_import);
   Py_CLEAR(clear_module_state->__pyx_n_s_index);
   Py_CLEAR(clear_module_state->__pyx_n_s_initializing);
+  Py_CLEAR(clear_module_state->__pyx_n_s_int8);
   Py_CLEAR(clear_module_state->__pyx_n_s_intp);
   Py_CLEAR(clear_module_state->__pyx_n_s_is_coroutine);
   Py_CLEAR(clear_module_state->__pyx_kp_u_isenabled);
   Py_CLEAR(clear_module_state->__pyx_n_s_itemsize);
   Py_CLEAR(clear_module_state->__pyx_kp_s_itemsize_0_for_cython_array);
   Py_CLEAR(clear_module_state->__pyx_n_s_lb);
   Py_CLEAR(clear_module_state->__pyx_n_u_lb);
@@ -4189,14 +4218,15 @@
   Py_CLEAR(clear_module_state->__pyx_n_s_struct);
   Py_CLEAR(clear_module_state->__pyx_n_s_sys);
   Py_CLEAR(clear_module_state->__pyx_n_s_test);
   Py_CLEAR(clear_module_state->__pyx_n_s_to_dict);
   Py_CLEAR(clear_module_state->__pyx_n_s_to_text);
   Py_CLEAR(clear_module_state->__pyx_n_s_ub);
   Py_CLEAR(clear_module_state->__pyx_n_u_ub);
+  Py_CLEAR(clear_module_state->__pyx_n_s_uint8);
   Py_CLEAR(clear_module_state->__pyx_kp_s_unable_to_allocate_array_data);
   Py_CLEAR(clear_module_state->__pyx_kp_s_unable_to_allocate_shape_and_str);
   Py_CLEAR(clear_module_state->__pyx_n_s_unpack);
   Py_CLEAR(clear_module_state->__pyx_n_s_update);
   Py_CLEAR(clear_module_state->__pyx_n_s_use_setstate);
   Py_CLEAR(clear_module_state->__pyx_n_s_version_info);
   Py_CLEAR(clear_module_state->__pyx_kp_u_x);
@@ -4361,14 +4391,15 @@
   Py_VISIT(traverse_module_state->__pyx_kp_u__6);
   Py_VISIT(traverse_module_state->__pyx_kp_u__7);
   Py_VISIT(traverse_module_state->__pyx_n_s_abc);
   Py_VISIT(traverse_module_state->__pyx_n_s_add_clause);
   Py_VISIT(traverse_module_state->__pyx_n_s_allocate_buffer);
   Py_VISIT(traverse_module_state->__pyx_kp_u_and);
   Py_VISIT(traverse_module_state->__pyx_kp_u_and_not_null);
+  Py_VISIT(traverse_module_state->__pyx_n_s_asarray);
   Py_VISIT(traverse_module_state->__pyx_n_s_asyncio_coroutines);
   Py_VISIT(traverse_module_state->__pyx_n_s_base);
   Py_VISIT(traverse_module_state->__pyx_n_s_c);
   Py_VISIT(traverse_module_state->__pyx_n_u_c);
   Py_VISIT(traverse_module_state->__pyx_n_s_check_rule);
   Py_VISIT(traverse_module_state->__pyx_n_s_class);
   Py_VISIT(traverse_module_state->__pyx_n_s_class_getitem);
@@ -4405,14 +4436,15 @@
   Py_VISIT(traverse_module_state->__pyx_kp_u_got);
   Py_VISIT(traverse_module_state->__pyx_kp_u_got_differing_extents_in_dimensi);
   Py_VISIT(traverse_module_state->__pyx_n_s_i);
   Py_VISIT(traverse_module_state->__pyx_n_s_id);
   Py_VISIT(traverse_module_state->__pyx_n_s_import);
   Py_VISIT(traverse_module_state->__pyx_n_s_index);
   Py_VISIT(traverse_module_state->__pyx_n_s_initializing);
+  Py_VISIT(traverse_module_state->__pyx_n_s_int8);
   Py_VISIT(traverse_module_state->__pyx_n_s_intp);
   Py_VISIT(traverse_module_state->__pyx_n_s_is_coroutine);
   Py_VISIT(traverse_module_state->__pyx_kp_u_isenabled);
   Py_VISIT(traverse_module_state->__pyx_n_s_itemsize);
   Py_VISIT(traverse_module_state->__pyx_kp_s_itemsize_0_for_cython_array);
   Py_VISIT(traverse_module_state->__pyx_n_s_lb);
   Py_VISIT(traverse_module_state->__pyx_n_u_lb);
@@ -4471,14 +4503,15 @@
   Py_VISIT(traverse_module_state->__pyx_n_s_struct);
   Py_VISIT(traverse_module_state->__pyx_n_s_sys);
   Py_VISIT(traverse_module_state->__pyx_n_s_test);
   Py_VISIT(traverse_module_state->__pyx_n_s_to_dict);
   Py_VISIT(traverse_module_state->__pyx_n_s_to_text);
   Py_VISIT(traverse_module_state->__pyx_n_s_ub);
   Py_VISIT(traverse_module_state->__pyx_n_u_ub);
+  Py_VISIT(traverse_module_state->__pyx_n_s_uint8);
   Py_VISIT(traverse_module_state->__pyx_kp_s_unable_to_allocate_array_data);
   Py_VISIT(traverse_module_state->__pyx_kp_s_unable_to_allocate_shape_and_str);
   Py_VISIT(traverse_module_state->__pyx_n_s_unpack);
   Py_VISIT(traverse_module_state->__pyx_n_s_update);
   Py_VISIT(traverse_module_state->__pyx_n_s_use_setstate);
   Py_VISIT(traverse_module_state->__pyx_n_s_version_info);
   Py_VISIT(traverse_module_state->__pyx_kp_u_x);
@@ -4673,14 +4706,15 @@
 #define __pyx_kp_u__6 __pyx_mstate_global->__pyx_kp_u__6
 #define __pyx_kp_u__7 __pyx_mstate_global->__pyx_kp_u__7
 #define __pyx_n_s_abc __pyx_mstate_global->__pyx_n_s_abc
 #define __pyx_n_s_add_clause __pyx_mstate_global->__pyx_n_s_add_clause
 #define __pyx_n_s_allocate_buffer __pyx_mstate_global->__pyx_n_s_allocate_buffer
 #define __pyx_kp_u_and __pyx_mstate_global->__pyx_kp_u_and
 #define __pyx_kp_u_and_not_null __pyx_mstate_global->__pyx_kp_u_and_not_null
+#define __pyx_n_s_asarray __pyx_mstate_global->__pyx_n_s_asarray
 #define __pyx_n_s_asyncio_coroutines __pyx_mstate_global->__pyx_n_s_asyncio_coroutines
 #define __pyx_n_s_base __pyx_mstate_global->__pyx_n_s_base
 #define __pyx_n_s_c __pyx_mstate_global->__pyx_n_s_c
 #define __pyx_n_u_c __pyx_mstate_global->__pyx_n_u_c
 #define __pyx_n_s_check_rule __pyx_mstate_global->__pyx_n_s_check_rule
 #define __pyx_n_s_class __pyx_mstate_global->__pyx_n_s_class
 #define __pyx_n_s_class_getitem __pyx_mstate_global->__pyx_n_s_class_getitem
@@ -4717,14 +4751,15 @@
 #define __pyx_kp_u_got __pyx_mstate_global->__pyx_kp_u_got
 #define __pyx_kp_u_got_differing_extents_in_dimensi __pyx_mstate_global->__pyx_kp_u_got_differing_extents_in_dimensi
 #define __pyx_n_s_i __pyx_mstate_global->__pyx_n_s_i
 #define __pyx_n_s_id __pyx_mstate_global->__pyx_n_s_id
 #define __pyx_n_s_import __pyx_mstate_global->__pyx_n_s_import
 #define __pyx_n_s_index __pyx_mstate_global->__pyx_n_s_index
 #define __pyx_n_s_initializing __pyx_mstate_global->__pyx_n_s_initializing
+#define __pyx_n_s_int8 __pyx_mstate_global->__pyx_n_s_int8
 #define __pyx_n_s_intp __pyx_mstate_global->__pyx_n_s_intp
 #define __pyx_n_s_is_coroutine __pyx_mstate_global->__pyx_n_s_is_coroutine
 #define __pyx_kp_u_isenabled __pyx_mstate_global->__pyx_kp_u_isenabled
 #define __pyx_n_s_itemsize __pyx_mstate_global->__pyx_n_s_itemsize
 #define __pyx_kp_s_itemsize_0_for_cython_array __pyx_mstate_global->__pyx_kp_s_itemsize_0_for_cython_array
 #define __pyx_n_s_lb __pyx_mstate_global->__pyx_n_s_lb
 #define __pyx_n_u_lb __pyx_mstate_global->__pyx_n_u_lb
@@ -4783,14 +4818,15 @@
 #define __pyx_n_s_struct __pyx_mstate_global->__pyx_n_s_struct
 #define __pyx_n_s_sys __pyx_mstate_global->__pyx_n_s_sys
 #define __pyx_n_s_test __pyx_mstate_global->__pyx_n_s_test
 #define __pyx_n_s_to_dict __pyx_mstate_global->__pyx_n_s_to_dict
 #define __pyx_n_s_to_text __pyx_mstate_global->__pyx_n_s_to_text
 #define __pyx_n_s_ub __pyx_mstate_global->__pyx_n_s_ub
 #define __pyx_n_u_ub __pyx_mstate_global->__pyx_n_u_ub
+#define __pyx_n_s_uint8 __pyx_mstate_global->__pyx_n_s_uint8
 #define __pyx_kp_s_unable_to_allocate_array_data __pyx_mstate_global->__pyx_kp_s_unable_to_allocate_array_data
 #define __pyx_kp_s_unable_to_allocate_shape_and_str __pyx_mstate_global->__pyx_kp_s_unable_to_allocate_shape_and_str
 #define __pyx_n_s_unpack __pyx_mstate_global->__pyx_n_s_unpack
 #define __pyx_n_s_update __pyx_mstate_global->__pyx_n_s_update
 #define __pyx_n_s_use_setstate __pyx_mstate_global->__pyx_n_s_use_setstate
 #define __pyx_n_s_version_info __pyx_mstate_global->__pyx_n_s_version_info
 #define __pyx_kp_u_x __pyx_mstate_global->__pyx_kp_u_x
@@ -18470,261 +18506,261 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":245
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":245
  * 
  *         @property
  *         cdef inline PyObject* base(self) nogil:             # <<<<<<<<<<<<<<
  *             """Returns a borrowed reference to the object owning the data/memory.
  *             """
  */
 
 static CYTHON_INLINE PyObject *__pyx_f_5numpy_7ndarray_4base_base(PyArrayObject *__pyx_v_self) {
   PyObject *__pyx_r;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":248
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":248
  *             """Returns a borrowed reference to the object owning the data/memory.
  *             """
  *             return PyArray_BASE(self)             # <<<<<<<<<<<<<<
  * 
  *         @property
  */
   __pyx_r = PyArray_BASE(__pyx_v_self);
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":245
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":245
  * 
  *         @property
  *         cdef inline PyObject* base(self) nogil:             # <<<<<<<<<<<<<<
  *             """Returns a borrowed reference to the object owning the data/memory.
  *             """
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":251
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":251
  * 
  *         @property
  *         cdef inline dtype descr(self):             # <<<<<<<<<<<<<<
  *             """Returns an owned reference to the dtype of the array.
  *             """
  */
 
 static CYTHON_INLINE PyArray_Descr *__pyx_f_5numpy_7ndarray_5descr_descr(PyArrayObject *__pyx_v_self) {
   PyArray_Descr *__pyx_r = NULL;
   __Pyx_RefNannyDeclarations
   PyArray_Descr *__pyx_t_1;
   __Pyx_RefNannySetupContext("descr", 1);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":254
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":254
  *             """Returns an owned reference to the dtype of the array.
  *             """
  *             return <dtype>PyArray_DESCR(self)             # <<<<<<<<<<<<<<
  * 
  *         @property
  */
   __Pyx_XDECREF((PyObject *)__pyx_r);
   __pyx_t_1 = PyArray_DESCR(__pyx_v_self);
   __Pyx_INCREF((PyObject *)((PyArray_Descr *)__pyx_t_1));
   __pyx_r = ((PyArray_Descr *)__pyx_t_1);
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":251
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":251
  * 
  *         @property
  *         cdef inline dtype descr(self):             # <<<<<<<<<<<<<<
  *             """Returns an owned reference to the dtype of the array.
  *             """
  */
 
   /* function exit code */
   __pyx_L0:;
   __Pyx_XGIVEREF((PyObject *)__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":257
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":257
  * 
  *         @property
  *         cdef inline int ndim(self) nogil:             # <<<<<<<<<<<<<<
  *             """Returns the number of dimensions in the array.
  *             """
  */
 
 static CYTHON_INLINE int __pyx_f_5numpy_7ndarray_4ndim_ndim(PyArrayObject *__pyx_v_self) {
   int __pyx_r;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":260
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":260
  *             """Returns the number of dimensions in the array.
  *             """
  *             return PyArray_NDIM(self)             # <<<<<<<<<<<<<<
  * 
  *         @property
  */
   __pyx_r = PyArray_NDIM(__pyx_v_self);
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":257
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":257
  * 
  *         @property
  *         cdef inline int ndim(self) nogil:             # <<<<<<<<<<<<<<
  *             """Returns the number of dimensions in the array.
  *             """
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":263
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":263
  * 
  *         @property
  *         cdef inline npy_intp *shape(self) nogil:             # <<<<<<<<<<<<<<
  *             """Returns a pointer to the dimensions/shape of the array.
  *             The number of elements matches the number of dimensions of the array (ndim).
  */
 
 static CYTHON_INLINE npy_intp *__pyx_f_5numpy_7ndarray_5shape_shape(PyArrayObject *__pyx_v_self) {
   npy_intp *__pyx_r;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":268
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":268
  *             Can return NULL for 0-dimensional arrays.
  *             """
  *             return PyArray_DIMS(self)             # <<<<<<<<<<<<<<
  * 
  *         @property
  */
   __pyx_r = PyArray_DIMS(__pyx_v_self);
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":263
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":263
  * 
  *         @property
  *         cdef inline npy_intp *shape(self) nogil:             # <<<<<<<<<<<<<<
  *             """Returns a pointer to the dimensions/shape of the array.
  *             The number of elements matches the number of dimensions of the array (ndim).
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":271
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":271
  * 
  *         @property
  *         cdef inline npy_intp *strides(self) nogil:             # <<<<<<<<<<<<<<
  *             """Returns a pointer to the strides of the array.
  *             The number of elements matches the number of dimensions of the array (ndim).
  */
 
 static CYTHON_INLINE npy_intp *__pyx_f_5numpy_7ndarray_7strides_strides(PyArrayObject *__pyx_v_self) {
   npy_intp *__pyx_r;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":275
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":275
  *             The number of elements matches the number of dimensions of the array (ndim).
  *             """
  *             return PyArray_STRIDES(self)             # <<<<<<<<<<<<<<
  * 
  *         @property
  */
   __pyx_r = PyArray_STRIDES(__pyx_v_self);
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":271
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":271
  * 
  *         @property
  *         cdef inline npy_intp *strides(self) nogil:             # <<<<<<<<<<<<<<
  *             """Returns a pointer to the strides of the array.
  *             The number of elements matches the number of dimensions of the array (ndim).
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":278
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":278
  * 
  *         @property
  *         cdef inline npy_intp size(self) nogil:             # <<<<<<<<<<<<<<
  *             """Returns the total size (in number of elements) of the array.
  *             """
  */
 
 static CYTHON_INLINE npy_intp __pyx_f_5numpy_7ndarray_4size_size(PyArrayObject *__pyx_v_self) {
   npy_intp __pyx_r;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":281
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":281
  *             """Returns the total size (in number of elements) of the array.
  *             """
  *             return PyArray_SIZE(self)             # <<<<<<<<<<<<<<
  * 
  *         @property
  */
   __pyx_r = PyArray_SIZE(__pyx_v_self);
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":278
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":278
  * 
  *         @property
  *         cdef inline npy_intp size(self) nogil:             # <<<<<<<<<<<<<<
  *             """Returns the total size (in number of elements) of the array.
  *             """
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":284
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":284
  * 
  *         @property
  *         cdef inline char* data(self) nogil:             # <<<<<<<<<<<<<<
  *             """The pointer to the data buffer as a char*.
  *             This is provided for legacy reasons to avoid direct struct field access.
  */
 
 static CYTHON_INLINE char *__pyx_f_5numpy_7ndarray_4data_data(PyArrayObject *__pyx_v_self) {
   char *__pyx_r;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":290
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":290
  *             of `PyArray_DATA()` instead, which returns a 'void*'.
  *             """
  *             return PyArray_BYTES(self)             # <<<<<<<<<<<<<<
  * 
  *     ctypedef unsigned char      npy_bool
  */
   __pyx_r = PyArray_BYTES(__pyx_v_self);
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":284
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":284
  * 
  *         @property
  *         cdef inline char* data(self) nogil:             # <<<<<<<<<<<<<<
  *             """The pointer to the data buffer as a char*.
  *             This is provided for legacy reasons to avoid direct struct field access.
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":773
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":773
  * ctypedef npy_cdouble     complex_t
  * 
  * cdef inline object PyArray_MultiIterNew1(a):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  */
 
@@ -18733,29 +18769,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew1", 1);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":774
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":774
  * 
  * cdef inline object PyArray_MultiIterNew1(a):
  *     return PyArray_MultiIterNew(1, <void*>a)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(1, ((void *)__pyx_v_a)); if (unlikely(!__pyx_t_1)) __PYX_ERR(2, 774, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":773
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":773
  * ctypedef npy_cdouble     complex_t
  * 
  * cdef inline object PyArray_MultiIterNew1(a):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  */
 
@@ -18766,15 +18802,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":776
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":776
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  */
 
@@ -18783,29 +18819,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew2", 1);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":777
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":777
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(2, ((void *)__pyx_v_a), ((void *)__pyx_v_b)); if (unlikely(!__pyx_t_1)) __PYX_ERR(2, 777, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":776
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":776
  *     return PyArray_MultiIterNew(1, <void*>a)
  * 
  * cdef inline object PyArray_MultiIterNew2(a, b):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  */
 
@@ -18816,15 +18852,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":779
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":779
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  */
 
@@ -18833,29 +18869,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew3", 1);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":780
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":780
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(3, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c)); if (unlikely(!__pyx_t_1)) __PYX_ERR(2, 780, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":779
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":779
  *     return PyArray_MultiIterNew(2, <void*>a, <void*>b)
  * 
  * cdef inline object PyArray_MultiIterNew3(a, b, c):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  */
 
@@ -18866,15 +18902,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":782
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":782
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  */
 
@@ -18883,29 +18919,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew4", 1);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":783
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":783
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(4, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c), ((void *)__pyx_v_d)); if (unlikely(!__pyx_t_1)) __PYX_ERR(2, 783, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":782
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":782
  *     return PyArray_MultiIterNew(3, <void*>a, <void*>b, <void*> c)
  * 
  * cdef inline object PyArray_MultiIterNew4(a, b, c, d):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  */
 
@@ -18916,15 +18952,15 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":785
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":785
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  */
 
@@ -18933,29 +18969,29 @@
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("PyArray_MultiIterNew5", 1);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":786
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":786
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)             # <<<<<<<<<<<<<<
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  */
   __Pyx_XDECREF(__pyx_r);
   __pyx_t_1 = PyArray_MultiIterNew(5, ((void *)__pyx_v_a), ((void *)__pyx_v_b), ((void *)__pyx_v_c), ((void *)__pyx_v_d), ((void *)__pyx_v_e)); if (unlikely(!__pyx_t_1)) __PYX_ERR(2, 786, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_1);
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":785
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":785
  *     return PyArray_MultiIterNew(4, <void*>a, <void*>b, <void*>c, <void*> d)
  * 
  * cdef inline object PyArray_MultiIterNew5(a, b, c, d, e):             # <<<<<<<<<<<<<<
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  */
 
@@ -18966,217 +19002,217 @@
   __pyx_r = 0;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":788
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":788
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):             # <<<<<<<<<<<<<<
  *     if PyDataType_HASSUBARRAY(d):
  *         return <tuple>d.subarray.shape
  */
 
 static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyDataType_SHAPE(PyArray_Descr *__pyx_v_d) {
   PyObject *__pyx_r = NULL;
   __Pyx_RefNannyDeclarations
   int __pyx_t_1;
   __Pyx_RefNannySetupContext("PyDataType_SHAPE", 1);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":789
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":789
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  *     if PyDataType_HASSUBARRAY(d):             # <<<<<<<<<<<<<<
  *         return <tuple>d.subarray.shape
  *     else:
  */
   __pyx_t_1 = PyDataType_HASSUBARRAY(__pyx_v_d);
   if (__pyx_t_1) {
 
-    /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":790
+    /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":790
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  *     if PyDataType_HASSUBARRAY(d):
  *         return <tuple>d.subarray.shape             # <<<<<<<<<<<<<<
  *     else:
  *         return ()
  */
     __Pyx_XDECREF(__pyx_r);
     __Pyx_INCREF(((PyObject*)__pyx_v_d->subarray->shape));
     __pyx_r = ((PyObject*)__pyx_v_d->subarray->shape);
     goto __pyx_L0;
 
-    /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":789
+    /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":789
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):
  *     if PyDataType_HASSUBARRAY(d):             # <<<<<<<<<<<<<<
  *         return <tuple>d.subarray.shape
  *     else:
  */
   }
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":792
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":792
  *         return <tuple>d.subarray.shape
  *     else:
  *         return ()             # <<<<<<<<<<<<<<
  * 
  * 
  */
   /*else*/ {
     __Pyx_XDECREF(__pyx_r);
     __Pyx_INCREF(__pyx_empty_tuple);
     __pyx_r = __pyx_empty_tuple;
     goto __pyx_L0;
   }
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":788
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":788
  *     return PyArray_MultiIterNew(5, <void*>a, <void*>b, <void*>c, <void*> d, <void*> e)
  * 
  * cdef inline tuple PyDataType_SHAPE(dtype d):             # <<<<<<<<<<<<<<
  *     if PyDataType_HASSUBARRAY(d):
  *         return <tuple>d.subarray.shape
  */
 
   /* function exit code */
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":968
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":968
  *     int _import_umath() except -1
  * 
  * cdef inline void set_array_base(ndarray arr, object base):             # <<<<<<<<<<<<<<
  *     Py_INCREF(base) # important to do this before stealing the reference below!
  *     PyArray_SetBaseObject(arr, base)
  */
 
 static CYTHON_INLINE void __pyx_f_5numpy_set_array_base(PyArrayObject *__pyx_v_arr, PyObject *__pyx_v_base) {
   int __pyx_t_1;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":969
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":969
  * 
  * cdef inline void set_array_base(ndarray arr, object base):
  *     Py_INCREF(base) # important to do this before stealing the reference below!             # <<<<<<<<<<<<<<
  *     PyArray_SetBaseObject(arr, base)
  * 
  */
   Py_INCREF(__pyx_v_base);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":970
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":970
  * cdef inline void set_array_base(ndarray arr, object base):
  *     Py_INCREF(base) # important to do this before stealing the reference below!
  *     PyArray_SetBaseObject(arr, base)             # <<<<<<<<<<<<<<
  * 
  * cdef inline object get_array_base(ndarray arr):
  */
   __pyx_t_1 = PyArray_SetBaseObject(__pyx_v_arr, __pyx_v_base); if (unlikely(__pyx_t_1 == ((int)-1))) __PYX_ERR(2, 970, __pyx_L1_error)
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":968
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":968
  *     int _import_umath() except -1
  * 
  * cdef inline void set_array_base(ndarray arr, object base):             # <<<<<<<<<<<<<<
  *     Py_INCREF(base) # important to do this before stealing the reference below!
  *     PyArray_SetBaseObject(arr, base)
  */
 
   /* function exit code */
   goto __pyx_L0;
   __pyx_L1_error:;
   __Pyx_AddTraceback("numpy.set_array_base", __pyx_clineno, __pyx_lineno, __pyx_filename);
   __pyx_L0:;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":972
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":972
  *     PyArray_SetBaseObject(arr, base)
  * 
  * cdef inline object get_array_base(ndarray arr):             # <<<<<<<<<<<<<<
  *     base = PyArray_BASE(arr)
  *     if base is NULL:
  */
 
 static CYTHON_INLINE PyObject *__pyx_f_5numpy_get_array_base(PyArrayObject *__pyx_v_arr) {
   PyObject *__pyx_v_base;
   PyObject *__pyx_r = NULL;
   __Pyx_RefNannyDeclarations
   int __pyx_t_1;
   __Pyx_RefNannySetupContext("get_array_base", 1);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":973
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":973
  * 
  * cdef inline object get_array_base(ndarray arr):
  *     base = PyArray_BASE(arr)             # <<<<<<<<<<<<<<
  *     if base is NULL:
  *         return None
  */
   __pyx_v_base = PyArray_BASE(__pyx_v_arr);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":974
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":974
  * cdef inline object get_array_base(ndarray arr):
  *     base = PyArray_BASE(arr)
  *     if base is NULL:             # <<<<<<<<<<<<<<
  *         return None
  *     return <object>base
  */
   __pyx_t_1 = (__pyx_v_base == NULL);
   if (__pyx_t_1) {
 
-    /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":975
+    /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":975
  *     base = PyArray_BASE(arr)
  *     if base is NULL:
  *         return None             # <<<<<<<<<<<<<<
  *     return <object>base
  * 
  */
     __Pyx_XDECREF(__pyx_r);
     __pyx_r = Py_None; __Pyx_INCREF(Py_None);
     goto __pyx_L0;
 
-    /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":974
+    /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":974
  * cdef inline object get_array_base(ndarray arr):
  *     base = PyArray_BASE(arr)
  *     if base is NULL:             # <<<<<<<<<<<<<<
  *         return None
  *     return <object>base
  */
   }
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":976
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":976
  *     if base is NULL:
  *         return None
  *     return <object>base             # <<<<<<<<<<<<<<
  * 
  * # Versions of the import_* functions which are more suitable for
  */
   __Pyx_XDECREF(__pyx_r);
   __Pyx_INCREF(((PyObject *)__pyx_v_base));
   __pyx_r = ((PyObject *)__pyx_v_base);
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":972
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":972
  *     PyArray_SetBaseObject(arr, base)
  * 
  * cdef inline object get_array_base(ndarray arr):             # <<<<<<<<<<<<<<
  *     base = PyArray_BASE(arr)
  *     if base is NULL:
  */
 
   /* function exit code */
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":980
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":980
  * # Versions of the import_* functions which are more suitable for
  * # Cython code.
  * cdef inline int import_array() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         __pyx_import_array()
  */
 
@@ -19192,15 +19228,15 @@
   PyObject *__pyx_t_7 = NULL;
   PyObject *__pyx_t_8 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("import_array", 1);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":981
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":981
  * # Cython code.
  * cdef inline int import_array() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         __pyx_import_array()
  *     except Exception:
  */
   {
@@ -19208,68 +19244,68 @@
     __Pyx_PyThreadState_assign
     __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
     __Pyx_XGOTREF(__pyx_t_1);
     __Pyx_XGOTREF(__pyx_t_2);
     __Pyx_XGOTREF(__pyx_t_3);
     /*try:*/ {
 
-      /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":982
+      /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":982
  * cdef inline int import_array() except -1:
  *     try:
  *         __pyx_import_array()             # <<<<<<<<<<<<<<
  *     except Exception:
  *         raise ImportError("numpy.core.multiarray failed to import")
  */
       __pyx_t_4 = _import_array(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(2, 982, __pyx_L3_error)
 
-      /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":981
+      /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":981
  * # Cython code.
  * cdef inline int import_array() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         __pyx_import_array()
  *     except Exception:
  */
     }
     __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
     __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
     __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
     goto __pyx_L8_try_end;
     __pyx_L3_error:;
 
-    /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":983
+    /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":983
  *     try:
  *         __pyx_import_array()
  *     except Exception:             # <<<<<<<<<<<<<<
  *         raise ImportError("numpy.core.multiarray failed to import")
  * 
  */
     __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
     if (__pyx_t_4) {
       __Pyx_AddTraceback("numpy.import_array", __pyx_clineno, __pyx_lineno, __pyx_filename);
       if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(2, 983, __pyx_L5_except_error)
       __Pyx_XGOTREF(__pyx_t_5);
       __Pyx_XGOTREF(__pyx_t_6);
       __Pyx_XGOTREF(__pyx_t_7);
 
-      /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":984
+      /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":984
  *         __pyx_import_array()
  *     except Exception:
  *         raise ImportError("numpy.core.multiarray failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_umath() except -1:
  */
       __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__9, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(2, 984, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_8);
       __Pyx_Raise(__pyx_t_8, 0, 0, 0);
       __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
       __PYX_ERR(2, 984, __pyx_L5_except_error)
     }
     goto __pyx_L5_except_error;
 
-    /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":981
+    /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":981
  * # Cython code.
  * cdef inline int import_array() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         __pyx_import_array()
  *     except Exception:
  */
     __pyx_L5_except_error:;
@@ -19277,15 +19313,15 @@
     __Pyx_XGIVEREF(__pyx_t_2);
     __Pyx_XGIVEREF(__pyx_t_3);
     __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
     goto __pyx_L1_error;
     __pyx_L8_try_end:;
   }
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":980
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":980
  * # Versions of the import_* functions which are more suitable for
  * # Cython code.
  * cdef inline int import_array() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         __pyx_import_array()
  */
 
@@ -19300,15 +19336,15 @@
   __Pyx_AddTraceback("numpy.import_array", __pyx_clineno, __pyx_lineno, __pyx_filename);
   __pyx_r = -1;
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":986
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":986
  *         raise ImportError("numpy.core.multiarray failed to import")
  * 
  * cdef inline int import_umath() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -19324,15 +19360,15 @@
   PyObject *__pyx_t_7 = NULL;
   PyObject *__pyx_t_8 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("import_umath", 1);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":987
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":987
  * 
  * cdef inline int import_umath() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
   {
@@ -19340,68 +19376,68 @@
     __Pyx_PyThreadState_assign
     __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
     __Pyx_XGOTREF(__pyx_t_1);
     __Pyx_XGOTREF(__pyx_t_2);
     __Pyx_XGOTREF(__pyx_t_3);
     /*try:*/ {
 
-      /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":988
+      /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":988
  * cdef inline int import_umath() except -1:
  *     try:
  *         _import_umath()             # <<<<<<<<<<<<<<
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")
  */
       __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(2, 988, __pyx_L3_error)
 
-      /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":987
+      /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":987
  * 
  * cdef inline int import_umath() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
     }
     __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
     __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
     __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
     goto __pyx_L8_try_end;
     __pyx_L3_error:;
 
-    /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":989
+    /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":989
  *     try:
  *         _import_umath()
  *     except Exception:             # <<<<<<<<<<<<<<
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  */
     __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
     if (__pyx_t_4) {
       __Pyx_AddTraceback("numpy.import_umath", __pyx_clineno, __pyx_lineno, __pyx_filename);
       if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(2, 989, __pyx_L5_except_error)
       __Pyx_XGOTREF(__pyx_t_5);
       __Pyx_XGOTREF(__pyx_t_6);
       __Pyx_XGOTREF(__pyx_t_7);
 
-      /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":990
+      /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":990
  *         _import_umath()
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_ufunc() except -1:
  */
       __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__10, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(2, 990, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_8);
       __Pyx_Raise(__pyx_t_8, 0, 0, 0);
       __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
       __PYX_ERR(2, 990, __pyx_L5_except_error)
     }
     goto __pyx_L5_except_error;
 
-    /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":987
+    /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":987
  * 
  * cdef inline int import_umath() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
     __pyx_L5_except_error:;
@@ -19409,15 +19445,15 @@
     __Pyx_XGIVEREF(__pyx_t_2);
     __Pyx_XGIVEREF(__pyx_t_3);
     __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
     goto __pyx_L1_error;
     __pyx_L8_try_end:;
   }
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":986
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":986
  *         raise ImportError("numpy.core.multiarray failed to import")
  * 
  * cdef inline int import_umath() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -19432,15 +19468,15 @@
   __Pyx_AddTraceback("numpy.import_umath", __pyx_clineno, __pyx_lineno, __pyx_filename);
   __pyx_r = -1;
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":992
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":992
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  * cdef inline int import_ufunc() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -19456,15 +19492,15 @@
   PyObject *__pyx_t_7 = NULL;
   PyObject *__pyx_t_8 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("import_ufunc", 1);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":993
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":993
  * 
  * cdef inline int import_ufunc() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
   {
@@ -19472,68 +19508,68 @@
     __Pyx_PyThreadState_assign
     __Pyx_ExceptionSave(&__pyx_t_1, &__pyx_t_2, &__pyx_t_3);
     __Pyx_XGOTREF(__pyx_t_1);
     __Pyx_XGOTREF(__pyx_t_2);
     __Pyx_XGOTREF(__pyx_t_3);
     /*try:*/ {
 
-      /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":994
+      /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":994
  * cdef inline int import_ufunc() except -1:
  *     try:
  *         _import_umath()             # <<<<<<<<<<<<<<
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")
  */
       __pyx_t_4 = _import_umath(); if (unlikely(__pyx_t_4 == ((int)-1))) __PYX_ERR(2, 994, __pyx_L3_error)
 
-      /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":993
+      /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":993
  * 
  * cdef inline int import_ufunc() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
     }
     __Pyx_XDECREF(__pyx_t_1); __pyx_t_1 = 0;
     __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;
     __Pyx_XDECREF(__pyx_t_3); __pyx_t_3 = 0;
     goto __pyx_L8_try_end;
     __pyx_L3_error:;
 
-    /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":995
+    /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":995
  *     try:
  *         _import_umath()
  *     except Exception:             # <<<<<<<<<<<<<<
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  */
     __pyx_t_4 = __Pyx_PyErr_ExceptionMatches(((PyObject *)(&((PyTypeObject*)PyExc_Exception)[0])));
     if (__pyx_t_4) {
       __Pyx_AddTraceback("numpy.import_ufunc", __pyx_clineno, __pyx_lineno, __pyx_filename);
       if (__Pyx_GetException(&__pyx_t_5, &__pyx_t_6, &__pyx_t_7) < 0) __PYX_ERR(2, 995, __pyx_L5_except_error)
       __Pyx_XGOTREF(__pyx_t_5);
       __Pyx_XGOTREF(__pyx_t_6);
       __Pyx_XGOTREF(__pyx_t_7);
 
-      /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":996
+      /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":996
  *         _import_umath()
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
  * 
  * 
  */
       __pyx_t_8 = __Pyx_PyObject_Call(__pyx_builtin_ImportError, __pyx_tuple__10, NULL); if (unlikely(!__pyx_t_8)) __PYX_ERR(2, 996, __pyx_L5_except_error)
       __Pyx_GOTREF(__pyx_t_8);
       __Pyx_Raise(__pyx_t_8, 0, 0, 0);
       __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
       __PYX_ERR(2, 996, __pyx_L5_except_error)
     }
     goto __pyx_L5_except_error;
 
-    /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":993
+    /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":993
  * 
  * cdef inline int import_ufunc() except -1:
  *     try:             # <<<<<<<<<<<<<<
  *         _import_umath()
  *     except Exception:
  */
     __pyx_L5_except_error:;
@@ -19541,15 +19577,15 @@
     __Pyx_XGIVEREF(__pyx_t_2);
     __Pyx_XGIVEREF(__pyx_t_3);
     __Pyx_ExceptionReset(__pyx_t_1, __pyx_t_2, __pyx_t_3);
     goto __pyx_L1_error;
     __pyx_L8_try_end:;
   }
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":992
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":992
  *         raise ImportError("numpy.core.umath failed to import")
  * 
  * cdef inline int import_ufunc() except -1:             # <<<<<<<<<<<<<<
  *     try:
  *         _import_umath()
  */
 
@@ -19564,170 +19600,170 @@
   __Pyx_AddTraceback("numpy.import_ufunc", __pyx_clineno, __pyx_lineno, __pyx_filename);
   __pyx_r = -1;
   __pyx_L0:;
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":999
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":999
  * 
  * 
  * cdef inline bint is_timedelta64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.timedelta64)`
  */
 
 static CYTHON_INLINE int __pyx_f_5numpy_is_timedelta64_object(PyObject *__pyx_v_obj) {
   int __pyx_r;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":1011
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":1011
  *     bool
  *     """
  *     return PyObject_TypeCheck(obj, &PyTimedeltaArrType_Type)             # <<<<<<<<<<<<<<
  * 
  * 
  */
   __pyx_r = PyObject_TypeCheck(__pyx_v_obj, (&PyTimedeltaArrType_Type));
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":999
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":999
  * 
  * 
  * cdef inline bint is_timedelta64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.timedelta64)`
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":1014
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":1014
  * 
  * 
  * cdef inline bint is_datetime64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.datetime64)`
  */
 
 static CYTHON_INLINE int __pyx_f_5numpy_is_datetime64_object(PyObject *__pyx_v_obj) {
   int __pyx_r;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":1026
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":1026
  *     bool
  *     """
  *     return PyObject_TypeCheck(obj, &PyDatetimeArrType_Type)             # <<<<<<<<<<<<<<
  * 
  * 
  */
   __pyx_r = PyObject_TypeCheck(__pyx_v_obj, (&PyDatetimeArrType_Type));
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":1014
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":1014
  * 
  * 
  * cdef inline bint is_datetime64_object(object obj):             # <<<<<<<<<<<<<<
  *     """
  *     Cython equivalent of `isinstance(obj, np.datetime64)`
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":1029
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":1029
  * 
  * 
  * cdef inline npy_datetime get_datetime64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy datetime64 object
  */
 
 static CYTHON_INLINE npy_datetime __pyx_f_5numpy_get_datetime64_value(PyObject *__pyx_v_obj) {
   npy_datetime __pyx_r;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":1036
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":1036
  *     also needed.  That can be found using `get_datetime64_unit`.
  *     """
  *     return (<PyDatetimeScalarObject*>obj).obval             # <<<<<<<<<<<<<<
  * 
  * 
  */
   __pyx_r = ((PyDatetimeScalarObject *)__pyx_v_obj)->obval;
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":1029
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":1029
  * 
  * 
  * cdef inline npy_datetime get_datetime64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy datetime64 object
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":1039
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":1039
  * 
  * 
  * cdef inline npy_timedelta get_timedelta64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy timedelta64 object
  */
 
 static CYTHON_INLINE npy_timedelta __pyx_f_5numpy_get_timedelta64_value(PyObject *__pyx_v_obj) {
   npy_timedelta __pyx_r;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":1043
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":1043
  *     returns the int64 value underlying scalar numpy timedelta64 object
  *     """
  *     return (<PyTimedeltaScalarObject*>obj).obval             # <<<<<<<<<<<<<<
  * 
  * 
  */
   __pyx_r = ((PyTimedeltaScalarObject *)__pyx_v_obj)->obval;
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":1039
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":1039
  * 
  * 
  * cdef inline npy_timedelta get_timedelta64_value(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the int64 value underlying scalar numpy timedelta64 object
  */
 
   /* function exit code */
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":1046
+/* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":1046
  * 
  * 
  * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the unit part of the dtype for a numpy datetime64 object.
  */
 
 static CYTHON_INLINE NPY_DATETIMEUNIT __pyx_f_5numpy_get_datetime64_unit(PyObject *__pyx_v_obj) {
   NPY_DATETIMEUNIT __pyx_r;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":1050
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":1050
  *     returns the unit part of the dtype for a numpy datetime64 object.
  *     """
  *     return <NPY_DATETIMEUNIT>(<PyDatetimeScalarObject*>obj).obmeta.base             # <<<<<<<<<<<<<<
  */
   __pyx_r = ((NPY_DATETIMEUNIT)((PyDatetimeScalarObject *)__pyx_v_obj)->obmeta.base);
   goto __pyx_L0;
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":1046
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":1046
  * 
  * 
  * cdef inline NPY_DATETIMEUNIT get_datetime64_unit(object obj) nogil:             # <<<<<<<<<<<<<<
  *     """
  *     returns the unit part of the dtype for a numpy datetime64 object.
  */
 
@@ -22009,15 +22045,15 @@
     (__pyx_v_self->clauses[__pyx_v_self->n_clauses]).na = __pyx_v_na;
 
     /* "ruleopt/aux_classes/aux_classes.pyx":132
  *             self.clauses[self.n_clauses].na = na
  * 
  *             self.n_clauses += 1             # <<<<<<<<<<<<<<
  * 
- *     cdef bint _check_rule_nogil(self, float[:] X) noexcept nogil:
+ *     cdef char[:] _check_rule_nogil(self, float[:,:] X) noexcept:
  */
     __pyx_v_self->n_clauses = (__pyx_v_self->n_clauses + 1);
 
     /* "ruleopt/aux_classes/aux_classes.pyx":124
  *                     break
  * 
  *         if check_exist == 0:             # <<<<<<<<<<<<<<
@@ -22221,226 +22257,417 @@
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
 /* "ruleopt/aux_classes/aux_classes.pyx":134
  *             self.n_clauses += 1
  * 
- *     cdef bint _check_rule_nogil(self, float[:] X) noexcept nogil:             # <<<<<<<<<<<<<<
+ *     cdef char[:] _check_rule_nogil(self, float[:,:] X) noexcept:             # <<<<<<<<<<<<<<
  *         """
  *         Checks if the rule applies to the given feature values,
  */
 
-static int __pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule__check_rule_nogil(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_v_self, __Pyx_memviewslice __pyx_v_X) {
+static __Pyx_memviewslice __pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule__check_rule_nogil(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_v_self, __Pyx_memviewslice __pyx_v_X) {
   int __pyx_v_i;
-  int __pyx_r;
-  int __pyx_t_1;
-  int __pyx_t_2;
-  int __pyx_t_3;
-  int __pyx_t_4;
+  int __pyx_v_j;
+  __Pyx_memviewslice __pyx_v_result_data = { 0, 0, { 0 }, { 0 }, { 0 } };
+  __Pyx_memviewslice __pyx_r = { 0, 0, { 0 }, { 0 }, { 0 } };
+  __Pyx_RefNannyDeclarations
+  PyObject *__pyx_t_1 = NULL;
+  PyObject *__pyx_t_2 = NULL;
+  PyObject *__pyx_t_3 = NULL;
+  PyObject *__pyx_t_4 = NULL;
+  PyObject *__pyx_t_5 = NULL;
+  __Pyx_memviewslice __pyx_t_6 = { 0, 0, { 0 }, { 0 }, { 0 } };
+  Py_ssize_t __pyx_t_7;
+  Py_ssize_t __pyx_t_8;
+  int __pyx_t_9;
+  Py_ssize_t __pyx_t_10;
+  int __pyx_t_11;
+  int __pyx_t_12;
+  int __pyx_t_13;
+  __Pyx_memviewslice __pyx_t_14 = { 0, 0, { 0 }, { 0 }, { 0 } };
+  int __pyx_t_15;
+  int __pyx_lineno = 0;
+  const char *__pyx_filename = NULL;
+  int __pyx_clineno = 0;
+  __Pyx_RefNannySetupContext("_check_rule_nogil", 1);
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":151
- *         cdef int i
+  /* "ruleopt/aux_classes/aux_classes.pyx":150
+ *         """
+ *         cdef int i, j
+ *         cdef char[:] result_data = np.empty(X.shape[0], dtype=np.int8)             # <<<<<<<<<<<<<<
+ * 
  *         with boundscheck(False):
- *             for i in range(self.n_clauses):             # <<<<<<<<<<<<<<
- *                 if not self._check_clause_nogil(X, i):
- *                     return False
  */
-  __pyx_t_1 = __pyx_v_self->n_clauses;
-  __pyx_t_2 = __pyx_t_1;
-  for (__pyx_t_3 = 0; __pyx_t_3 < __pyx_t_2; __pyx_t_3+=1) {
-    __pyx_v_i = __pyx_t_3;
+  __Pyx_GetModuleGlobalName(__pyx_t_1, __pyx_n_s_np); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 150, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_1);
+  __pyx_t_2 = __Pyx_PyObject_GetAttrStr(__pyx_t_1, __pyx_n_s_empty); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 150, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_2);
+  __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
+  __pyx_t_1 = PyInt_FromSsize_t((__pyx_v_X.shape[0])); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 150, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_1);
+  __pyx_t_3 = PyTuple_New(1); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 150, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_3);
+  __Pyx_GIVEREF(__pyx_t_1);
+  if (__Pyx_PyTuple_SET_ITEM(__pyx_t_3, 0, __pyx_t_1)) __PYX_ERR(0, 150, __pyx_L1_error);
+  __pyx_t_1 = 0;
+  __pyx_t_1 = __Pyx_PyDict_NewPresized(1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 150, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_1);
+  __Pyx_GetModuleGlobalName(__pyx_t_4, __pyx_n_s_np); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 150, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_4);
+  __pyx_t_5 = __Pyx_PyObject_GetAttrStr(__pyx_t_4, __pyx_n_s_int8); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 150, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_5);
+  __Pyx_DECREF(__pyx_t_4); __pyx_t_4 = 0;
+  if (PyDict_SetItem(__pyx_t_1, __pyx_n_s_dtype, __pyx_t_5) < 0) __PYX_ERR(0, 150, __pyx_L1_error)
+  __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
+  __pyx_t_5 = __Pyx_PyObject_Call(__pyx_t_2, __pyx_t_3, __pyx_t_1); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 150, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_5);
+  __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
+  __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
+  __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
+  __pyx_t_6 = __Pyx_PyObject_to_MemoryviewSlice_ds_char(__pyx_t_5, PyBUF_WRITABLE); if (unlikely(!__pyx_t_6.memview)) __PYX_ERR(0, 150, __pyx_L1_error)
+  __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
+  __pyx_v_result_data = __pyx_t_6;
+  __pyx_t_6.memview = NULL;
+  __pyx_t_6.data = NULL;
 
-    /* "ruleopt/aux_classes/aux_classes.pyx":152
+  /* "ruleopt/aux_classes/aux_classes.pyx":153
+ * 
  *         with boundscheck(False):
- *             for i in range(self.n_clauses):
- *                 if not self._check_clause_nogil(X, i):             # <<<<<<<<<<<<<<
- *                     return False
- *         return True
+ *             with nogil:             # <<<<<<<<<<<<<<
+ *                 for i in range(X.shape[0]):
+ *                     result_data[i] = 1  # Assume rule applies
  */
-    __pyx_t_4 = (!((struct __pyx_vtabstruct_7ruleopt_11aux_classes_11aux_classes_Rule *)__pyx_v_self->__pyx_vtab)->_check_clause_nogil(__pyx_v_self, __pyx_v_X, __pyx_v_i));
-    if (__pyx_t_4) {
+  {
+      #ifdef WITH_THREAD
+      PyThreadState *_save;
+      _save = NULL;
+      Py_UNBLOCK_THREADS
+      __Pyx_FastGIL_Remember();
+      #endif
+      /*try:*/ {
 
-      /* "ruleopt/aux_classes/aux_classes.pyx":153
- *             for i in range(self.n_clauses):
- *                 if not self._check_clause_nogil(X, i):
- *                     return False             # <<<<<<<<<<<<<<
- *         return True
- * 
+        /* "ruleopt/aux_classes/aux_classes.pyx":154
+ *         with boundscheck(False):
+ *             with nogil:
+ *                 for i in range(X.shape[0]):             # <<<<<<<<<<<<<<
+ *                     result_data[i] = 1  # Assume rule applies
+ *                     for j in range(self.n_clauses):
+ */
+        __pyx_t_7 = (__pyx_v_X.shape[0]);
+        __pyx_t_8 = __pyx_t_7;
+        for (__pyx_t_9 = 0; __pyx_t_9 < __pyx_t_8; __pyx_t_9+=1) {
+          __pyx_v_i = __pyx_t_9;
+
+          /* "ruleopt/aux_classes/aux_classes.pyx":155
+ *             with nogil:
+ *                 for i in range(X.shape[0]):
+ *                     result_data[i] = 1  # Assume rule applies             # <<<<<<<<<<<<<<
+ *                     for j in range(self.n_clauses):
+ *                         if not self._check_clause_nogil(X[i], j):
+ */
+          __pyx_t_10 = __pyx_v_i;
+          if (__pyx_t_10 < 0) __pyx_t_10 += __pyx_v_result_data.shape[0];
+          *((char *) ( /* dim=0 */ (__pyx_v_result_data.data + __pyx_t_10 * __pyx_v_result_data.strides[0]) )) = 1;
+
+          /* "ruleopt/aux_classes/aux_classes.pyx":156
+ *                 for i in range(X.shape[0]):
+ *                     result_data[i] = 1  # Assume rule applies
+ *                     for j in range(self.n_clauses):             # <<<<<<<<<<<<<<
+ *                         if not self._check_clause_nogil(X[i], j):
+ *                             result_data[i] = 0  # Rule doesn't apply
+ */
+          __pyx_t_11 = __pyx_v_self->n_clauses;
+          __pyx_t_12 = __pyx_t_11;
+          for (__pyx_t_13 = 0; __pyx_t_13 < __pyx_t_12; __pyx_t_13+=1) {
+            __pyx_v_j = __pyx_t_13;
+
+            /* "ruleopt/aux_classes/aux_classes.pyx":157
+ *                     result_data[i] = 1  # Assume rule applies
+ *                     for j in range(self.n_clauses):
+ *                         if not self._check_clause_nogil(X[i], j):             # <<<<<<<<<<<<<<
+ *                             result_data[i] = 0  # Rule doesn't apply
+ *                             break
+ */
+            __pyx_t_14.data = __pyx_v_X.data;
+            __pyx_t_14.memview = __pyx_v_X.memview;
+            __PYX_INC_MEMVIEW(&__pyx_t_14, 0);
+            {
+    Py_ssize_t __pyx_tmp_idx = __pyx_v_i;
+        Py_ssize_t __pyx_tmp_shape = __pyx_v_X.shape[0];
+    Py_ssize_t __pyx_tmp_stride = __pyx_v_X.strides[0];
+        if (__pyx_tmp_idx < 0)
+            __pyx_tmp_idx += __pyx_tmp_shape;
+        __pyx_t_14.data += __pyx_tmp_idx * __pyx_tmp_stride;
+}
+
+__pyx_t_14.shape[0] = __pyx_v_X.shape[1];
+__pyx_t_14.strides[0] = __pyx_v_X.strides[1];
+    __pyx_t_14.suboffsets[0] = -1;
+
+__pyx_t_15 = (!((struct __pyx_vtabstruct_7ruleopt_11aux_classes_11aux_classes_Rule *)__pyx_v_self->__pyx_vtab)->_check_clause_nogil(__pyx_v_self, __pyx_t_14, __pyx_v_j));
+            __PYX_XCLEAR_MEMVIEW(&__pyx_t_14, 0);
+            __pyx_t_14.memview = NULL; __pyx_t_14.data = NULL;
+            if (__pyx_t_15) {
+
+              /* "ruleopt/aux_classes/aux_classes.pyx":158
+ *                     for j in range(self.n_clauses):
+ *                         if not self._check_clause_nogil(X[i], j):
+ *                             result_data[i] = 0  # Rule doesn't apply             # <<<<<<<<<<<<<<
+ *                             break
+ * 
+ */
+              __pyx_t_10 = __pyx_v_i;
+              if (__pyx_t_10 < 0) __pyx_t_10 += __pyx_v_result_data.shape[0];
+              *((char *) ( /* dim=0 */ (__pyx_v_result_data.data + __pyx_t_10 * __pyx_v_result_data.strides[0]) )) = 0;
+
+              /* "ruleopt/aux_classes/aux_classes.pyx":159
+ *                         if not self._check_clause_nogil(X[i], j):
+ *                             result_data[i] = 0  # Rule doesn't apply
+ *                             break             # <<<<<<<<<<<<<<
+ * 
+ *         return result_data
+ */
+              goto __pyx_L9_break;
+
+              /* "ruleopt/aux_classes/aux_classes.pyx":157
+ *                     result_data[i] = 1  # Assume rule applies
+ *                     for j in range(self.n_clauses):
+ *                         if not self._check_clause_nogil(X[i], j):             # <<<<<<<<<<<<<<
+ *                             result_data[i] = 0  # Rule doesn't apply
+ *                             break
  */
-      __pyx_r = 0;
-      goto __pyx_L0;
+            }
+          }
+          __pyx_L9_break:;
+        }
+      }
 
-      /* "ruleopt/aux_classes/aux_classes.pyx":152
+      /* "ruleopt/aux_classes/aux_classes.pyx":153
+ * 
  *         with boundscheck(False):
- *             for i in range(self.n_clauses):
- *                 if not self._check_clause_nogil(X, i):             # <<<<<<<<<<<<<<
- *                     return False
- *         return True
- */
-    }
+ *             with nogil:             # <<<<<<<<<<<<<<
+ *                 for i in range(X.shape[0]):
+ *                     result_data[i] = 1  # Assume rule applies
+ */
+      /*finally:*/ {
+        /*normal exit:*/{
+          #ifdef WITH_THREAD
+          __Pyx_FastGIL_Forget();
+          Py_BLOCK_THREADS
+          #endif
+          goto __pyx_L5;
+        }
+        __pyx_L5:;
+      }
   }
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":154
- *                 if not self._check_clause_nogil(X, i):
- *                     return False
- *         return True             # <<<<<<<<<<<<<<
+  /* "ruleopt/aux_classes/aux_classes.pyx":161
+ *                             break
+ * 
+ *         return result_data             # <<<<<<<<<<<<<<
  * 
  *     cdef bint _check_clause_nogil(self, float[:] X, int idx) noexcept nogil:
  */
-  __pyx_r = 1;
+  __PYX_INC_MEMVIEW(&__pyx_v_result_data, 1);
+  __pyx_r = __pyx_v_result_data;
   goto __pyx_L0;
 
   /* "ruleopt/aux_classes/aux_classes.pyx":134
  *             self.n_clauses += 1
  * 
- *     cdef bint _check_rule_nogil(self, float[:] X) noexcept nogil:             # <<<<<<<<<<<<<<
+ *     cdef char[:] _check_rule_nogil(self, float[:,:] X) noexcept:             # <<<<<<<<<<<<<<
  *         """
  *         Checks if the rule applies to the given feature values,
  */
 
   /* function exit code */
+  __pyx_L1_error:;
+  __Pyx_XDECREF(__pyx_t_1);
+  __Pyx_XDECREF(__pyx_t_2);
+  __Pyx_XDECREF(__pyx_t_3);
+  __Pyx_XDECREF(__pyx_t_4);
+  __Pyx_XDECREF(__pyx_t_5);
+  __PYX_XCLEAR_MEMVIEW(&__pyx_t_6, 1);
+  __PYX_XCLEAR_MEMVIEW(&__pyx_t_14, 1);
+  __pyx_r.data = NULL;
+  __pyx_r.memview = NULL;
+  __Pyx_AddTraceback("ruleopt.aux_classes.aux_classes.Rule._check_rule_nogil", __pyx_clineno, __pyx_lineno, __pyx_filename);
+  goto __pyx_L2;
   __pyx_L0:;
+  if (unlikely(!__pyx_r.memview)) {
+    PyErr_SetString(PyExc_TypeError, "Memoryview return value is not initialized");
+  }
+  __pyx_L2:;
+  __PYX_XCLEAR_MEMVIEW(&__pyx_v_result_data, 1);
+  __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "ruleopt/aux_classes/aux_classes.pyx":156
- *         return True
+/* "ruleopt/aux_classes/aux_classes.pyx":163
+ *         return result_data
  * 
  *     cdef bint _check_clause_nogil(self, float[:] X, int idx) noexcept nogil:             # <<<<<<<<<<<<<<
  *         """
  *         Checks if a specific clause of the rule applies to the given feature values,
  */
 
 static int __pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule__check_clause_nogil(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_v_self, __Pyx_memviewslice __pyx_v_X, int __pyx_v_idx) {
   struct __pyx_t_7ruleopt_11aux_classes_11aux_classes_ClauseStruct __pyx_v_clause;
   float __pyx_v_val;
   int __pyx_r;
   Py_ssize_t __pyx_t_1;
   int __pyx_t_2;
+  int __pyx_t_3;
+  int __pyx_lineno = 0;
+  const char *__pyx_filename = NULL;
+  int __pyx_clineno = 0;
+  #ifdef WITH_THREAD
+  PyGILState_STATE __pyx_gilstate_save;
+  #endif
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":177
- * 
- *         with boundscheck(False):
- *             clause = self.clauses[idx]             # <<<<<<<<<<<<<<
- *             val = X[clause.feature]
+  /* "ruleopt/aux_classes/aux_classes.pyx":180
+ *             True if the clause applies, False otherwise.
+ *         """
+ *         cdef ClauseStruct clause = self.clauses[idx]             # <<<<<<<<<<<<<<
+ *         cdef float val = X[clause.feature]
  * 
  */
   __pyx_v_clause = (__pyx_v_self->clauses[__pyx_v_idx]);
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":178
- *         with boundscheck(False):
- *             clause = self.clauses[idx]
- *             val = X[clause.feature]             # <<<<<<<<<<<<<<
+  /* "ruleopt/aux_classes/aux_classes.pyx":181
+ *         """
+ *         cdef ClauseStruct clause = self.clauses[idx]
+ *         cdef float val = X[clause.feature]             # <<<<<<<<<<<<<<
  * 
  *         if val != val:  # Checking for NaN
  */
   __pyx_t_1 = __pyx_v_clause.feature;
-  if (__pyx_t_1 < 0) __pyx_t_1 += __pyx_v_X.shape[0];
+  __pyx_t_2 = -1;
+  if (__pyx_t_1 < 0) {
+    __pyx_t_1 += __pyx_v_X.shape[0];
+    if (unlikely(__pyx_t_1 < 0)) __pyx_t_2 = 0;
+  } else if (unlikely(__pyx_t_1 >= __pyx_v_X.shape[0])) __pyx_t_2 = 0;
+  if (unlikely(__pyx_t_2 != -1)) {
+    __Pyx_RaiseBufferIndexErrorNogil(__pyx_t_2);
+    __PYX_ERR(0, 181, __pyx_L1_error)
+  }
   __pyx_v_val = (*((float *) ( /* dim=0 */ (__pyx_v_X.data + __pyx_t_1 * __pyx_v_X.strides[0]) )));
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":180
- *             val = X[clause.feature]
+  /* "ruleopt/aux_classes/aux_classes.pyx":183
+ *         cdef float val = X[clause.feature]
  * 
  *         if val != val:  # Checking for NaN             # <<<<<<<<<<<<<<
  *             return clause.na
  *         return clause.lb < val <= clause.ub
  */
-  __pyx_t_2 = (__pyx_v_val != __pyx_v_val);
-  if (__pyx_t_2) {
+  __pyx_t_3 = (__pyx_v_val != __pyx_v_val);
+  if (__pyx_t_3) {
 
-    /* "ruleopt/aux_classes/aux_classes.pyx":181
+    /* "ruleopt/aux_classes/aux_classes.pyx":184
  * 
  *         if val != val:  # Checking for NaN
  *             return clause.na             # <<<<<<<<<<<<<<
  *         return clause.lb < val <= clause.ub
  * 
  */
     __pyx_r = __pyx_v_clause.na;
     goto __pyx_L0;
 
-    /* "ruleopt/aux_classes/aux_classes.pyx":180
- *             val = X[clause.feature]
+    /* "ruleopt/aux_classes/aux_classes.pyx":183
+ *         cdef float val = X[clause.feature]
  * 
  *         if val != val:  # Checking for NaN             # <<<<<<<<<<<<<<
  *             return clause.na
  *         return clause.lb < val <= clause.ub
  */
   }
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":182
+  /* "ruleopt/aux_classes/aux_classes.pyx":185
  *         if val != val:  # Checking for NaN
  *             return clause.na
  *         return clause.lb < val <= clause.ub             # <<<<<<<<<<<<<<
  * 
- *     cpdef bint check_rule(self, float[:] X):
+ * 
  */
-  __pyx_t_2 = (__pyx_v_clause.lb < __pyx_v_val);
-  if (__pyx_t_2) {
-    __pyx_t_2 = (__pyx_v_val <= __pyx_v_clause.ub);
+  __pyx_t_3 = (__pyx_v_clause.lb < __pyx_v_val);
+  if (__pyx_t_3) {
+    __pyx_t_3 = (__pyx_v_val <= __pyx_v_clause.ub);
   }
-  __pyx_r = __pyx_t_2;
+  __pyx_r = __pyx_t_3;
   goto __pyx_L0;
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":156
- *         return True
+  /* "ruleopt/aux_classes/aux_classes.pyx":163
+ *         return result_data
  * 
  *     cdef bint _check_clause_nogil(self, float[:] X, int idx) noexcept nogil:             # <<<<<<<<<<<<<<
  *         """
  *         Checks if a specific clause of the rule applies to the given feature values,
  */
 
   /* function exit code */
+  __pyx_L1_error:;
+  #ifdef WITH_THREAD
+  __pyx_gilstate_save = __Pyx_PyGILState_Ensure();
+  #endif
+  __Pyx_WriteUnraisable("ruleopt.aux_classes.aux_classes.Rule._check_clause_nogil", __pyx_clineno, __pyx_lineno, __pyx_filename, 1, 0);
+  __pyx_r = 0;
+  #ifdef WITH_THREAD
+  __Pyx_PyGILState_Release(__pyx_gilstate_save);
+  #endif
   __pyx_L0:;
   return __pyx_r;
 }
 
-/* "ruleopt/aux_classes/aux_classes.pyx":184
- *         return clause.lb < val <= clause.ub
+/* "ruleopt/aux_classes/aux_classes.pyx":189
  * 
- *     cpdef bint check_rule(self, float[:] X):             # <<<<<<<<<<<<<<
+ * 
+ *     cpdef cnp.ndarray[cnp.uint8_t, ndim=1] check_rule(self, float[:, :] X):             # <<<<<<<<<<<<<<
  *         """
  *         Checks if the rule applies to the given feature values.
  */
 
 static PyObject *__pyx_pw_7ruleopt_11aux_classes_11aux_classes_4Rule_13check_rule(PyObject *__pyx_v_self, 
 #if CYTHON_METH_FASTCALL
 PyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds
 #else
 PyObject *__pyx_args, PyObject *__pyx_kwds
 #endif
 ); /*proto*/
-static int __pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule_check_rule(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_v_self, __Pyx_memviewslice __pyx_v_X, int __pyx_skip_dispatch) {
-  int __pyx_r;
+static PyArrayObject *__pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule_check_rule(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_v_self, __Pyx_memviewslice __pyx_v_X, int __pyx_skip_dispatch) {
+  __Pyx_memviewslice __pyx_v_result = { 0, 0, { 0 }, { 0 }, { 0 } };
+  PyArrayObject *__pyx_r = NULL;
   __Pyx_RefNannyDeclarations
   PyObject *__pyx_t_1 = NULL;
   PyObject *__pyx_t_2 = NULL;
   PyObject *__pyx_t_3 = NULL;
   PyObject *__pyx_t_4 = NULL;
   PyObject *__pyx_t_5 = NULL;
   int __pyx_t_6;
-  int __pyx_t_7;
+  __Pyx_memviewslice __pyx_t_7 = { 0, 0, { 0 }, { 0 }, { 0 } };
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("check_rule", 1);
   /* Check if called by wrapper */
   if (unlikely(__pyx_skip_dispatch)) ;
   /* Check if overridden in Python */
   else if (unlikely((Py_TYPE(((PyObject *)__pyx_v_self))->tp_dictoffset != 0) || __Pyx_PyType_HasFeature(Py_TYPE(((PyObject *)__pyx_v_self)), (Py_TPFLAGS_IS_ABSTRACT | Py_TPFLAGS_HEAPTYPE)))) {
     #if CYTHON_USE_DICT_VERSIONS && CYTHON_USE_PYTYPE_LOOKUP && CYTHON_USE_TYPE_SLOTS
     static PY_UINT64_T __pyx_tp_dict_version = __PYX_DICT_VERSION_INIT, __pyx_obj_dict_version = __PYX_DICT_VERSION_INIT;
     if (unlikely(!__Pyx_object_dict_version_matches(((PyObject *)__pyx_v_self), __pyx_tp_dict_version, __pyx_obj_dict_version))) {
       PY_UINT64_T __pyx_typedict_guard = __Pyx_get_tp_dict_version(((PyObject *)__pyx_v_self));
       #endif
-      __pyx_t_1 = __Pyx_PyObject_GetAttrStr(((PyObject *)__pyx_v_self), __pyx_n_s_check_rule); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 184, __pyx_L1_error)
+      __pyx_t_1 = __Pyx_PyObject_GetAttrStr(((PyObject *)__pyx_v_self), __pyx_n_s_check_rule); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 189, __pyx_L1_error)
       __Pyx_GOTREF(__pyx_t_1);
       if (!__Pyx_IsSameCFunction(__pyx_t_1, (void*) __pyx_pw_7ruleopt_11aux_classes_11aux_classes_4Rule_13check_rule)) {
-        if (unlikely(!__pyx_v_X.memview)) { __Pyx_RaiseUnboundLocalError("X"); __PYX_ERR(0, 184, __pyx_L1_error) }
-        __pyx_t_3 = __pyx_memoryview_fromslice(__pyx_v_X, 1, (PyObject *(*)(char *)) __pyx_memview_get_float, (int (*)(char *, PyObject *)) __pyx_memview_set_float, 0);; if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 184, __pyx_L1_error)
+        __Pyx_XDECREF((PyObject *)__pyx_r);
+        if (unlikely(!__pyx_v_X.memview)) { __Pyx_RaiseUnboundLocalError("X"); __PYX_ERR(0, 189, __pyx_L1_error) }
+        __pyx_t_3 = __pyx_memoryview_fromslice(__pyx_v_X, 2, (PyObject *(*)(char *)) __pyx_memview_get_float, (int (*)(char *, PyObject *)) __pyx_memview_set_float, 0);; if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 189, __pyx_L1_error)
         __Pyx_GOTREF(__pyx_t_3);
         __Pyx_INCREF(__pyx_t_1);
         __pyx_t_4 = __pyx_t_1; __pyx_t_5 = NULL;
         __pyx_t_6 = 0;
         #if CYTHON_UNPACK_METHODS
         if (unlikely(PyMethod_Check(__pyx_t_4))) {
           __pyx_t_5 = PyMethod_GET_SELF(__pyx_t_4);
@@ -22454,21 +22681,21 @@
         }
         #endif
         {
           PyObject *__pyx_callargs[2] = {__pyx_t_5, __pyx_t_3};
           __pyx_t_2 = __Pyx_PyObject_FastCall(__pyx_t_4, __pyx_callargs+1-__pyx_t_6, 1+__pyx_t_6);
           __Pyx_XDECREF(__pyx_t_5); __pyx_t_5 = 0;
           __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
-          if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 184, __pyx_L1_error)
+          if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 189, __pyx_L1_error)
           __Pyx_GOTREF(__pyx_t_2);
           __Pyx_DECREF(__pyx_t_4); __pyx_t_4 = 0;
         }
-        __pyx_t_7 = __Pyx_PyObject_IsTrue(__pyx_t_2); if (unlikely((__pyx_t_7 == (int)-1) && PyErr_Occurred())) __PYX_ERR(0, 184, __pyx_L1_error)
-        __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
-        __pyx_r = __pyx_t_7;
+        if (!(likely(((__pyx_t_2) == Py_None) || likely(__Pyx_TypeTest(__pyx_t_2, __pyx_ptype_5numpy_ndarray))))) __PYX_ERR(0, 189, __pyx_L1_error)
+        __pyx_r = ((PyArrayObject *)__pyx_t_2);
+        __pyx_t_2 = 0;
         __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
         goto __pyx_L0;
       }
       #if CYTHON_USE_DICT_VERSIONS && CYTHON_USE_PYTYPE_LOOKUP && CYTHON_USE_TYPE_SLOTS
       __pyx_tp_dict_version = __Pyx_get_tp_dict_version(((PyObject *)__pyx_v_self));
       __pyx_obj_dict_version = __Pyx_get_object_dict_version(((PyObject *)__pyx_v_self));
       if (unlikely(__pyx_typedict_guard != __pyx_tp_dict_version)) {
@@ -22477,55 +22704,99 @@
       #endif
       __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
       #if CYTHON_USE_DICT_VERSIONS && CYTHON_USE_PYTYPE_LOOKUP && CYTHON_USE_TYPE_SLOTS
     }
     #endif
   }
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":198
- *             True if the rule applies, False otherwise.
+  /* "ruleopt/aux_classes/aux_classes.pyx":204
+ *             indicating True (1) if the rule applies, False (0) otherwise.
  *         """
- *         return self._check_rule_nogil(X)             # <<<<<<<<<<<<<<
+ *         cdef char[:] result = self._check_rule_nogil(X)             # <<<<<<<<<<<<<<
+ *         return np.asarray(result, dtype=np.uint8)
  * 
- *     def to_text(self, feature_names=None):
  */
-  __pyx_r = ((struct __pyx_vtabstruct_7ruleopt_11aux_classes_11aux_classes_Rule *)__pyx_v_self->__pyx_vtab)->_check_rule_nogil(__pyx_v_self, __pyx_v_X);
+  __pyx_t_7 = ((struct __pyx_vtabstruct_7ruleopt_11aux_classes_11aux_classes_Rule *)__pyx_v_self->__pyx_vtab)->_check_rule_nogil(__pyx_v_self, __pyx_v_X); if (unlikely(!__pyx_t_7.memview)) __PYX_ERR(0, 204, __pyx_L1_error)
+  __pyx_v_result = __pyx_t_7;
+  __pyx_t_7.memview = NULL;
+  __pyx_t_7.data = NULL;
+
+  /* "ruleopt/aux_classes/aux_classes.pyx":205
+ *         """
+ *         cdef char[:] result = self._check_rule_nogil(X)
+ *         return np.asarray(result, dtype=np.uint8)             # <<<<<<<<<<<<<<
+ * 
+ * 
+ */
+  __Pyx_XDECREF((PyObject *)__pyx_r);
+  __Pyx_GetModuleGlobalName(__pyx_t_1, __pyx_n_s_np); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 205, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_1);
+  __pyx_t_2 = __Pyx_PyObject_GetAttrStr(__pyx_t_1, __pyx_n_s_asarray); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 205, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_2);
+  __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
+  __pyx_t_1 = __pyx_memoryview_fromslice(__pyx_v_result, 1, (PyObject *(*)(char *)) __pyx_memview_get_char, (int (*)(char *, PyObject *)) __pyx_memview_set_char, 0);; if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 205, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_1);
+  __pyx_t_4 = PyTuple_New(1); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 205, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_4);
+  __Pyx_GIVEREF(__pyx_t_1);
+  if (__Pyx_PyTuple_SET_ITEM(__pyx_t_4, 0, __pyx_t_1)) __PYX_ERR(0, 205, __pyx_L1_error);
+  __pyx_t_1 = 0;
+  __pyx_t_1 = __Pyx_PyDict_NewPresized(1); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 205, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_1);
+  __Pyx_GetModuleGlobalName(__pyx_t_3, __pyx_n_s_np); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 205, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_3);
+  __pyx_t_5 = __Pyx_PyObject_GetAttrStr(__pyx_t_3, __pyx_n_s_uint8); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 205, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_5);
+  __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;
+  if (PyDict_SetItem(__pyx_t_1, __pyx_n_s_dtype, __pyx_t_5) < 0) __PYX_ERR(0, 205, __pyx_L1_error)
+  __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
+  __pyx_t_5 = __Pyx_PyObject_Call(__pyx_t_2, __pyx_t_4, __pyx_t_1); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 205, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_5);
+  __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;
+  __Pyx_DECREF(__pyx_t_4); __pyx_t_4 = 0;
+  __Pyx_DECREF(__pyx_t_1); __pyx_t_1 = 0;
+  if (!(likely(((__pyx_t_5) == Py_None) || likely(__Pyx_TypeTest(__pyx_t_5, __pyx_ptype_5numpy_ndarray))))) __PYX_ERR(0, 205, __pyx_L1_error)
+  __pyx_r = ((PyArrayObject *)__pyx_t_5);
+  __pyx_t_5 = 0;
   goto __pyx_L0;
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":184
- *         return clause.lb < val <= clause.ub
+  /* "ruleopt/aux_classes/aux_classes.pyx":189
+ * 
  * 
- *     cpdef bint check_rule(self, float[:] X):             # <<<<<<<<<<<<<<
+ *     cpdef cnp.ndarray[cnp.uint8_t, ndim=1] check_rule(self, float[:, :] X):             # <<<<<<<<<<<<<<
  *         """
  *         Checks if the rule applies to the given feature values.
  */
 
   /* function exit code */
   __pyx_L1_error:;
   __Pyx_XDECREF(__pyx_t_1);
   __Pyx_XDECREF(__pyx_t_2);
   __Pyx_XDECREF(__pyx_t_3);
   __Pyx_XDECREF(__pyx_t_4);
   __Pyx_XDECREF(__pyx_t_5);
+  __PYX_XCLEAR_MEMVIEW(&__pyx_t_7, 1);
   __Pyx_AddTraceback("ruleopt.aux_classes.aux_classes.Rule.check_rule", __pyx_clineno, __pyx_lineno, __pyx_filename);
   __pyx_r = 0;
   __pyx_L0:;
+  __PYX_XCLEAR_MEMVIEW(&__pyx_v_result, 1);
+  __Pyx_XGIVEREF((PyObject *)__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
 /* Python wrapper */
 static PyObject *__pyx_pw_7ruleopt_11aux_classes_11aux_classes_4Rule_13check_rule(PyObject *__pyx_v_self, 
 #if CYTHON_METH_FASTCALL
 PyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds
 #else
 PyObject *__pyx_args, PyObject *__pyx_kwds
 #endif
 ); /*proto*/
-PyDoc_STRVAR(__pyx_doc_7ruleopt_11aux_classes_11aux_classes_4Rule_12check_rule, "\n        Checks if the rule applies to the given feature values.\n\n        Parameters:\n        -----------\n        X : cnp.ndarray[float, ndim=1]\n            The array of feature values to check against the rule.\n\n        Returns:\n        --------\n        bint\n            True if the rule applies, False otherwise.\n        ");
+PyDoc_STRVAR(__pyx_doc_7ruleopt_11aux_classes_11aux_classes_4Rule_12check_rule, "\n        Checks if the rule applies to the given feature values.\n\n        Parameters:\n        -----------\n        X : cnp.ndarray[cnp.float32_t, ndim=2]\n            The 2D array of feature values to check against the rule. Each row represents a different set of features.\n\n        Returns:\n        --------\n        np.ndarray[cnp.uint8_t, ndim=1]\n            An array indicating whether the rule applies to each feature value. Each element in the array corresponds to a row in X, \n            indicating True (1) if the rule applies, False (0) otherwise.\n        ");
 static PyMethodDef __pyx_mdef_7ruleopt_11aux_classes_11aux_classes_4Rule_13check_rule = {"check_rule", (PyCFunction)(void*)(__Pyx_PyCFunction_FastCallWithKeywords)__pyx_pw_7ruleopt_11aux_classes_11aux_classes_4Rule_13check_rule, __Pyx_METH_FASTCALL|METH_KEYWORDS, __pyx_doc_7ruleopt_11aux_classes_11aux_classes_4Rule_12check_rule};
 static PyObject *__pyx_pw_7ruleopt_11aux_classes_11aux_classes_4Rule_13check_rule(PyObject *__pyx_v_self, 
 #if CYTHON_METH_FASTCALL
 PyObject *const *__pyx_args, Py_ssize_t __pyx_nargs, PyObject *__pyx_kwds
 #else
 PyObject *__pyx_args, PyObject *__pyx_kwds
 #endif
@@ -22563,31 +22834,31 @@
       kw_args = __Pyx_NumKwargs_FASTCALL(__pyx_kwds);
       switch (__pyx_nargs) {
         case  0:
         if (likely((values[0] = __Pyx_GetKwValue_FASTCALL(__pyx_kwds, __pyx_kwvalues, __pyx_n_s_X)) != 0)) {
           (void)__Pyx_Arg_NewRef_FASTCALL(values[0]);
           kw_args--;
         }
-        else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 184, __pyx_L3_error)
+        else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 189, __pyx_L3_error)
         else goto __pyx_L5_argtuple_error;
       }
       if (unlikely(kw_args > 0)) {
         const Py_ssize_t kwd_pos_args = __pyx_nargs;
-        if (unlikely(__Pyx_ParseOptionalKeywords(__pyx_kwds, __pyx_kwvalues, __pyx_pyargnames, 0, values + 0, kwd_pos_args, "check_rule") < 0)) __PYX_ERR(0, 184, __pyx_L3_error)
+        if (unlikely(__Pyx_ParseOptionalKeywords(__pyx_kwds, __pyx_kwvalues, __pyx_pyargnames, 0, values + 0, kwd_pos_args, "check_rule") < 0)) __PYX_ERR(0, 189, __pyx_L3_error)
       }
     } else if (unlikely(__pyx_nargs != 1)) {
       goto __pyx_L5_argtuple_error;
     } else {
       values[0] = __Pyx_Arg_FASTCALL(__pyx_args, 0);
     }
-    __pyx_v_X = __Pyx_PyObject_to_MemoryviewSlice_ds_float(values[0], PyBUF_WRITABLE); if (unlikely(!__pyx_v_X.memview)) __PYX_ERR(0, 184, __pyx_L3_error)
+    __pyx_v_X = __Pyx_PyObject_to_MemoryviewSlice_dsds_float(values[0], PyBUF_WRITABLE); if (unlikely(!__pyx_v_X.memview)) __PYX_ERR(0, 189, __pyx_L3_error)
   }
   goto __pyx_L6_skip;
   __pyx_L5_argtuple_error:;
-  __Pyx_RaiseArgtupleInvalid("check_rule", 1, 1, 1, __pyx_nargs); __PYX_ERR(0, 184, __pyx_L3_error)
+  __Pyx_RaiseArgtupleInvalid("check_rule", 1, 1, 1, __pyx_nargs); __PYX_ERR(0, 189, __pyx_L3_error)
   __pyx_L6_skip:;
   goto __pyx_L4_argument_unpacking_done;
   __pyx_L3_error:;
   {
     Py_ssize_t __pyx_temp;
     for (__pyx_temp=0; __pyx_temp < (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {
       __Pyx_Arg_XDECREF_FASTCALL(values[__pyx_temp]);
@@ -22611,42 +22882,40 @@
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
 static PyObject *__pyx_pf_7ruleopt_11aux_classes_11aux_classes_4Rule_12check_rule(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *__pyx_v_self, __Pyx_memviewslice __pyx_v_X) {
   PyObject *__pyx_r = NULL;
   __Pyx_RefNannyDeclarations
-  int __pyx_t_1;
-  PyObject *__pyx_t_2 = NULL;
+  PyObject *__pyx_t_1 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("check_rule", 1);
   __Pyx_XDECREF(__pyx_r);
-  if (unlikely(!__pyx_v_X.memview)) { __Pyx_RaiseUnboundLocalError("X"); __PYX_ERR(0, 184, __pyx_L1_error) }
-  __pyx_t_1 = __pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule_check_rule(__pyx_v_self, __pyx_v_X, 1); if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 184, __pyx_L1_error)
-  __pyx_t_2 = __Pyx_PyBool_FromLong(__pyx_t_1); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 184, __pyx_L1_error)
-  __Pyx_GOTREF(__pyx_t_2);
-  __pyx_r = __pyx_t_2;
-  __pyx_t_2 = 0;
+  if (unlikely(!__pyx_v_X.memview)) { __Pyx_RaiseUnboundLocalError("X"); __PYX_ERR(0, 189, __pyx_L1_error) }
+  __pyx_t_1 = ((PyObject *)__pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule_check_rule(__pyx_v_self, __pyx_v_X, 1)); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 189, __pyx_L1_error)
+  __Pyx_GOTREF(__pyx_t_1);
+  __pyx_r = __pyx_t_1;
+  __pyx_t_1 = 0;
   goto __pyx_L0;
 
   /* function exit code */
   __pyx_L1_error:;
-  __Pyx_XDECREF(__pyx_t_2);
+  __Pyx_XDECREF(__pyx_t_1);
   __Pyx_AddTraceback("ruleopt.aux_classes.aux_classes.Rule.check_rule", __pyx_clineno, __pyx_lineno, __pyx_filename);
   __pyx_r = NULL;
   __pyx_L0:;
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "ruleopt/aux_classes/aux_classes.pyx":200
- *         return self._check_rule_nogil(X)
+/* "ruleopt/aux_classes/aux_classes.pyx":209
+ * 
  * 
  *     def to_text(self, feature_names=None):             # <<<<<<<<<<<<<<
  *         """
  *         Converts the rule to a human-readable text representation, using optional
  */
 
 /* Python wrapper */
@@ -22699,34 +22968,34 @@
       }
       kw_args = __Pyx_NumKwargs_FASTCALL(__pyx_kwds);
       switch (__pyx_nargs) {
         case  0:
         if (kw_args > 0) {
           PyObject* value = __Pyx_GetKwValue_FASTCALL(__pyx_kwds, __pyx_kwvalues, __pyx_n_s_feature_names);
           if (value) { values[0] = __Pyx_Arg_NewRef_FASTCALL(value); kw_args--; }
-          else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 200, __pyx_L3_error)
+          else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 209, __pyx_L3_error)
         }
       }
       if (unlikely(kw_args > 0)) {
         const Py_ssize_t kwd_pos_args = __pyx_nargs;
-        if (unlikely(__Pyx_ParseOptionalKeywords(__pyx_kwds, __pyx_kwvalues, __pyx_pyargnames, 0, values + 0, kwd_pos_args, "to_text") < 0)) __PYX_ERR(0, 200, __pyx_L3_error)
+        if (unlikely(__Pyx_ParseOptionalKeywords(__pyx_kwds, __pyx_kwvalues, __pyx_pyargnames, 0, values + 0, kwd_pos_args, "to_text") < 0)) __PYX_ERR(0, 209, __pyx_L3_error)
       }
     } else {
       switch (__pyx_nargs) {
         case  1: values[0] = __Pyx_Arg_FASTCALL(__pyx_args, 0);
         CYTHON_FALLTHROUGH;
         case  0: break;
         default: goto __pyx_L5_argtuple_error;
       }
     }
     __pyx_v_feature_names = values[0];
   }
   goto __pyx_L6_skip;
   __pyx_L5_argtuple_error:;
-  __Pyx_RaiseArgtupleInvalid("to_text", 0, 0, 1, __pyx_nargs); __PYX_ERR(0, 200, __pyx_L3_error)
+  __Pyx_RaiseArgtupleInvalid("to_text", 0, 0, 1, __pyx_nargs); __PYX_ERR(0, 209, __pyx_L3_error)
   __pyx_L6_skip:;
   goto __pyx_L4_argument_unpacking_done;
   __pyx_L3_error:;
   {
     Py_ssize_t __pyx_temp;
     for (__pyx_temp=0; __pyx_temp < (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {
       __Pyx_Arg_XDECREF_FASTCALL(values[__pyx_temp]);
@@ -22766,94 +23035,94 @@
   Py_UCS4 __pyx_t_8;
   PyObject *__pyx_t_9 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("to_text", 1);
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":216
+  /* "ruleopt/aux_classes/aux_classes.pyx":225
  *             A human-readable string representation of the rule.
  *         """
  *         print_text = ""             # <<<<<<<<<<<<<<
  *         for i in range(self.n_clauses):
  *             feature_label = (f"x[{self.clauses[i].feature}]"
  */
   __Pyx_INCREF(__pyx_kp_u__15);
   __pyx_v_print_text = __pyx_kp_u__15;
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":217
+  /* "ruleopt/aux_classes/aux_classes.pyx":226
  *         """
  *         print_text = ""
  *         for i in range(self.n_clauses):             # <<<<<<<<<<<<<<
  *             feature_label = (f"x[{self.clauses[i].feature}]"
  *                              if feature_names is None
  */
   __pyx_t_1 = __pyx_v_self->n_clauses;
   __pyx_t_2 = __pyx_t_1;
   for (__pyx_t_3 = 0; __pyx_t_3 < __pyx_t_2; __pyx_t_3+=1) {
     __pyx_v_i = __pyx_t_3;
 
-    /* "ruleopt/aux_classes/aux_classes.pyx":219
+    /* "ruleopt/aux_classes/aux_classes.pyx":228
  *         for i in range(self.n_clauses):
  *             feature_label = (f"x[{self.clauses[i].feature}]"
  *                              if feature_names is None             # <<<<<<<<<<<<<<
  *                              else feature_names[self.clauses[i].feature])
  *             na_string = " or null" if self.clauses[i].na else " and not null"
  */
     __pyx_t_5 = (__pyx_v_feature_names == Py_None);
     if (__pyx_t_5) {
 
-      /* "ruleopt/aux_classes/aux_classes.pyx":218
+      /* "ruleopt/aux_classes/aux_classes.pyx":227
  *         print_text = ""
  *         for i in range(self.n_clauses):
  *             feature_label = (f"x[{self.clauses[i].feature}]"             # <<<<<<<<<<<<<<
  *                              if feature_names is None
  *                              else feature_names[self.clauses[i].feature])
  */
-      __pyx_t_6 = PyTuple_New(3); if (unlikely(!__pyx_t_6)) __PYX_ERR(0, 218, __pyx_L1_error)
+      __pyx_t_6 = PyTuple_New(3); if (unlikely(!__pyx_t_6)) __PYX_ERR(0, 227, __pyx_L1_error)
       __Pyx_GOTREF(__pyx_t_6);
       __pyx_t_7 = 0;
       __pyx_t_8 = 127;
       __Pyx_INCREF(__pyx_kp_u_x);
       __pyx_t_7 += 2;
       __Pyx_GIVEREF(__pyx_kp_u_x);
       PyTuple_SET_ITEM(__pyx_t_6, 0, __pyx_kp_u_x);
-      __pyx_t_9 = __Pyx_PyUnicode_From_int((__pyx_v_self->clauses[__pyx_v_i]).feature, 0, ' ', 'd'); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 218, __pyx_L1_error)
+      __pyx_t_9 = __Pyx_PyUnicode_From_int((__pyx_v_self->clauses[__pyx_v_i]).feature, 0, ' ', 'd'); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 227, __pyx_L1_error)
       __Pyx_GOTREF(__pyx_t_9);
       __pyx_t_7 += __Pyx_PyUnicode_GET_LENGTH(__pyx_t_9);
       __Pyx_GIVEREF(__pyx_t_9);
       PyTuple_SET_ITEM(__pyx_t_6, 1, __pyx_t_9);
       __pyx_t_9 = 0;
       __Pyx_INCREF(__pyx_kp_u__16);
       __pyx_t_7 += 1;
       __Pyx_GIVEREF(__pyx_kp_u__16);
       PyTuple_SET_ITEM(__pyx_t_6, 2, __pyx_kp_u__16);
-      __pyx_t_9 = __Pyx_PyUnicode_Join(__pyx_t_6, 3, __pyx_t_7, __pyx_t_8); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 218, __pyx_L1_error)
+      __pyx_t_9 = __Pyx_PyUnicode_Join(__pyx_t_6, 3, __pyx_t_7, __pyx_t_8); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 227, __pyx_L1_error)
       __Pyx_GOTREF(__pyx_t_9);
       __Pyx_DECREF(__pyx_t_6); __pyx_t_6 = 0;
       __pyx_t_4 = __pyx_t_9;
       __pyx_t_9 = 0;
     } else {
 
-      /* "ruleopt/aux_classes/aux_classes.pyx":220
+      /* "ruleopt/aux_classes/aux_classes.pyx":229
  *             feature_label = (f"x[{self.clauses[i].feature}]"
  *                              if feature_names is None
  *                              else feature_names[self.clauses[i].feature])             # <<<<<<<<<<<<<<
  *             na_string = " or null" if self.clauses[i].na else " and not null"
  *             print_text += f"{self.clauses[i].lb:<9.2f} < {feature_label:<9} <= {self.clauses[i].ub:<9.2f}{na_string}\n"
  */
-      __pyx_t_9 = __Pyx_GetItemInt(__pyx_v_feature_names, (__pyx_v_self->clauses[__pyx_v_i]).feature, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 220, __pyx_L1_error)
+      __pyx_t_9 = __Pyx_GetItemInt(__pyx_v_feature_names, (__pyx_v_self->clauses[__pyx_v_i]).feature, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 229, __pyx_L1_error)
       __Pyx_GOTREF(__pyx_t_9);
       __pyx_t_4 = __pyx_t_9;
       __pyx_t_9 = 0;
     }
     __Pyx_XDECREF_SET(__pyx_v_feature_label, __pyx_t_4);
     __pyx_t_4 = 0;
 
-    /* "ruleopt/aux_classes/aux_classes.pyx":221
+    /* "ruleopt/aux_classes/aux_classes.pyx":230
  *                              if feature_names is None
  *                              else feature_names[self.clauses[i].feature])
  *             na_string = " or null" if self.clauses[i].na else " and not null"             # <<<<<<<<<<<<<<
  *             print_text += f"{self.clauses[i].lb:<9.2f} < {feature_label:<9} <= {self.clauses[i].ub:<9.2f}{na_string}\n"
  *         return print_text.rstrip("\n")
  */
     if ((__pyx_v_self->clauses[__pyx_v_i]).na) {
@@ -22862,97 +23131,97 @@
     } else {
       __Pyx_INCREF(__pyx_kp_u_and_not_null);
       __pyx_t_4 = __pyx_kp_u_and_not_null;
     }
     __Pyx_XDECREF_SET(__pyx_v_na_string, ((PyObject*)__pyx_t_4));
     __pyx_t_4 = 0;
 
-    /* "ruleopt/aux_classes/aux_classes.pyx":222
+    /* "ruleopt/aux_classes/aux_classes.pyx":231
  *                              else feature_names[self.clauses[i].feature])
  *             na_string = " or null" if self.clauses[i].na else " and not null"
  *             print_text += f"{self.clauses[i].lb:<9.2f} < {feature_label:<9} <= {self.clauses[i].ub:<9.2f}{na_string}\n"             # <<<<<<<<<<<<<<
  *         return print_text.rstrip("\n")
  * 
  */
-    __pyx_t_4 = PyTuple_New(7); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 222, __pyx_L1_error)
+    __pyx_t_4 = PyTuple_New(7); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 231, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_4);
     __pyx_t_7 = 0;
     __pyx_t_8 = 127;
-    __pyx_t_9 = PyFloat_FromDouble((__pyx_v_self->clauses[__pyx_v_i]).lb); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 222, __pyx_L1_error)
+    __pyx_t_9 = PyFloat_FromDouble((__pyx_v_self->clauses[__pyx_v_i]).lb); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 231, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_9);
-    __pyx_t_6 = __Pyx_PyObject_Format(__pyx_t_9, __pyx_kp_u_9_2f); if (unlikely(!__pyx_t_6)) __PYX_ERR(0, 222, __pyx_L1_error)
+    __pyx_t_6 = __Pyx_PyObject_Format(__pyx_t_9, __pyx_kp_u_9_2f); if (unlikely(!__pyx_t_6)) __PYX_ERR(0, 231, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_6);
     __Pyx_DECREF(__pyx_t_9); __pyx_t_9 = 0;
     __pyx_t_8 = (__Pyx_PyUnicode_MAX_CHAR_VALUE(__pyx_t_6) > __pyx_t_8) ? __Pyx_PyUnicode_MAX_CHAR_VALUE(__pyx_t_6) : __pyx_t_8;
     __pyx_t_7 += __Pyx_PyUnicode_GET_LENGTH(__pyx_t_6);
     __Pyx_GIVEREF(__pyx_t_6);
     PyTuple_SET_ITEM(__pyx_t_4, 0, __pyx_t_6);
     __pyx_t_6 = 0;
     __Pyx_INCREF(__pyx_kp_u__17);
     __pyx_t_7 += 3;
     __Pyx_GIVEREF(__pyx_kp_u__17);
     PyTuple_SET_ITEM(__pyx_t_4, 1, __pyx_kp_u__17);
-    __pyx_t_6 = __Pyx_PyObject_Format(__pyx_v_feature_label, __pyx_kp_u_9); if (unlikely(!__pyx_t_6)) __PYX_ERR(0, 222, __pyx_L1_error)
+    __pyx_t_6 = __Pyx_PyObject_Format(__pyx_v_feature_label, __pyx_kp_u_9); if (unlikely(!__pyx_t_6)) __PYX_ERR(0, 231, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_6);
     __pyx_t_8 = (__Pyx_PyUnicode_MAX_CHAR_VALUE(__pyx_t_6) > __pyx_t_8) ? __Pyx_PyUnicode_MAX_CHAR_VALUE(__pyx_t_6) : __pyx_t_8;
     __pyx_t_7 += __Pyx_PyUnicode_GET_LENGTH(__pyx_t_6);
     __Pyx_GIVEREF(__pyx_t_6);
     PyTuple_SET_ITEM(__pyx_t_4, 2, __pyx_t_6);
     __pyx_t_6 = 0;
     __Pyx_INCREF(__pyx_kp_u__18);
     __pyx_t_7 += 4;
     __Pyx_GIVEREF(__pyx_kp_u__18);
     PyTuple_SET_ITEM(__pyx_t_4, 3, __pyx_kp_u__18);
-    __pyx_t_6 = PyFloat_FromDouble((__pyx_v_self->clauses[__pyx_v_i]).ub); if (unlikely(!__pyx_t_6)) __PYX_ERR(0, 222, __pyx_L1_error)
+    __pyx_t_6 = PyFloat_FromDouble((__pyx_v_self->clauses[__pyx_v_i]).ub); if (unlikely(!__pyx_t_6)) __PYX_ERR(0, 231, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_6);
-    __pyx_t_9 = __Pyx_PyObject_Format(__pyx_t_6, __pyx_kp_u_9_2f); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 222, __pyx_L1_error)
+    __pyx_t_9 = __Pyx_PyObject_Format(__pyx_t_6, __pyx_kp_u_9_2f); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 231, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_9);
     __Pyx_DECREF(__pyx_t_6); __pyx_t_6 = 0;
     __pyx_t_8 = (__Pyx_PyUnicode_MAX_CHAR_VALUE(__pyx_t_9) > __pyx_t_8) ? __Pyx_PyUnicode_MAX_CHAR_VALUE(__pyx_t_9) : __pyx_t_8;
     __pyx_t_7 += __Pyx_PyUnicode_GET_LENGTH(__pyx_t_9);
     __Pyx_GIVEREF(__pyx_t_9);
     PyTuple_SET_ITEM(__pyx_t_4, 4, __pyx_t_9);
     __pyx_t_9 = 0;
-    __pyx_t_9 = __Pyx_PyUnicode_Unicode(__pyx_v_na_string); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 222, __pyx_L1_error)
+    __pyx_t_9 = __Pyx_PyUnicode_Unicode(__pyx_v_na_string); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 231, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_9);
     __pyx_t_8 = (__Pyx_PyUnicode_MAX_CHAR_VALUE(__pyx_t_9) > __pyx_t_8) ? __Pyx_PyUnicode_MAX_CHAR_VALUE(__pyx_t_9) : __pyx_t_8;
     __pyx_t_7 += __Pyx_PyUnicode_GET_LENGTH(__pyx_t_9);
     __Pyx_GIVEREF(__pyx_t_9);
     PyTuple_SET_ITEM(__pyx_t_4, 5, __pyx_t_9);
     __pyx_t_9 = 0;
     __Pyx_INCREF(__pyx_kp_u__19);
     __pyx_t_7 += 1;
     __Pyx_GIVEREF(__pyx_kp_u__19);
     PyTuple_SET_ITEM(__pyx_t_4, 6, __pyx_kp_u__19);
-    __pyx_t_9 = __Pyx_PyUnicode_Join(__pyx_t_4, 7, __pyx_t_7, __pyx_t_8); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 222, __pyx_L1_error)
+    __pyx_t_9 = __Pyx_PyUnicode_Join(__pyx_t_4, 7, __pyx_t_7, __pyx_t_8); if (unlikely(!__pyx_t_9)) __PYX_ERR(0, 231, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_9);
     __Pyx_DECREF(__pyx_t_4); __pyx_t_4 = 0;
-    __pyx_t_4 = __Pyx_PyUnicode_ConcatInPlace(__pyx_v_print_text, __pyx_t_9); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 222, __pyx_L1_error)
+    __pyx_t_4 = __Pyx_PyUnicode_ConcatInPlace(__pyx_v_print_text, __pyx_t_9); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 231, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_4);
     __Pyx_DECREF(__pyx_t_9); __pyx_t_9 = 0;
     __Pyx_DECREF_SET(__pyx_v_print_text, ((PyObject*)__pyx_t_4));
     __pyx_t_4 = 0;
   }
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":223
+  /* "ruleopt/aux_classes/aux_classes.pyx":232
  *             na_string = " or null" if self.clauses[i].na else " and not null"
  *             print_text += f"{self.clauses[i].lb:<9.2f} < {feature_label:<9} <= {self.clauses[i].ub:<9.2f}{na_string}\n"
  *         return print_text.rstrip("\n")             # <<<<<<<<<<<<<<
  * 
  *     def to_dict(self, feature_names=None):
  */
   __Pyx_XDECREF(__pyx_r);
-  __pyx_t_4 = __Pyx_CallUnboundCMethod1(&__pyx_umethod_PyUnicode_Type_rstrip, __pyx_v_print_text, __pyx_kp_u__19); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 223, __pyx_L1_error)
+  __pyx_t_4 = __Pyx_CallUnboundCMethod1(&__pyx_umethod_PyUnicode_Type_rstrip, __pyx_v_print_text, __pyx_kp_u__19); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 232, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_4);
   __pyx_r = __pyx_t_4;
   __pyx_t_4 = 0;
   goto __pyx_L0;
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":200
- *         return self._check_rule_nogil(X)
+  /* "ruleopt/aux_classes/aux_classes.pyx":209
+ * 
  * 
  *     def to_text(self, feature_names=None):             # <<<<<<<<<<<<<<
  *         """
  *         Converts the rule to a human-readable text representation, using optional
  */
 
   /* function exit code */
@@ -22967,15 +23236,15 @@
   __Pyx_XDECREF(__pyx_v_feature_label);
   __Pyx_XDECREF(__pyx_v_na_string);
   __Pyx_XGIVEREF(__pyx_r);
   __Pyx_RefNannyFinishContext();
   return __pyx_r;
 }
 
-/* "ruleopt/aux_classes/aux_classes.pyx":225
+/* "ruleopt/aux_classes/aux_classes.pyx":234
  *         return print_text.rstrip("\n")
  * 
  *     def to_dict(self, feature_names=None):             # <<<<<<<<<<<<<<
  *         """
  *         Converts the rule to a dictionary representation, using optional feature names for keys.
  */
 
@@ -23029,34 +23298,34 @@
       }
       kw_args = __Pyx_NumKwargs_FASTCALL(__pyx_kwds);
       switch (__pyx_nargs) {
         case  0:
         if (kw_args > 0) {
           PyObject* value = __Pyx_GetKwValue_FASTCALL(__pyx_kwds, __pyx_kwvalues, __pyx_n_s_feature_names);
           if (value) { values[0] = __Pyx_Arg_NewRef_FASTCALL(value); kw_args--; }
-          else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 225, __pyx_L3_error)
+          else if (unlikely(PyErr_Occurred())) __PYX_ERR(0, 234, __pyx_L3_error)
         }
       }
       if (unlikely(kw_args > 0)) {
         const Py_ssize_t kwd_pos_args = __pyx_nargs;
-        if (unlikely(__Pyx_ParseOptionalKeywords(__pyx_kwds, __pyx_kwvalues, __pyx_pyargnames, 0, values + 0, kwd_pos_args, "to_dict") < 0)) __PYX_ERR(0, 225, __pyx_L3_error)
+        if (unlikely(__Pyx_ParseOptionalKeywords(__pyx_kwds, __pyx_kwvalues, __pyx_pyargnames, 0, values + 0, kwd_pos_args, "to_dict") < 0)) __PYX_ERR(0, 234, __pyx_L3_error)
       }
     } else {
       switch (__pyx_nargs) {
         case  1: values[0] = __Pyx_Arg_FASTCALL(__pyx_args, 0);
         CYTHON_FALLTHROUGH;
         case  0: break;
         default: goto __pyx_L5_argtuple_error;
       }
     }
     __pyx_v_feature_names = values[0];
   }
   goto __pyx_L6_skip;
   __pyx_L5_argtuple_error:;
-  __Pyx_RaiseArgtupleInvalid("to_dict", 0, 0, 1, __pyx_nargs); __PYX_ERR(0, 225, __pyx_L3_error)
+  __Pyx_RaiseArgtupleInvalid("to_dict", 0, 0, 1, __pyx_nargs); __PYX_ERR(0, 234, __pyx_L3_error)
   __pyx_L6_skip:;
   goto __pyx_L4_argument_unpacking_done;
   __pyx_L3_error:;
   {
     Py_ssize_t __pyx_temp;
     for (__pyx_temp=0; __pyx_temp < (Py_ssize_t)(sizeof(values)/sizeof(values[0])); ++__pyx_temp) {
       __Pyx_Arg_XDECREF_FASTCALL(values[__pyx_temp]);
@@ -23092,88 +23361,88 @@
   PyObject *__pyx_t_7 = NULL;
   PyObject *__pyx_t_8 = NULL;
   int __pyx_lineno = 0;
   const char *__pyx_filename = NULL;
   int __pyx_clineno = 0;
   __Pyx_RefNannySetupContext("to_dict", 1);
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":239
+  /* "ruleopt/aux_classes/aux_classes.pyx":248
  *             A dictionary representation of the rule, with feature names or indices as keys and clause details as values.
  *         """
  *         return {             # <<<<<<<<<<<<<<
  *             (feature_names[self.clauses[i].feature] if feature_names else self.clauses[i].feature):
  *             {"lb": self.clauses[i].lb, "ub": self.clauses[i].ub, "na": self.clauses[i].na}
  */
   __Pyx_XDECREF(__pyx_r);
   { /* enter inner scope */
-    __pyx_t_1 = PyDict_New(); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 239, __pyx_L1_error)
+    __pyx_t_1 = PyDict_New(); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 248, __pyx_L1_error)
     __Pyx_GOTREF(__pyx_t_1);
 
-    /* "ruleopt/aux_classes/aux_classes.pyx":242
+    /* "ruleopt/aux_classes/aux_classes.pyx":251
  *             (feature_names[self.clauses[i].feature] if feature_names else self.clauses[i].feature):
  *             {"lb": self.clauses[i].lb, "ub": self.clauses[i].ub, "na": self.clauses[i].na}
  *             for i in range(self.n_clauses)             # <<<<<<<<<<<<<<
  *         }
  */
     __pyx_t_2 = __pyx_v_self->n_clauses;
     __pyx_t_3 = __pyx_t_2;
     for (__pyx_t_4 = 0; __pyx_t_4 < __pyx_t_3; __pyx_t_4+=1) {
       __pyx_8genexpr2__pyx_v_i = __pyx_t_4;
 
-      /* "ruleopt/aux_classes/aux_classes.pyx":240
+      /* "ruleopt/aux_classes/aux_classes.pyx":249
  *         """
  *         return {
  *             (feature_names[self.clauses[i].feature] if feature_names else self.clauses[i].feature):             # <<<<<<<<<<<<<<
  *             {"lb": self.clauses[i].lb, "ub": self.clauses[i].ub, "na": self.clauses[i].na}
  *             for i in range(self.n_clauses)
  */
-      __pyx_t_6 = __Pyx_PyObject_IsTrue(__pyx_v_feature_names); if (unlikely((__pyx_t_6 < 0))) __PYX_ERR(0, 240, __pyx_L1_error)
+      __pyx_t_6 = __Pyx_PyObject_IsTrue(__pyx_v_feature_names); if (unlikely((__pyx_t_6 < 0))) __PYX_ERR(0, 249, __pyx_L1_error)
       if (__pyx_t_6) {
-        __pyx_t_7 = __Pyx_GetItemInt(__pyx_v_feature_names, (__pyx_v_self->clauses[__pyx_8genexpr2__pyx_v_i]).feature, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 240, __pyx_L1_error)
+        __pyx_t_7 = __Pyx_GetItemInt(__pyx_v_feature_names, (__pyx_v_self->clauses[__pyx_8genexpr2__pyx_v_i]).feature, int, 1, __Pyx_PyInt_From_int, 0, 1, 1); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 249, __pyx_L1_error)
         __Pyx_GOTREF(__pyx_t_7);
         __pyx_t_5 = __pyx_t_7;
         __pyx_t_7 = 0;
       } else {
-        __pyx_t_7 = __Pyx_PyInt_From_int((__pyx_v_self->clauses[__pyx_8genexpr2__pyx_v_i]).feature); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 240, __pyx_L1_error)
+        __pyx_t_7 = __Pyx_PyInt_From_int((__pyx_v_self->clauses[__pyx_8genexpr2__pyx_v_i]).feature); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 249, __pyx_L1_error)
         __Pyx_GOTREF(__pyx_t_7);
         __pyx_t_5 = __pyx_t_7;
         __pyx_t_7 = 0;
       }
 
-      /* "ruleopt/aux_classes/aux_classes.pyx":241
+      /* "ruleopt/aux_classes/aux_classes.pyx":250
  *         return {
  *             (feature_names[self.clauses[i].feature] if feature_names else self.clauses[i].feature):
  *             {"lb": self.clauses[i].lb, "ub": self.clauses[i].ub, "na": self.clauses[i].na}             # <<<<<<<<<<<<<<
  *             for i in range(self.n_clauses)
  *         }
  */
-      __pyx_t_7 = __Pyx_PyDict_NewPresized(3); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 241, __pyx_L1_error)
+      __pyx_t_7 = __Pyx_PyDict_NewPresized(3); if (unlikely(!__pyx_t_7)) __PYX_ERR(0, 250, __pyx_L1_error)
       __Pyx_GOTREF(__pyx_t_7);
-      __pyx_t_8 = PyFloat_FromDouble((__pyx_v_self->clauses[__pyx_8genexpr2__pyx_v_i]).lb); if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 241, __pyx_L1_error)
+      __pyx_t_8 = PyFloat_FromDouble((__pyx_v_self->clauses[__pyx_8genexpr2__pyx_v_i]).lb); if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 250, __pyx_L1_error)
       __Pyx_GOTREF(__pyx_t_8);
-      if (PyDict_SetItem(__pyx_t_7, __pyx_n_u_lb, __pyx_t_8) < 0) __PYX_ERR(0, 241, __pyx_L1_error)
+      if (PyDict_SetItem(__pyx_t_7, __pyx_n_u_lb, __pyx_t_8) < 0) __PYX_ERR(0, 250, __pyx_L1_error)
       __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
-      __pyx_t_8 = PyFloat_FromDouble((__pyx_v_self->clauses[__pyx_8genexpr2__pyx_v_i]).ub); if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 241, __pyx_L1_error)
+      __pyx_t_8 = PyFloat_FromDouble((__pyx_v_self->clauses[__pyx_8genexpr2__pyx_v_i]).ub); if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 250, __pyx_L1_error)
       __Pyx_GOTREF(__pyx_t_8);
-      if (PyDict_SetItem(__pyx_t_7, __pyx_n_u_ub, __pyx_t_8) < 0) __PYX_ERR(0, 241, __pyx_L1_error)
+      if (PyDict_SetItem(__pyx_t_7, __pyx_n_u_ub, __pyx_t_8) < 0) __PYX_ERR(0, 250, __pyx_L1_error)
       __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
-      __pyx_t_8 = __Pyx_PyBool_FromLong((__pyx_v_self->clauses[__pyx_8genexpr2__pyx_v_i]).na); if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 241, __pyx_L1_error)
+      __pyx_t_8 = __Pyx_PyBool_FromLong((__pyx_v_self->clauses[__pyx_8genexpr2__pyx_v_i]).na); if (unlikely(!__pyx_t_8)) __PYX_ERR(0, 250, __pyx_L1_error)
       __Pyx_GOTREF(__pyx_t_8);
-      if (PyDict_SetItem(__pyx_t_7, __pyx_n_u_na, __pyx_t_8) < 0) __PYX_ERR(0, 241, __pyx_L1_error)
+      if (PyDict_SetItem(__pyx_t_7, __pyx_n_u_na, __pyx_t_8) < 0) __PYX_ERR(0, 250, __pyx_L1_error)
       __Pyx_DECREF(__pyx_t_8); __pyx_t_8 = 0;
-      if (unlikely(PyDict_SetItem(__pyx_t_1, (PyObject*)__pyx_t_5, (PyObject*)__pyx_t_7))) __PYX_ERR(0, 240, __pyx_L1_error)
+      if (unlikely(PyDict_SetItem(__pyx_t_1, (PyObject*)__pyx_t_5, (PyObject*)__pyx_t_7))) __PYX_ERR(0, 249, __pyx_L1_error)
       __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
       __Pyx_DECREF(__pyx_t_7); __pyx_t_7 = 0;
     }
   } /* exit inner scope */
   __pyx_r = __pyx_t_1;
   __pyx_t_1 = 0;
   goto __pyx_L0;
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":225
+  /* "ruleopt/aux_classes/aux_classes.pyx":234
  *         return print_text.rstrip("\n")
  * 
  *     def to_dict(self, feature_names=None):             # <<<<<<<<<<<<<<
  *         """
  *         Converts the rule to a dictionary representation, using optional feature names for keys.
  */
 
@@ -25824,14 +26093,15 @@
     {&__pyx_kp_u__6, __pyx_k__6, sizeof(__pyx_k__6), 0, 1, 0, 0},
     {&__pyx_kp_u__7, __pyx_k__7, sizeof(__pyx_k__7), 0, 1, 0, 0},
     {&__pyx_n_s_abc, __pyx_k_abc, sizeof(__pyx_k_abc), 0, 0, 1, 1},
     {&__pyx_n_s_add_clause, __pyx_k_add_clause, sizeof(__pyx_k_add_clause), 0, 0, 1, 1},
     {&__pyx_n_s_allocate_buffer, __pyx_k_allocate_buffer, sizeof(__pyx_k_allocate_buffer), 0, 0, 1, 1},
     {&__pyx_kp_u_and, __pyx_k_and, sizeof(__pyx_k_and), 0, 1, 0, 0},
     {&__pyx_kp_u_and_not_null, __pyx_k_and_not_null, sizeof(__pyx_k_and_not_null), 0, 1, 0, 0},
+    {&__pyx_n_s_asarray, __pyx_k_asarray, sizeof(__pyx_k_asarray), 0, 0, 1, 1},
     {&__pyx_n_s_asyncio_coroutines, __pyx_k_asyncio_coroutines, sizeof(__pyx_k_asyncio_coroutines), 0, 0, 1, 1},
     {&__pyx_n_s_base, __pyx_k_base, sizeof(__pyx_k_base), 0, 0, 1, 1},
     {&__pyx_n_s_c, __pyx_k_c, sizeof(__pyx_k_c), 0, 0, 1, 1},
     {&__pyx_n_u_c, __pyx_k_c, sizeof(__pyx_k_c), 0, 1, 0, 1},
     {&__pyx_n_s_check_rule, __pyx_k_check_rule, sizeof(__pyx_k_check_rule), 0, 0, 1, 1},
     {&__pyx_n_s_class, __pyx_k_class, sizeof(__pyx_k_class), 0, 0, 1, 1},
     {&__pyx_n_s_class_getitem, __pyx_k_class_getitem, sizeof(__pyx_k_class_getitem), 0, 0, 1, 1},
@@ -25868,14 +26138,15 @@
     {&__pyx_kp_u_got, __pyx_k_got, sizeof(__pyx_k_got), 0, 1, 0, 0},
     {&__pyx_kp_u_got_differing_extents_in_dimensi, __pyx_k_got_differing_extents_in_dimensi, sizeof(__pyx_k_got_differing_extents_in_dimensi), 0, 1, 0, 0},
     {&__pyx_n_s_i, __pyx_k_i, sizeof(__pyx_k_i), 0, 0, 1, 1},
     {&__pyx_n_s_id, __pyx_k_id, sizeof(__pyx_k_id), 0, 0, 1, 1},
     {&__pyx_n_s_import, __pyx_k_import, sizeof(__pyx_k_import), 0, 0, 1, 1},
     {&__pyx_n_s_index, __pyx_k_index, sizeof(__pyx_k_index), 0, 0, 1, 1},
     {&__pyx_n_s_initializing, __pyx_k_initializing, sizeof(__pyx_k_initializing), 0, 0, 1, 1},
+    {&__pyx_n_s_int8, __pyx_k_int8, sizeof(__pyx_k_int8), 0, 0, 1, 1},
     {&__pyx_n_s_intp, __pyx_k_intp, sizeof(__pyx_k_intp), 0, 0, 1, 1},
     {&__pyx_n_s_is_coroutine, __pyx_k_is_coroutine, sizeof(__pyx_k_is_coroutine), 0, 0, 1, 1},
     {&__pyx_kp_u_isenabled, __pyx_k_isenabled, sizeof(__pyx_k_isenabled), 0, 1, 0, 0},
     {&__pyx_n_s_itemsize, __pyx_k_itemsize, sizeof(__pyx_k_itemsize), 0, 0, 1, 1},
     {&__pyx_kp_s_itemsize_0_for_cython_array, __pyx_k_itemsize_0_for_cython_array, sizeof(__pyx_k_itemsize_0_for_cython_array), 0, 0, 1, 0},
     {&__pyx_n_s_lb, __pyx_k_lb, sizeof(__pyx_k_lb), 0, 0, 1, 1},
     {&__pyx_n_u_lb, __pyx_k_lb, sizeof(__pyx_k_lb), 0, 1, 0, 1},
@@ -25934,14 +26205,15 @@
     {&__pyx_n_s_struct, __pyx_k_struct, sizeof(__pyx_k_struct), 0, 0, 1, 1},
     {&__pyx_n_s_sys, __pyx_k_sys, sizeof(__pyx_k_sys), 0, 0, 1, 1},
     {&__pyx_n_s_test, __pyx_k_test, sizeof(__pyx_k_test), 0, 0, 1, 1},
     {&__pyx_n_s_to_dict, __pyx_k_to_dict, sizeof(__pyx_k_to_dict), 0, 0, 1, 1},
     {&__pyx_n_s_to_text, __pyx_k_to_text, sizeof(__pyx_k_to_text), 0, 0, 1, 1},
     {&__pyx_n_s_ub, __pyx_k_ub, sizeof(__pyx_k_ub), 0, 0, 1, 1},
     {&__pyx_n_u_ub, __pyx_k_ub, sizeof(__pyx_k_ub), 0, 1, 0, 1},
+    {&__pyx_n_s_uint8, __pyx_k_uint8, sizeof(__pyx_k_uint8), 0, 0, 1, 1},
     {&__pyx_kp_s_unable_to_allocate_array_data, __pyx_k_unable_to_allocate_array_data, sizeof(__pyx_k_unable_to_allocate_array_data), 0, 0, 1, 0},
     {&__pyx_kp_s_unable_to_allocate_shape_and_str, __pyx_k_unable_to_allocate_shape_and_str, sizeof(__pyx_k_unable_to_allocate_shape_and_str), 0, 0, 1, 0},
     {&__pyx_n_s_unpack, __pyx_k_unpack, sizeof(__pyx_k_unpack), 0, 0, 1, 1},
     {&__pyx_n_s_update, __pyx_k_update, sizeof(__pyx_k_update), 0, 0, 1, 1},
     {&__pyx_n_s_use_setstate, __pyx_k_use_setstate, sizeof(__pyx_k_use_setstate), 0, 0, 1, 1},
     {&__pyx_n_s_version_info, __pyx_k_version_info, sizeof(__pyx_k_version_info), 0, 0, 1, 1},
     {&__pyx_kp_u_x, __pyx_k_x, sizeof(__pyx_k_x), 0, 1, 0, 0},
@@ -26005,26 +26277,26 @@
  *         from pickle import PickleError as __pyx_PickleError
  *         raise __pyx_PickleError, "Incompatible checksums (0x%x vs (0x82a3537, 0x6ae9995, 0xb068931) = (name))" % __pyx_checksum
  */
   __pyx_tuple__8 = PyTuple_Pack(3, __pyx_int_136983863, __pyx_int_112105877, __pyx_int_184977713); if (unlikely(!__pyx_tuple__8)) __PYX_ERR(1, 4, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__8);
   __Pyx_GIVEREF(__pyx_tuple__8);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":984
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":984
  *         __pyx_import_array()
  *     except Exception:
  *         raise ImportError("numpy.core.multiarray failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_umath() except -1:
  */
   __pyx_tuple__9 = PyTuple_Pack(1, __pyx_kp_u_numpy_core_multiarray_failed_to); if (unlikely(!__pyx_tuple__9)) __PYX_ERR(2, 984, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__9);
   __Pyx_GIVEREF(__pyx_tuple__9);
 
-  /* "C:/Users/runneradmin/AppData/Local/Temp/pip-build-env-fqu8v5xm/overlay/Lib/site-packages/numpy/__init__.cython-30.pxd":990
+  /* "../../../../../private/var/folders/62/nb0pn_857qd74kdkfyhcct0c0000gn/T/pip-build-env-qxca1ili/overlay/lib/python3.9/site-packages/numpy/__init__.cython-30.pxd":990
  *         _import_umath()
  *     except Exception:
  *         raise ImportError("numpy.core.umath failed to import")             # <<<<<<<<<<<<<<
  * 
  * cdef inline int import_ufunc() except -1:
  */
   __pyx_tuple__10 = PyTuple_Pack(1, __pyx_kp_u_numpy_core_umath_failed_to_impor); if (unlikely(!__pyx_tuple__10)) __PYX_ERR(2, 990, __pyx_L1_error)
@@ -26196,52 +26468,52 @@
  *         Adds a new clause to the rule or updates an existing clause for the given feature.
  */
   __pyx_tuple__40 = PyTuple_Pack(5, __pyx_n_s_self, __pyx_n_s_feature, __pyx_n_s_ub, __pyx_n_s_lb, __pyx_n_s_na); if (unlikely(!__pyx_tuple__40)) __PYX_ERR(0, 98, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__40);
   __Pyx_GIVEREF(__pyx_tuple__40);
   __pyx_codeobj__41 = (PyObject*)__Pyx_PyCode_New(5, 0, 0, 5, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__40, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_ruleopt_aux_classes_aux_classes, __pyx_n_s_add_clause, 98, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__41)) __PYX_ERR(0, 98, __pyx_L1_error)
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":184
- *         return clause.lb < val <= clause.ub
+  /* "ruleopt/aux_classes/aux_classes.pyx":189
  * 
- *     cpdef bint check_rule(self, float[:] X):             # <<<<<<<<<<<<<<
+ * 
+ *     cpdef cnp.ndarray[cnp.uint8_t, ndim=1] check_rule(self, float[:, :] X):             # <<<<<<<<<<<<<<
  *         """
  *         Checks if the rule applies to the given feature values.
  */
-  __pyx_tuple__42 = PyTuple_Pack(2, __pyx_n_s_self, __pyx_n_s_X); if (unlikely(!__pyx_tuple__42)) __PYX_ERR(0, 184, __pyx_L1_error)
+  __pyx_tuple__42 = PyTuple_Pack(2, __pyx_n_s_self, __pyx_n_s_X); if (unlikely(!__pyx_tuple__42)) __PYX_ERR(0, 189, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__42);
   __Pyx_GIVEREF(__pyx_tuple__42);
-  __pyx_codeobj__43 = (PyObject*)__Pyx_PyCode_New(2, 0, 0, 2, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__42, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_ruleopt_aux_classes_aux_classes, __pyx_n_s_check_rule, 184, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__43)) __PYX_ERR(0, 184, __pyx_L1_error)
+  __pyx_codeobj__43 = (PyObject*)__Pyx_PyCode_New(2, 0, 0, 2, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__42, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_ruleopt_aux_classes_aux_classes, __pyx_n_s_check_rule, 189, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__43)) __PYX_ERR(0, 189, __pyx_L1_error)
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":200
- *         return self._check_rule_nogil(X)
+  /* "ruleopt/aux_classes/aux_classes.pyx":209
+ * 
  * 
  *     def to_text(self, feature_names=None):             # <<<<<<<<<<<<<<
  *         """
  *         Converts the rule to a human-readable text representation, using optional
  */
-  __pyx_tuple__44 = PyTuple_Pack(6, __pyx_n_s_self, __pyx_n_s_feature_names, __pyx_n_s_print_text, __pyx_n_s_i, __pyx_n_s_feature_label, __pyx_n_s_na_string); if (unlikely(!__pyx_tuple__44)) __PYX_ERR(0, 200, __pyx_L1_error)
+  __pyx_tuple__44 = PyTuple_Pack(6, __pyx_n_s_self, __pyx_n_s_feature_names, __pyx_n_s_print_text, __pyx_n_s_i, __pyx_n_s_feature_label, __pyx_n_s_na_string); if (unlikely(!__pyx_tuple__44)) __PYX_ERR(0, 209, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__44);
   __Pyx_GIVEREF(__pyx_tuple__44);
-  __pyx_codeobj__45 = (PyObject*)__Pyx_PyCode_New(2, 0, 0, 6, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__44, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_ruleopt_aux_classes_aux_classes, __pyx_n_s_to_text, 200, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__45)) __PYX_ERR(0, 200, __pyx_L1_error)
-  __pyx_tuple__46 = PyTuple_Pack(1, Py_None); if (unlikely(!__pyx_tuple__46)) __PYX_ERR(0, 200, __pyx_L1_error)
+  __pyx_codeobj__45 = (PyObject*)__Pyx_PyCode_New(2, 0, 0, 6, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__44, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_ruleopt_aux_classes_aux_classes, __pyx_n_s_to_text, 209, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__45)) __PYX_ERR(0, 209, __pyx_L1_error)
+  __pyx_tuple__46 = PyTuple_Pack(1, Py_None); if (unlikely(!__pyx_tuple__46)) __PYX_ERR(0, 209, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__46);
   __Pyx_GIVEREF(__pyx_tuple__46);
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":225
+  /* "ruleopt/aux_classes/aux_classes.pyx":234
  *         return print_text.rstrip("\n")
  * 
  *     def to_dict(self, feature_names=None):             # <<<<<<<<<<<<<<
  *         """
  *         Converts the rule to a dictionary representation, using optional feature names for keys.
  */
-  __pyx_tuple__47 = PyTuple_Pack(3, __pyx_n_s_self, __pyx_n_s_feature_names, __pyx_n_s_i); if (unlikely(!__pyx_tuple__47)) __PYX_ERR(0, 225, __pyx_L1_error)
+  __pyx_tuple__47 = PyTuple_Pack(3, __pyx_n_s_self, __pyx_n_s_feature_names, __pyx_n_s_i); if (unlikely(!__pyx_tuple__47)) __PYX_ERR(0, 234, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_tuple__47);
   __Pyx_GIVEREF(__pyx_tuple__47);
-  __pyx_codeobj__48 = (PyObject*)__Pyx_PyCode_New(2, 0, 0, 3, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__47, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_ruleopt_aux_classes_aux_classes, __pyx_n_s_to_dict, 225, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__48)) __PYX_ERR(0, 225, __pyx_L1_error)
+  __pyx_codeobj__48 = (PyObject*)__Pyx_PyCode_New(2, 0, 0, 3, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__47, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_ruleopt_aux_classes_aux_classes, __pyx_n_s_to_dict, 234, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__48)) __PYX_ERR(0, 234, __pyx_L1_error)
 
   /* "(tree fragment)":1
  * def __reduce_cython__(self):             # <<<<<<<<<<<<<<
  *     raise TypeError, "no default __reduce__ due to non-trivial __cinit__"
  * def __setstate_cython__(self, __pyx_state):
  */
   __pyx_codeobj__49 = (PyObject*)__Pyx_PyCode_New(1, 0, 0, 1, 0, CO_OPTIMIZED|CO_NEWLOCALS, __pyx_empty_bytes, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_tuple__32, __pyx_empty_tuple, __pyx_empty_tuple, __pyx_kp_s_stringsource, __pyx_n_s_reduce_cython, 1, __pyx_empty_bytes); if (unlikely(!__pyx_codeobj__49)) __PYX_ERR(1, 1, __pyx_L1_error)
@@ -26368,17 +26640,17 @@
   if (PyObject_SetAttr(__pyx_m, __pyx_n_s_Coefficients, (PyObject *) __pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Coefficients) < 0) __PYX_ERR(0, 7, __pyx_L1_error)
   #if !CYTHON_COMPILING_IN_LIMITED_API
   if (__Pyx_setup_reduce((PyObject *) __pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Coefficients) < 0) __PYX_ERR(0, 7, __pyx_L1_error)
   #endif
   __pyx_vtabptr_7ruleopt_11aux_classes_11aux_classes_Rule = &__pyx_vtable_7ruleopt_11aux_classes_11aux_classes_Rule;
   __pyx_vtable_7ruleopt_11aux_classes_11aux_classes_Rule.add_clause = (PyObject *(*)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, int, double, double, int, int __pyx_skip_dispatch))__pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule_add_clause;
   __pyx_vtable_7ruleopt_11aux_classes_11aux_classes_Rule._get_clause = (PyObject *(*)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, int, int __pyx_skip_dispatch))__pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule__get_clause;
-  __pyx_vtable_7ruleopt_11aux_classes_11aux_classes_Rule._check_rule_nogil = (int (*)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, __Pyx_memviewslice))__pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule__check_rule_nogil;
+  __pyx_vtable_7ruleopt_11aux_classes_11aux_classes_Rule._check_rule_nogil = (__Pyx_memviewslice (*)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, __Pyx_memviewslice))__pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule__check_rule_nogil;
   __pyx_vtable_7ruleopt_11aux_classes_11aux_classes_Rule._check_clause_nogil = (int (*)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, __Pyx_memviewslice, int))__pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule__check_clause_nogil;
-  __pyx_vtable_7ruleopt_11aux_classes_11aux_classes_Rule.check_rule = (int (*)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, __Pyx_memviewslice, int __pyx_skip_dispatch))__pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule_check_rule;
+  __pyx_vtable_7ruleopt_11aux_classes_11aux_classes_Rule.check_rule = (PyArrayObject *(*)(struct __pyx_obj_7ruleopt_11aux_classes_11aux_classes_Rule *, __Pyx_memviewslice, int __pyx_skip_dispatch))__pyx_f_7ruleopt_11aux_classes_11aux_classes_4Rule_check_rule;
   #if CYTHON_USE_TYPE_SPECS
   __pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule = (PyTypeObject *) __Pyx_PyType_FromModuleAndSpec(__pyx_m, &__pyx_type_7ruleopt_11aux_classes_11aux_classes_Rule_spec, NULL); if (unlikely(!__pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule)) __PYX_ERR(0, 45, __pyx_L1_error)
   if (__Pyx_fix_up_extension_type_from_spec(&__pyx_type_7ruleopt_11aux_classes_11aux_classes_Rule_spec, __pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule) < 0) __PYX_ERR(0, 45, __pyx_L1_error)
   #else
   __pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule = &__pyx_type_7ruleopt_11aux_classes_11aux_classes_Rule;
   #endif
   #if !CYTHON_COMPILING_IN_LIMITED_API
@@ -27625,52 +27897,52 @@
  */
   __pyx_t_5 = __Pyx_CyFunction_New(&__pyx_mdef_7ruleopt_11aux_classes_11aux_classes_4Rule_11add_clause, __Pyx_CYFUNCTION_CCLASS, __pyx_n_s_Rule_add_clause, NULL, __pyx_n_s_ruleopt_aux_classes_aux_classes_2, __pyx_d, ((PyObject *)__pyx_codeobj__41)); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 98, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_5);
   if (__Pyx_SetItemOnTypeDict((PyObject *)__pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule, __pyx_n_s_add_clause, __pyx_t_5) < 0) __PYX_ERR(0, 98, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
   PyType_Modified(__pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule);
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":184
- *         return clause.lb < val <= clause.ub
+  /* "ruleopt/aux_classes/aux_classes.pyx":189
+ * 
  * 
- *     cpdef bint check_rule(self, float[:] X):             # <<<<<<<<<<<<<<
+ *     cpdef cnp.ndarray[cnp.uint8_t, ndim=1] check_rule(self, float[:, :] X):             # <<<<<<<<<<<<<<
  *         """
  *         Checks if the rule applies to the given feature values.
  */
-  __pyx_t_5 = __Pyx_CyFunction_New(&__pyx_mdef_7ruleopt_11aux_classes_11aux_classes_4Rule_13check_rule, __Pyx_CYFUNCTION_CCLASS, __pyx_n_s_Rule_check_rule, NULL, __pyx_n_s_ruleopt_aux_classes_aux_classes_2, __pyx_d, ((PyObject *)__pyx_codeobj__43)); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 184, __pyx_L1_error)
+  __pyx_t_5 = __Pyx_CyFunction_New(&__pyx_mdef_7ruleopt_11aux_classes_11aux_classes_4Rule_13check_rule, __Pyx_CYFUNCTION_CCLASS, __pyx_n_s_Rule_check_rule, NULL, __pyx_n_s_ruleopt_aux_classes_aux_classes_2, __pyx_d, ((PyObject *)__pyx_codeobj__43)); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 189, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_5);
-  if (__Pyx_SetItemOnTypeDict((PyObject *)__pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule, __pyx_n_s_check_rule, __pyx_t_5) < 0) __PYX_ERR(0, 184, __pyx_L1_error)
+  if (__Pyx_SetItemOnTypeDict((PyObject *)__pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule, __pyx_n_s_check_rule, __pyx_t_5) < 0) __PYX_ERR(0, 189, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
   PyType_Modified(__pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule);
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":200
- *         return self._check_rule_nogil(X)
+  /* "ruleopt/aux_classes/aux_classes.pyx":209
+ * 
  * 
  *     def to_text(self, feature_names=None):             # <<<<<<<<<<<<<<
  *         """
  *         Converts the rule to a human-readable text representation, using optional
  */
-  __pyx_t_5 = __Pyx_CyFunction_New(&__pyx_mdef_7ruleopt_11aux_classes_11aux_classes_4Rule_15to_text, __Pyx_CYFUNCTION_CCLASS, __pyx_n_s_Rule_to_text, NULL, __pyx_n_s_ruleopt_aux_classes_aux_classes_2, __pyx_d, ((PyObject *)__pyx_codeobj__45)); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 200, __pyx_L1_error)
+  __pyx_t_5 = __Pyx_CyFunction_New(&__pyx_mdef_7ruleopt_11aux_classes_11aux_classes_4Rule_15to_text, __Pyx_CYFUNCTION_CCLASS, __pyx_n_s_Rule_to_text, NULL, __pyx_n_s_ruleopt_aux_classes_aux_classes_2, __pyx_d, ((PyObject *)__pyx_codeobj__45)); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 209, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_5);
   __Pyx_CyFunction_SetDefaultsTuple(__pyx_t_5, __pyx_tuple__46);
-  if (__Pyx_SetItemOnTypeDict((PyObject *)__pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule, __pyx_n_s_to_text, __pyx_t_5) < 0) __PYX_ERR(0, 200, __pyx_L1_error)
+  if (__Pyx_SetItemOnTypeDict((PyObject *)__pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule, __pyx_n_s_to_text, __pyx_t_5) < 0) __PYX_ERR(0, 209, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
   PyType_Modified(__pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule);
 
-  /* "ruleopt/aux_classes/aux_classes.pyx":225
+  /* "ruleopt/aux_classes/aux_classes.pyx":234
  *         return print_text.rstrip("\n")
  * 
  *     def to_dict(self, feature_names=None):             # <<<<<<<<<<<<<<
  *         """
  *         Converts the rule to a dictionary representation, using optional feature names for keys.
  */
-  __pyx_t_5 = __Pyx_CyFunction_New(&__pyx_mdef_7ruleopt_11aux_classes_11aux_classes_4Rule_17to_dict, __Pyx_CYFUNCTION_CCLASS, __pyx_n_s_Rule_to_dict, NULL, __pyx_n_s_ruleopt_aux_classes_aux_classes_2, __pyx_d, ((PyObject *)__pyx_codeobj__48)); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 225, __pyx_L1_error)
+  __pyx_t_5 = __Pyx_CyFunction_New(&__pyx_mdef_7ruleopt_11aux_classes_11aux_classes_4Rule_17to_dict, __Pyx_CYFUNCTION_CCLASS, __pyx_n_s_Rule_to_dict, NULL, __pyx_n_s_ruleopt_aux_classes_aux_classes_2, __pyx_d, ((PyObject *)__pyx_codeobj__48)); if (unlikely(!__pyx_t_5)) __PYX_ERR(0, 234, __pyx_L1_error)
   __Pyx_GOTREF(__pyx_t_5);
   __Pyx_CyFunction_SetDefaultsTuple(__pyx_t_5, __pyx_tuple__46);
-  if (__Pyx_SetItemOnTypeDict((PyObject *)__pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule, __pyx_n_s_to_dict, __pyx_t_5) < 0) __PYX_ERR(0, 225, __pyx_L1_error)
+  if (__Pyx_SetItemOnTypeDict((PyObject *)__pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule, __pyx_n_s_to_dict, __pyx_t_5) < 0) __PYX_ERR(0, 234, __pyx_L1_error)
   __Pyx_DECREF(__pyx_t_5); __pyx_t_5 = 0;
   PyType_Modified(__pyx_ptype_7ruleopt_11aux_classes_11aux_classes_Rule);
 
   /* "(tree fragment)":1
  * def __reduce_cython__(self):             # <<<<<<<<<<<<<<
  *     raise TypeError, "no default __reduce__ due to non-trivial __cinit__"
  * def __setstate_cython__(self, __pyx_state):
@@ -30196,14 +30468,75 @@
     } else {
         Py_DECREF(r);
         return 1;
     }
 }
 #endif
 
+/* BufferIndexError */
+static void __Pyx_RaiseBufferIndexError(int axis) {
+  PyErr_Format(PyExc_IndexError,
+     "Out of bounds on buffer access (axis %d)", axis);
+}
+
+/* BufferIndexErrorNogil */
+static void __Pyx_RaiseBufferIndexErrorNogil(int axis) {
+    #ifdef WITH_THREAD
+    PyGILState_STATE gilstate = PyGILState_Ensure();
+    #endif
+    __Pyx_RaiseBufferIndexError(axis);
+    #ifdef WITH_THREAD
+    PyGILState_Release(gilstate);
+    #endif
+}
+
+/* WriteUnraisableException */
+static void __Pyx_WriteUnraisable(const char *name, int clineno,
+                                  int lineno, const char *filename,
+                                  int full_traceback, int nogil) {
+    PyObject *old_exc, *old_val, *old_tb;
+    PyObject *ctx;
+    __Pyx_PyThreadState_declare
+#ifdef WITH_THREAD
+    PyGILState_STATE state;
+    if (nogil)
+        state = PyGILState_Ensure();
+    else state = (PyGILState_STATE)0;
+#endif
+    CYTHON_UNUSED_VAR(clineno);
+    CYTHON_UNUSED_VAR(lineno);
+    CYTHON_UNUSED_VAR(filename);
+    CYTHON_MAYBE_UNUSED_VAR(nogil);
+    __Pyx_PyThreadState_assign
+    __Pyx_ErrFetch(&old_exc, &old_val, &old_tb);
+    if (full_traceback) {
+        Py_XINCREF(old_exc);
+        Py_XINCREF(old_val);
+        Py_XINCREF(old_tb);
+        __Pyx_ErrRestore(old_exc, old_val, old_tb);
+        PyErr_PrintEx(1);
+    }
+    #if PY_MAJOR_VERSION < 3
+    ctx = PyString_FromString(name);
+    #else
+    ctx = PyUnicode_FromString(name);
+    #endif
+    __Pyx_ErrRestore(old_exc, old_val, old_tb);
+    if (!ctx) {
+        PyErr_WriteUnraisable(Py_None);
+    } else {
+        PyErr_WriteUnraisable(ctx);
+        Py_DECREF(ctx);
+    }
+#ifdef WITH_THREAD
+    if (nogil)
+        PyGILState_Release(state);
+#endif
+}
+
 /* PyObjectFormat */
 #if CYTHON_USE_UNICODE_WRITER
 static PyObject* __Pyx_PyObject_Format(PyObject* obj, PyObject* format_spec) {
     int ret;
     _PyUnicodeWriter writer;
     if (likely(PyFloat_CheckExact(obj))) {
 #if CYTHON_COMPILING_IN_CPYTHON && PY_VERSION_HEX < 0x03040000
@@ -33530,26 +33863,49 @@
 __pyx_fail:
     result.memview = NULL;
     result.data = NULL;
     return result;
 }
 
 /* ObjectToMemviewSlice */
-  static CYTHON_INLINE __Pyx_memviewslice __Pyx_PyObject_to_MemoryviewSlice_ds_float(PyObject *obj, int writable_flag) {
+  static CYTHON_INLINE __Pyx_memviewslice __Pyx_PyObject_to_MemoryviewSlice_dsds_float(PyObject *obj, int writable_flag) {
+    __Pyx_memviewslice result = { 0, 0, { 0 }, { 0 }, { 0 } };
+    __Pyx_BufFmt_StackElem stack[1];
+    int axes_specs[] = { (__Pyx_MEMVIEW_DIRECT | __Pyx_MEMVIEW_STRIDED), (__Pyx_MEMVIEW_DIRECT | __Pyx_MEMVIEW_STRIDED) };
+    int retcode;
+    if (obj == Py_None) {
+        result.memview = (struct __pyx_memoryview_obj *) Py_None;
+        return result;
+    }
+    retcode = __Pyx_ValidateAndInit_memviewslice(axes_specs, 0,
+                                                 PyBUF_RECORDS_RO | writable_flag, 2,
+                                                 &__Pyx_TypeInfo_float, stack,
+                                                 &result, obj);
+    if (unlikely(retcode == -1))
+        goto __pyx_fail;
+    return result;
+__pyx_fail:
+    result.memview = NULL;
+    result.data = NULL;
+    return result;
+}
+
+/* ObjectToMemviewSlice */
+  static CYTHON_INLINE __Pyx_memviewslice __Pyx_PyObject_to_MemoryviewSlice_ds_char(PyObject *obj, int writable_flag) {
     __Pyx_memviewslice result = { 0, 0, { 0 }, { 0 }, { 0 } };
     __Pyx_BufFmt_StackElem stack[1];
     int axes_specs[] = { (__Pyx_MEMVIEW_DIRECT | __Pyx_MEMVIEW_STRIDED) };
     int retcode;
     if (obj == Py_None) {
         result.memview = (struct __pyx_memoryview_obj *) Py_None;
         return result;
     }
     retcode = __Pyx_ValidateAndInit_memviewslice(axes_specs, 0,
                                                  PyBUF_RECORDS_RO | writable_flag, 1,
-                                                 &__Pyx_TypeInfo_float, stack,
+                                                 &__Pyx_TypeInfo_char, stack,
                                                  &result, obj);
     if (unlikely(retcode == -1))
         goto __pyx_fail;
     return result;
 __pyx_fail:
     result.memview = NULL;
     result.data = NULL;
@@ -33564,14 +33920,26 @@
     float value = __pyx_PyFloat_AsFloat(obj);
     if (unlikely((value == (float)-1) && PyErr_Occurred()))
         return 0;
     *(float *) itemp = value;
     return 1;
 }
 
+/* MemviewDtypeToObject */
+  static CYTHON_INLINE PyObject *__pyx_memview_get_char(const char *itemp) {
+    return (PyObject *) __Pyx_PyInt_From_char(*(char *) itemp);
+}
+static CYTHON_INLINE int __pyx_memview_set_char(const char *itemp, PyObject *obj) {
+    char value = __Pyx_PyInt_As_char(obj);
+    if (unlikely((value == (char)-1) && PyErr_Occurred()))
+        return 0;
+    *(char *) itemp = value;
+    return 1;
+}
+
 /* Declarations */
   #if CYTHON_CCOMPLEX && (1) && (!0 || __cplusplus)
   #ifdef __cplusplus
     static CYTHON_INLINE __pyx_t_float_complex __pyx_t_float_complex_from_parts(float x, float y) {
       return ::std::complex< float >(x, y);
     }
   #else
@@ -35022,55 +35390,55 @@
         Py_XDECREF(from_bytes);
         return result;
 #endif
     }
 }
 
 /* CIntToPy */
-  static CYTHON_INLINE PyObject* __Pyx_PyInt_From_long(long value) {
+  static CYTHON_INLINE PyObject* __Pyx_PyInt_From_char(char value) {
 #ifdef __Pyx_HAS_GCC_DIAGNOSTIC
 #pragma GCC diagnostic push
 #pragma GCC diagnostic ignored "-Wconversion"
 #endif
-    const long neg_one = (long) -1, const_zero = (long) 0;
+    const char neg_one = (char) -1, const_zero = (char) 0;
 #ifdef __Pyx_HAS_GCC_DIAGNOSTIC
 #pragma GCC diagnostic pop
 #endif
     const int is_unsigned = neg_one > const_zero;
     if (is_unsigned) {
-        if (sizeof(long) < sizeof(long)) {
+        if (sizeof(char) < sizeof(long)) {
             return PyInt_FromLong((long) value);
-        } else if (sizeof(long) <= sizeof(unsigned long)) {
+        } else if (sizeof(char) <= sizeof(unsigned long)) {
             return PyLong_FromUnsignedLong((unsigned long) value);
 #ifdef HAVE_LONG_LONG
-        } else if (sizeof(long) <= sizeof(unsigned PY_LONG_LONG)) {
+        } else if (sizeof(char) <= sizeof(unsigned PY_LONG_LONG)) {
             return PyLong_FromUnsignedLongLong((unsigned PY_LONG_LONG) value);
 #endif
         }
     } else {
-        if (sizeof(long) <= sizeof(long)) {
+        if (sizeof(char) <= sizeof(long)) {
             return PyInt_FromLong((long) value);
 #ifdef HAVE_LONG_LONG
-        } else if (sizeof(long) <= sizeof(PY_LONG_LONG)) {
+        } else if (sizeof(char) <= sizeof(PY_LONG_LONG)) {
             return PyLong_FromLongLong((PY_LONG_LONG) value);
 #endif
         }
     }
     {
         int one = 1; int little = (int)*(unsigned char *)&one;
         unsigned char *bytes = (unsigned char *)&value;
 #if !CYTHON_COMPILING_IN_LIMITED_API && PY_VERSION_HEX < 0x030d0000
-        return _PyLong_FromByteArray(bytes, sizeof(long),
+        return _PyLong_FromByteArray(bytes, sizeof(char),
                                      little, !is_unsigned);
 #else
         PyObject *from_bytes, *result = NULL;
         PyObject *py_bytes = NULL, *arg_tuple = NULL, *kwds = NULL, *order_str = NULL;
         from_bytes = PyObject_GetAttrString((PyObject*)&PyLong_Type, "from_bytes");
         if (!from_bytes) return NULL;
-        py_bytes = PyBytes_FromStringAndSize((char*)bytes, sizeof(long));
+        py_bytes = PyBytes_FromStringAndSize((char*)bytes, sizeof(char));
         if (!py_bytes) goto limited_bad;
         order_str = PyUnicode_FromString(little ? "little" : "big");
         if (!order_str) goto limited_bad;
         arg_tuple = PyTuple_Pack(2, py_bytes, order_str);
         if (!arg_tuple) goto limited_bad;
         if (!is_unsigned) {
             kwds = PyDict_New();
@@ -35358,14 +35726,78 @@
     return (char) -1;
 raise_neg_overflow:
     PyErr_SetString(PyExc_OverflowError,
         "can't convert negative value to char");
     return (char) -1;
 }
 
+/* CIntToPy */
+  static CYTHON_INLINE PyObject* __Pyx_PyInt_From_long(long value) {
+#ifdef __Pyx_HAS_GCC_DIAGNOSTIC
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wconversion"
+#endif
+    const long neg_one = (long) -1, const_zero = (long) 0;
+#ifdef __Pyx_HAS_GCC_DIAGNOSTIC
+#pragma GCC diagnostic pop
+#endif
+    const int is_unsigned = neg_one > const_zero;
+    if (is_unsigned) {
+        if (sizeof(long) < sizeof(long)) {
+            return PyInt_FromLong((long) value);
+        } else if (sizeof(long) <= sizeof(unsigned long)) {
+            return PyLong_FromUnsignedLong((unsigned long) value);
+#ifdef HAVE_LONG_LONG
+        } else if (sizeof(long) <= sizeof(unsigned PY_LONG_LONG)) {
+            return PyLong_FromUnsignedLongLong((unsigned PY_LONG_LONG) value);
+#endif
+        }
+    } else {
+        if (sizeof(long) <= sizeof(long)) {
+            return PyInt_FromLong((long) value);
+#ifdef HAVE_LONG_LONG
+        } else if (sizeof(long) <= sizeof(PY_LONG_LONG)) {
+            return PyLong_FromLongLong((PY_LONG_LONG) value);
+#endif
+        }
+    }
+    {
+        int one = 1; int little = (int)*(unsigned char *)&one;
+        unsigned char *bytes = (unsigned char *)&value;
+#if !CYTHON_COMPILING_IN_LIMITED_API && PY_VERSION_HEX < 0x030d0000
+        return _PyLong_FromByteArray(bytes, sizeof(long),
+                                     little, !is_unsigned);
+#else
+        PyObject *from_bytes, *result = NULL;
+        PyObject *py_bytes = NULL, *arg_tuple = NULL, *kwds = NULL, *order_str = NULL;
+        from_bytes = PyObject_GetAttrString((PyObject*)&PyLong_Type, "from_bytes");
+        if (!from_bytes) return NULL;
+        py_bytes = PyBytes_FromStringAndSize((char*)bytes, sizeof(long));
+        if (!py_bytes) goto limited_bad;
+        order_str = PyUnicode_FromString(little ? "little" : "big");
+        if (!order_str) goto limited_bad;
+        arg_tuple = PyTuple_Pack(2, py_bytes, order_str);
+        if (!arg_tuple) goto limited_bad;
+        if (!is_unsigned) {
+            kwds = PyDict_New();
+            if (!kwds) goto limited_bad;
+            if (PyDict_SetItemString(kwds, "signed", __Pyx_NewRef(Py_True))) goto limited_bad;
+        }
+        result = PyObject_Call(from_bytes, arg_tuple, kwds);
+        limited_bad:
+        Py_XDECREF(kwds);
+        Py_XDECREF(arg_tuple);
+        Py_XDECREF(order_str);
+        Py_XDECREF(py_bytes);
+        Py_XDECREF(from_bytes);
+        return result;
+#endif
+    }
+}
+
 /* FormatTypeName */
   #if CYTHON_COMPILING_IN_LIMITED_API
 static __Pyx_TypeName
 __Pyx_PyType_GetName(PyTypeObject* tp)
 {
     PyObject *name = __Pyx_PyObject_GetAttrStr((PyObject *)tp,
                                                __pyx_n_s_name_2);
```

## ruleopt/estimator/__init__.py

 * *Ordering differences only*

```diff
@@ -1,10 +1,10 @@
-from .sklearn_.rug import (RUGClassifier)
-from .sklearn_.rux import (RUXClassifier)
-from .xgboost_ import (RUXXGBClassifier)
-from .lightgbm_ import (RUXLGBMClassifier)
-
-__all__ = [
-    "RUGClassifier",
-    "RUXClassifier",
-    "RUXLGBMClassifier",
+from .sklearn_.rug import (RUGClassifier)
+from .sklearn_.rux import (RUXClassifier)
+from .xgboost_ import (RUXXGBClassifier)
+from .lightgbm_ import (RUXLGBMClassifier)
+
+__all__ = [
+    "RUGClassifier",
+    "RUXClassifier",
+    "RUXLGBMClassifier",
     "RUXXGBClassifier"]
```

## ruleopt/estimator/base.py

```diff
@@ -1,702 +1,682 @@
-from __future__ import annotations
-import warnings
-from typing import Union, Dict
-from abc import abstractmethod
-import numpy as np
-from numpy.typing import ArrayLike
-from sklearn.base import BaseEstimator, ClassifierMixin
-from sklearn.utils.class_weight import compute_sample_weight
-
-from ..aux_classes import Coefficients, Rule
-from ..rule_cost import RuleCost
-from ..utils import check_inputs
-from ..solver.base import OptimizationSolver
-
-
-class _RUGBASE(BaseEstimator, ClassifierMixin):
-    """
-    The foundational class for all estimators in the ruleopt library. `_RUGBASE` provides
-    the core framework that every model in ruleopt builds upon.
-
-    Parameters
-    ----------
-    solver : OptimizationSolver
-        An instance of a derived class inherits from the 'Optimization Solver' base class.
-        The solver is responsible for optimizing the rule set based on the cost function
-        and constraints.
-
-    rule_cost : RuleCost or int
-        Defines the cost of rules, either as a specific calculation method (RuleCost instance)
-        or a fixed cost
-
-    class_weight: dict, "balanced" or None
-        A dictionary mapping class labels to their respective weights, the string "balanced"
-        to automatically adjust weights inversely proportional to class frequencies,
-        or None for no weights. Used to adjust the model in favor of certain classes.
-
-    threshold : float
-        The minimum weight threshold for including a rule in the final model
-
-    random_state : int or None
-        Seed for the random number generator to ensure reproducible results.
-        Defaults to None.
-    """
-
-    def __init__(
-        self,
-        solver: OptimizationSolver,
-        rule_cost: Union[RuleCost, int],
-        class_weight: Dict[int, float],
-        threshold: float,
-        random_state: Union[None, int],
-    ):
-
-        self._validate_rug_parameters(
-            threshold=threshold,
-            random_state=random_state,
-            rule_cost=rule_cost,
-            solver=solver,
-            class_weight=class_weight,
-        )
-        self.threshold = float(threshold)
-        self.rule_cost = rule_cost
-        self.solver = solver
-        self.random_state = random_state
-        self.class_weight = class_weight
-
-        # Additional initializations
-        self._rng = np.random.default_rng(
-            random_state if random_state is not None else None
-        )
-        self.decision_trees_ = {}
-        self.decision_rules_ = {}
-        self.rule_info_ = {}
-        self.coefficients_ = Coefficients()
-
-        self._is_fitted: bool = False
-        self.majority_class_: int = None
-        self.majority_probability_: float = None
-        self.k_: float = None
-        self.classes_: np.array = None
-        self.rule_columns_: np.array = None
-
-    def _cleanup(self) -> None:
-        """
-        Clean up the model by resetting all of its attributes.
-        """
-        # Resetting all dictionaries
-        self.decision_trees_ = {}
-        self.decision_rules_ = {}
-        self.rule_info_ = {}
-
-        # Cleaning up coefficients
-        self.coefficients_.cleanup()
-
-        # Resetting the random number generator
-        self._rng = np.random.default_rng(self.random_state)
-
-    @abstractmethod
-    def _get_rule(self, *arg, **kwargs) -> Rule: ...
-
-    @abstractmethod
-    def _get_matrix(self, *arg, **kwargs) -> None: ...
-
-    def _get_class_infos(self, y: np.ndarray) -> None:
-        """
-        Computes and stores information about the classes in the dataset.
-
-        This method calculates the majority class, its probability, the total number
-        of unique classes, and stores an array of unique class labels.
-
-        Parameters
-        ----------
-        y : np.ndarray
-            The target values, expected to be a 1D numpy array of class labels.
-
-        Sets Attributes
-        ---------------
-        majority_class_ : int
-            The class label with the highest frequency in `y`.
-        majority_probability_ : float
-            The proportion of samples in `y` belonging to the majority class,
-            calculated as the count of the majority class divided by the total number of samples.
-        k_ : float
-            The total number of unique classes in `y`.
-        classes_ : np.array
-            An array of the unique class labels present in the dataset.
-        """
-        classes, class_counts = np.unique(y, return_counts=True)
-
-        self.majority_class_ = classes[np.argmax(class_counts)]
-        self.majority_probability_ = class_counts / np.sum(class_counts)
-
-        self.k_ = classes.shape[0]
-        self.classes_ = classes
-
-    def _preprocess(self, y: np.ndarray) -> np.ndarray:
-        """
-        Transforms the target values into a vector. If the target
-        class is k and there are K classes, then all components but
-        the kth are set to -1/(K-1) and the kth component is set to 1. 
-
-        Parameters
-        ----------
-        y : np.ndarray
-            The target values as a 1D numpy array of class labels.
-
-        Returns
-        -------
-        np.ndarray
-            The preprocessed target values in a one-hot-encoded format, adjusted for the model's
-            optimization process.
-        """
-
-        # Convert the labels into kth unit vector
-        vec_y = np.eye(self.k_)[y]
-
-        # Replace 0s with -1/(K-1)
-        vec_y[vec_y == 0] = -1 / (self.k_ - 1)
-
-        return vec_y
-
-    def _get_rule_cost(
-        self, temp_rule: Rule, covers: np.ndarray, counts: np.ndarray, y: np.ndarray
-    ) -> float:
-        """
-        Calculates the cost of a rule.
-
-        Depending on the `rule_cost` attribute, this method either calls a custom cost
-        function defined in a `RuleCost` instance or returns a fixed cost for the rule.
-
-        Parameters
-        ----------
-        temp_rule : Rule
-            The rule for which the cost is being calculated.
-        covers : np.ndarray
-            An array indicating whether each instance in the dataset is covered by the rule.
-        counts : np.ndarray
-            An array indicating the count of instances covered by the rule, segmented by class.
-        y : np.ndarray
-            The target array, containing the actual class labels of the instances.
-
-        Returns
-        -------
-        float
-            The calculated cost of the rule.
-        """
-
-        if isinstance(self.rule_cost, RuleCost):
-            return self.rule_cost(
-                temp_rule=temp_rule, covers=covers, counts=counts, y=y
-            )
-        elif isinstance(self.rule_cost, int):
-            return self.rule_cost
-        else:
-            raise TypeError(
-                f"Unsupported type for `rule_cost`: {type(self.rule_cost)}.",
-                "Expected a RuleCost instance or an int.",
-            )
-
-    def _fill_rules(self, weights: np.ndarray) -> None:
-        """
-        Selects and stores rules based on their weights and a predefined threshold.
-
-        Parameters
-        ----------
-        weights : np.ndarray
-            An array containing the weights of each rule.
-
-        Modifies
-        --------
-        rule_columns_ : np.ndarray
-            Updated to include indices of the selected rules, ordered by their weight.
-        decision_rules_ : Dict[int, Rule]
-            Populated with the selected and ordered rules, keyed by their new indices.
-        """
-        # Scale the weights
-        max_weight = np.max(weights)
-        if max_weight != 1 and max_weight > 1.0e-6:
-            weights = np.divide(weights, max_weight)
-
-        # Select columns where weights are above the threshold
-        selected_columns = np.where(weights > self.threshold)[0]
-
-        # Order the selected columns by their weights
-        weight_order = np.argsort(-weights[selected_columns])
-        ordered_columns = selected_columns[weight_order]
-
-        # Assign the ordered columns to the class attribute
-        self.rule_columns_ = ordered_columns
-
-        # Iterate over the columns and fill the rules dictionary
-        for i, col in enumerate(ordered_columns):
-            treeno, leafno, label, sdist = self.rule_info_[col]
-            fit_tree = self.decision_trees_[treeno]
-            rule = self._get_rule(fit_tree, leafno)
-            rule.label = label
-            rule.weight = weights[col]
-            rule.sdist = sdist
-            self.decision_rules_[i] = rule
-
-    def _predict_base(
-        self,
-        x: np.ndarray,
-        indices: list = None,
-        threshold: float = 0,
-        *,
-        predict_info=False,
-    ) -> np.ndarray:
-        """
-        Calculates the base class weights for each instance based on selected rules.
-        Optionally returns additional prediction info.
-
-        Parameters
-        ----------
-        x : np.ndarray
-            The feature matrix for the instances to predict.
-        indices : list, optional
-            Specific indices of rules to use for prediction. If None, all rules
-            are used.
-        threshold : float, default=0
-            The threshold for selecting rules based on their weights.
-        predict_info : bool, default=False
-            If True, returns additional information about the prediction process
-            including indices of samples with missed values, number of rules
-            applied per sample, and average rule length per sample. Otherwise,
-            returns only the array of raw class weights.
-
-        Returns
-        -------
-        np.ndarray
-            An array of raw class weights for each instance, used as the basis for final
-            prediction.
-            If predict_info is True, also returns arrays containing indices of samples
-            with missed values, number of rules applied per sample, and average rule
-            length per sample.
-        """
-
-        # Check if the model has been fitted
-        if indices is None:
-            indices = []
-        if not self.decision_trees_:
-            raise ValueError("You need to fit the RUG model first")
-
-        # If no specific indices are provided, use all rule indices
-        if len(indices) == 0:
-            indices = list(self.decision_rules_.keys())
-
-        # If provided indices exceed the available rules, return a warning
-        elif np.max(indices) > len(self.decision_rules_):
-            warnings.warn(f"\n There are only {len(self.decision_rules_)} rules")
-            return None
-
-        rule_lengths = np.zeros(x.shape[0], dtype=np.intp)
-        sum_class_weights_arr = np.empty(shape=(x.shape[0], self.k_), dtype=np.float64)
-
-        # Initialize arrays to track missing values and rule lengths per sample
-        missed_values_index_ = np.empty((0,), dtype=np.intp)
-        rules_per_sample_ = np.zeros(x.shape[0], dtype=np.intp)
-        rule_length_per_sample_ = np.zeros(x.shape[0], dtype=np.float64)
-
-        # Iterate over each sample in the feature matrix x
-        for sindx, x0 in enumerate(x):
-            # Initialize an array to hold class weights for this sample
-            sum_class_weights = np.zeros(self.k_, dtype=np.float64)
-            rule_lengths[sindx] = 0
-
-            # Iterate over each rule and apply it to the sample
-            for indx, rule in self.decision_rules_.items():
-                # Only apply the rule if its index is in the specified indices,
-                # it applies to the sample, and its weight is above the threshold
-                if indx in indices and rule.check_rule(x0) and rule.weight >= threshold:
-                    # Add the weight of the rule to the weight for its class
-                    sum_class_weights[rule.label] += rule.weight
-                    # Increment the count of rules applied to this sample
-                    rules_per_sample_[sindx] += 1
-                    # Add the length of the rule to the total rule length for this sample
-                    rule_lengths[sindx] += len(rule)
-
-            # If any rules were applied to this sample, calculate the average rule length
-            if rule_lengths[sindx] > 0:
-                rule_length_per_sample_[sindx] = (
-                    rule_lengths[sindx] / rules_per_sample_[sindx]
-                )
-            else:
-                # If no rules were applied to this sample, add it to the missed values index
-                missed_values_index_ = np.concatenate((missed_values_index_, [sindx]))
-
-            # Add the class weights for this sample to the array of class weights
-            sum_class_weights_arr[sindx, :] = sum_class_weights
-
-        # Return the array of class weights
-        if predict_info:
-            warnings.warn(
-                "Enabling 'predict_info' will return additional prediction "
-                "details, including indices of samples with missed values, "
-                "number of rules applied per sample, and average rule length "
-                "per sample. While this information is useful for in-depth "
-                "analysis, it may increase computational overhead and complexity "
-                "of result interpretation. Use this feature judiciously."
-            )
-            return missed_values_index_, rules_per_sample_, rule_length_per_sample_
-
-        else:
-            return sum_class_weights_arr
-
-    def predict(
-        self,
-        x: ArrayLike,
-        indices: list | None = None,
-        threshold: float = 0.0,
-        *,
-        predict_info: bool = False,
-    ) -> np.ndarray:
-        """
-        Predicts class labels for the given data, optionally returning
-        additional prediction info.
-
-        Parameters
-        ----------
-        x : array-like of shape (n_samples, n_features)
-            The training input samples. Internally, it will be converted to dtype=np.float32.
-        indices : list or None, default=None
-            Specific indices of rules to use for prediction. If None,
-            all rules are used.
-        threshold : float, default=0
-            The threshold for selecting rules based on their weights.
-        predict_info : bool, default=False
-            If True, returns additional information about the prediction
-            process including indices of samples with missed values, number
-            of rules applied per sample, and average rule length per sample.
-            Otherwise, returns only the predicted class labels.
-
-        Returns
-        -------
-        np.ndarray
-            An array of predicted class labels for each instance in `x`.
-            If predict_info is True, also returns arrays containing indices
-            of samples with missed values, number of rules applied per sample,
-            and average rule length per sample.
-        """
-
-        if indices is None:
-            indices = []
-
-        x = check_inputs(x)
-
-        if predict_info:
-            missed_values_index_, rules_per_sample_, rule_length_per_sample_ = (
-                self._predict_base(x, indices, threshold, predict_info=predict_info)
-            )
-            return missed_values_index_, rules_per_sample_, rule_length_per_sample_
-
-        else:
-            sum_class_weights_arr = self._predict_base(
-                x, indices, threshold, predict_info=predict_info
-            )
-
-        return_prediction = np.empty(shape=(x.shape[0],), dtype=np.intp)
-
-        # Convert the class weights into class predictions
-        for sindx, sum_class_weights in enumerate(sum_class_weights_arr):
-            # If the sum of the class weights is 0, predict the majority class
-            if np.sum(sum_class_weights) <= 10e-6:
-                return_prediction[sindx] = self.majority_class_
-            else:
-                # Otherwise, predict the class with the highest weight
-                return_prediction[sindx] = np.argmax(sum_class_weights)
-
-        # Return the predicted classes
-        return return_prediction
-
-    def predict_proba(
-        self,
-        x: ArrayLike,
-        indices: list | None = None,
-        threshold: float = 0.0,
-        *,
-        predict_info: bool = False,
-    ):
-        """
-        Predicts class probabilities for the given data, optionally
-        returning additional prediction info.
-
-        Parameters
-        ----------
-        x : array-like of shape (n_samples, n_features)
-            The training input samples. Internally, it will be converted to dtype=np.float32.
-        indices : list or None, default=None
-            Specific indices of rules to use for calculating probabilities.
-            If None, all rules are used.
-        threshold : float, default=0
-            The threshold for selecting rules based on their weights.
-        predict_info : bool, default=False
-            If True, returns additional information about the prediction process
-            including indices of samples with missed values, number of rules applied
-            per sample, and average rule length per sample. Otherwise, returns only
-            the probabilities of each class for each sample.
-
-        Returns
-        -------
-        np.ndarray
-            An array where each row corresponds to a sample in `x` and each column
-            to a class, containing the probability of each class for each sample.
-            If predict_info is True, also returns arrays containing indices of samples
-            with missed values, number of rules applied per sample, and average rule
-            length per sample.
-        """
-
-        if indices is None:
-            indices = []
-
-        x = check_inputs(x)
-
-        if predict_info:
-            missed_values_index_, rules_per_sample_, rule_length_per_sample_ = (
-                self._predict_base(x, indices, threshold, predict_info=predict_info)
-            )
-            return missed_values_index_, rules_per_sample_, rule_length_per_sample_
-
-        else:
-            sum_class_weights_arr = self._predict_base(
-                x, indices, threshold, predict_info=predict_info
-            )
-
-        return_prediction = np.empty(shape=(x.shape[0], self.k_), dtype=np.float64)
-
-        # Convert the class weights into class probabilities
-        for sindx, sum_class_weights in enumerate(sum_class_weights_arr):
-            total_weight = np.sum(sum_class_weights)
-            if total_weight <= 10e-6:
-                # If the sum of the class weights is 0, predict the majority class probabilities
-                return_prediction[sindx, :] = self.majority_probability_
-            else:
-                # Otherwise, divide the class weights by the total weight
-                # to get the class probabilities
-                return_prediction[sindx, :] = np.divide(sum_class_weights, total_weight)
-
-        # Return the class probabilities
-        return return_prediction
-
-    def _get_sample_wight(self, sample_weight, class_weight, y):
-        """
-        Calculates the final sample weights based on initial sample weights, class weights and
-        target values.
-
-        Parameters
-        ----------
-        sample_weight : array-like, shape (n_samples,) or None
-            Initial weights of samples. If None, all samples are assumed to have weight one.
-        class_weight : dict, "balanced" or None
-            Weights associated with classes in the form {class_label: weight}. Can be "balanced"
-            to automatically adjust weights inversely proportional to class frequencies in the input data
-            or None for equal weights.
-        y : array-like, shape (n_samples,)
-            Array of target values (class labels).
-
-        Returns
-        -------
-        final_sample_weights : array-like, shape (n_samples,) or None
-            The computed array of weights for each sample in the dataset. Returns None if all computed
-            weights are equal to one, indicating no weighting is necessary.
-        """
-        if class_weight is None and sample_weight is None:
-            return None
-
-        if sample_weight is not None:
-            if (
-                not isinstance(sample_weight, np.ndarray)
-                or sample_weight.shape != y.shape
-            ):
-                raise ValueError(
-                    "sample_weight must be a numpy array of the same shape as y."
-                )
-
-        final_sample_weights = np.ones_like(y, dtype=np.float64)
-
-        if class_weight is not None:
-            if isinstance(class_weight, dict):
-                if len(class_weight.keys()) != np.unique(y).size:
-                    raise ValueError(
-                        "The class_weight dictionary must have a key for each unique value in y."
-                    )
-
-            final_sample_weights *= compute_sample_weight(class_weight, y)
-        if sample_weight is not None:
-            final_sample_weights *= sample_weight
-
-        if np.min(final_sample_weights) != 1:
-            warnings.warn(
-                "Minimum sample weight automatically scaled to 1 for consistency."
-            )
-            final_sample_weights /= np.min(final_sample_weights)
-
-        return final_sample_weights
-
-    def _validate_rug_parameters(
-        self,
-        threshold: float,
-        solver: OptimizationSolver,
-        class_weight: Dict[int:float],
-        random_state: int | None,
-        rule_cost,
-    ):
-        if not isinstance(threshold, (float, int)) or threshold < 0:
-            raise TypeError("threshold must be a non-negative float or integer.")
-
-        if not isinstance(solver, (OptimizationSolver)):
-            raise TypeError("solver should be inherited from OptimizationSolver.")
-
-        if not (isinstance(random_state, int) or random_state is None):
-            raise TypeError("random_state must be an integer or None.")
-
-        if not isinstance(rule_cost, (int, RuleCost)):
-            raise TypeError("rule_cost must be an instance of RuleCost or an integer.")
-
-        if isinstance(rule_cost, int) and rule_cost < 0:
-            raise ValueError("If rule_cost is an integer, it must be non-negative.")
-
-        if not (
-            isinstance(class_weight, dict)
-            and all(
-                isinstance(k, int) and isinstance(v, float)
-                for k, v in class_weight.items()
-            )
-            or class_weight in ["balanced", None]
-        ):
-            raise TypeError(
-                "class_weight must be a dictionary with integer keys and float values, 'balanced', or None."
-            )
-
-    @property
-    def is_fitted(self):
-        """
-        Indicates whether the model is fitted.
-
-        Returns
-        -------
-        bool
-            True if the model is fitted, False otherwise.
-        """
-
-        return self._is_fitted
-
-    @property
-    def decision_rules(self):
-        """
-        Returns the rules extracted from the decision trees, after optimization.
-
-        Returns
-        -------
-        Dict[int, Rule]
-            A dictionary where keys are rule indices and values are Rule objects.
-        """
-
-        return self.decision_rules_
-
-    @property
-    def decision_trees(self):
-        """
-        Returns dictionary that stores the decision tree models.
-
-        Returns
-        -------
-        Dict[int, Any]
-            A dictionary containing decision tree models, with identifiers as keys
-            and decision
-            tree instances as values.
-        """
-        return self.decision_trees_
-
-    @property
-    def rule_info(self):
-        """
-        Returns information about each rule.
-
-        Returns
-        -------
-        Dict[int, Tuple[int, int, int, np.ndarray]]
-            A dictionary with rule indices as keys and tuples containing information about
-            each rule as values. The tuple structure is (rule_id, feature_index, threshold,
-            values_array).
-        """
-
-        return self.rule_info_
-
-    @property
-    def coefficients(self):
-        """
-        Stores coefficients associated with the rules during optimization.
-
-        Returns
-        -------
-        Coefficients
-            An object or array-like structure storing coefficients related to each rule.
-        """
-
-        return self.coefficients_
-
-    @property
-    def majority_class(self):
-        """
-        Returns the class label of the majority class in the dataset.
-
-        Returns
-        -------
-        int
-            The label of the majority class.
-        """
-
-        return self.majority_class_
-
-    @property
-    def majority_probability(self):
-        """
-        Returns the probability of the majority class in the dataset.
-
-        Returns
-        -------
-        float
-            The probability of encountering the majority class in the dataset.
-        """
-
-        return self.majority_probability_
-
-    @property
-    def k(self):
-        """
-        Returns the total number of unique classes in the dataset.
-
-        Returns
-        -------
-        float
-            The total number of unique classes.
-        """
-
-        return self.k_
-
-    @property
-    def classes(self):
-        """
-        Returns unique class labels in the dataset.
-
-        Returns
-        -------
-        np.ndarray
-            An array containing the unique class labels of the dataset.
-        """
-
-        return self.classes_
-
-    @property
-    def rule_columns(self):
-        """
-        Returns indices of rules selected as part of the model.
-
-        Returns
-        -------
-        np.ndarray
-            An array of indices corresponding to the rules included in the model.
-        """
-
-        return self.rule_columns_
+from __future__ import annotations
+import warnings
+from typing import Union, Dict
+from abc import abstractmethod
+import numpy as np
+from numpy.typing import ArrayLike
+from sklearn.base import BaseEstimator, ClassifierMixin
+from sklearn.utils.class_weight import compute_sample_weight
+
+from ..aux_classes import Coefficients, Rule
+from ..rule_cost import RuleCost
+from ..utils import check_inputs
+from ..solver.base import OptimizationSolver
+
+
+class _RUGBASE(BaseEstimator, ClassifierMixin):
+    """
+    The foundational class for all estimators in the ruleopt library. `_RUGBASE` provides
+    the core framework that every model in ruleopt builds upon.
+
+    Parameters
+    ----------
+    solver : OptimizationSolver
+        An instance of a derived class inherits from the 'Optimization Solver' base class.
+        The solver is responsible for optimizing the rule set based on the cost function
+        and constraints.
+
+    rule_cost : RuleCost or int
+        Defines the cost of rules, either as a specific calculation method (RuleCost instance)
+        or a fixed cost
+
+    class_weight: dict, "balanced" or None
+        A dictionary mapping class labels to their respective weights, the string "balanced"
+        to automatically adjust weights inversely proportional to class frequencies,
+        or None for no weights. Used to adjust the model in favor of certain classes.
+
+    threshold : float
+        The minimum weight threshold for including a rule in the final model
+
+    random_state : int or None
+        Seed for the random number generator to ensure reproducible results.
+        Defaults to None.
+    """
+
+    def __init__(
+        self,
+        solver: OptimizationSolver,
+        rule_cost: Union[RuleCost, int],
+        class_weight: Dict[int, float],
+        threshold: float,
+        random_state: Union[None, int],
+    ):
+
+        self._validate_rug_parameters(
+            threshold=threshold,
+            random_state=random_state,
+            rule_cost=rule_cost,
+            solver=solver,
+            class_weight=class_weight,
+        )
+        self.threshold = float(threshold)
+        self.rule_cost = rule_cost
+        self.solver = solver
+        self.random_state = random_state
+        self.class_weight = class_weight
+
+        # Additional initializations
+        self._rng = np.random.default_rng(
+            random_state if random_state is not None else None
+        )
+        self.decision_trees_ = {}
+        self.decision_rules_ = {}
+        self.rule_info_ = {}
+        self.coefficients_ = Coefficients()
+
+        self._is_fitted: bool = False
+        self.majority_class_: int = None
+        self.majority_probability_: float = None
+        self.k_: float = None
+        self.classes_: np.array = None
+        self.rule_columns_: np.array = None
+
+    def _cleanup(self) -> None:
+        """
+        Clean up the model by resetting all of its attributes.
+        """
+        # Resetting all dictionaries
+        self.decision_trees_ = {}
+        self.decision_rules_ = {}
+        self.rule_info_ = {}
+
+        # Cleaning up coefficients
+        self.coefficients_.cleanup()
+
+        # Resetting the random number generator
+        self._rng = np.random.default_rng(self.random_state)
+
+    @abstractmethod
+    def _get_rule(self, *arg, **kwargs) -> Rule: ...
+
+    @abstractmethod
+    def _get_matrix(self, *arg, **kwargs) -> None: ...
+
+    def _get_class_infos(self, y: np.ndarray) -> None:
+        """
+        Computes and stores information about the classes in the dataset.
+
+        This method calculates the majority class, its probability, the total number
+        of unique classes, and stores an array of unique class labels.
+
+        Parameters
+        ----------
+        y : np.ndarray
+            The target values, expected to be a 1D numpy array of class labels.
+
+        Sets Attributes
+        ---------------
+        majority_class_ : int
+            The class label with the highest frequency in `y`.
+        majority_probability_ : float
+            The proportion of samples in `y` belonging to the majority class,
+            calculated as the count of the majority class divided by the total number of samples.
+        k_ : float
+            The total number of unique classes in `y`.
+        classes_ : np.array
+            An array of the unique class labels present in the dataset.
+        """
+        classes, class_counts = np.unique(y, return_counts=True)
+
+        self.majority_class_ = classes[np.argmax(class_counts)]
+        self.majority_probability_ = class_counts / np.sum(class_counts)
+
+        self.k_ = classes.shape[0]
+        self.classes_ = classes
+
+    def _preprocess(self, y: np.ndarray) -> np.ndarray:
+        """
+        Transforms the target values into a vector. If the target
+        class is k and there are K classes, then all components but
+        the kth are set to -1/(K-1) and the kth component is set to 1.
+
+        Parameters
+        ----------
+        y : np.ndarray
+            The target values as a 1D numpy array of class labels.
+
+        Returns
+        -------
+        np.ndarray
+            The preprocessed target values in a one-hot-encoded format, adjusted for the model's
+            optimization process.
+        """
+
+        # Convert the labels into kth unit vector
+        vec_y = np.eye(self.k_)[y]
+
+        # Replace 0s with -1/(K-1)
+        vec_y[vec_y == 0] = -1 / (self.k_ - 1)
+
+        return vec_y
+
+    def _get_rule_cost(
+        self, temp_rule: Rule, covers: np.ndarray, counts: np.ndarray, y: np.ndarray
+    ) -> float:
+        """
+        Calculates the cost of a rule.
+
+        Depending on the `rule_cost` attribute, this method either calls a custom cost
+        function defined in a `RuleCost` instance or returns a fixed cost for the rule.
+
+        Parameters
+        ----------
+        temp_rule : Rule
+            The rule for which the cost is being calculated.
+        covers : np.ndarray
+            An array indicating whether each instance in the dataset is covered by the rule.
+        counts : np.ndarray
+            An array indicating the count of instances covered by the rule, segmented by class.
+        y : np.ndarray
+            The target array, containing the actual class labels of the instances.
+
+        Returns
+        -------
+        float
+            The calculated cost of the rule.
+        """
+
+        if isinstance(self.rule_cost, RuleCost):
+            return self.rule_cost(
+                temp_rule=temp_rule, covers=covers, counts=counts, y=y
+            )
+        elif isinstance(self.rule_cost, int):
+            return self.rule_cost
+        else:
+            raise TypeError(
+                f"Unsupported type for `rule_cost`: {type(self.rule_cost)}.",
+                "Expected a RuleCost instance or an int.",
+            )
+
+    def _fill_rules(self, weights: np.ndarray) -> None:
+        """
+        Selects and stores rules based on their weights and a predefined threshold.
+
+        Parameters
+        ----------
+        weights : np.ndarray
+            An array containing the weights of each rule.
+
+        Modifies
+        --------
+        rule_columns_ : np.ndarray
+            Updated to include indices of the selected rules, ordered by their weight.
+        decision_rules_ : Dict[int, Rule]
+            Populated with the selected and ordered rules, keyed by their new indices.
+        """
+        # Scale the weights
+        max_weight = np.max(weights)
+        if max_weight != 1 and max_weight > 1.0e-6:
+            weights = np.divide(weights, max_weight)
+
+        # Select columns where weights are above the threshold
+        selected_columns = np.where(weights > self.threshold)[0]
+
+        # Order the selected columns by their weights
+        weight_order = np.argsort(-weights[selected_columns])
+        ordered_columns = selected_columns[weight_order]
+
+        # Assign the ordered columns to the class attribute
+        self.rule_columns_ = ordered_columns
+
+        # Iterate over the columns and fill the rules dictionary
+        for i, col in enumerate(ordered_columns):
+            treeno, leafno, label, sdist = self.rule_info_[col]
+            fit_tree = self.decision_trees_[treeno]
+            rule = self._get_rule(fit_tree, leafno)
+            rule.label = label
+            rule.weight = weights[col]
+            rule.sdist = sdist
+            self.decision_rules_[i] = rule
+
+    def _predict_base(
+        self,
+        x: np.ndarray,
+        indices: list = None,
+        threshold: float = 0,
+        *,
+        predict_info=False,
+    ) -> np.ndarray:
+        """
+        Calculates the base class weights for each instance based on selected rules.
+        Optionally returns additional prediction info.
+
+        Parameters
+        ----------
+        x : np.ndarray
+            The feature matrix for the instances to predict.
+        indices : list, optional
+            Specific indices of rules to use for prediction. If None, all rules
+            are used.
+        threshold : float, default=0
+            The threshold for selecting rules based on their weights.
+        predict_info : bool, default=False
+            If True, returns additional information about the prediction process
+            including indices of samples with missed values, number of rules
+            applied per sample, and average rule length per sample. Otherwise,
+            returns only the array of raw class weights.
+
+        Returns
+        -------
+        np.ndarray
+            An array of raw class weights for each instance, used as the basis for final
+            prediction.
+            If predict_info is True, also returns arrays containing indices of samples
+            with missed values, number of rules applied per sample, and average rule
+            length per sample.
+        """
+
+        # Check if the model has been fitted
+        if indices is None:
+            indices = []
+
+        if not self._is_fitted:
+            raise ValueError("You need to fit the RUG model first")
+
+        # If no specific indices are provided, use all rule indices
+        if len(indices) == 0:
+            indices = list(self.decision_rules_.keys())
+
+        # If provided indices exceed the available rules, return a warning
+        elif np.max(indices) > len(self.decision_rules_):
+            raise ValueError(f"There are only {len(self.decision_rules_)} rules")
+
+        selected_rules = []
+        for rule_index in indices:
+            rule = self.decision_rules_[rule_index]
+            if rule.weight >= threshold:
+                selected_rules.append(rule)
+
+        rule_matrix = np.zeros((x.shape[0], len(selected_rules)), dtype=np.int8)
+        rule_weights = np.zeros(len(selected_rules), dtype=np.float32)
+        rule_lengths = np.zeros(len(selected_rules), dtype=np.uint8)
+        rule_labels = np.zeros(len(selected_rules), dtype=np.uint8)
+
+        for rule_index, rule in enumerate(selected_rules):
+            rule_weights[rule_index] = rule.weight
+            rule_lengths[rule_index] = len(rule)
+            rule_labels[rule_index] = rule.label
+            rule_matrix[:, rule_index] = rule.check_rule(x)
+
+        sum_class_weights_arr = np.zeros(shape=(x.shape[0], self.k_), dtype=np.float32)
+
+        for rule_label in range(self.k_):
+            rule_index = np.where(rule_labels == rule_label)[0]
+            selected_rules = rule_matrix[:, rule_index]
+            sum_class_weights_arr[:, rule_label] = np.sum(
+                selected_rules * rule_weights[rule_index], axis=1
+            )
+
+        # Return the array of class weights
+        if predict_info:
+            warnings.warn(
+                "Enabling 'predict_info' will return additional prediction "
+                "details, including indices of samples with missed values, "
+                "number of rules applied per sample, and average rule length "
+                "per sample. While this information is useful for in-depth "
+                "analysis, it may increase computational overhead and complexity "
+                "of result interpretation. Use this feature judiciously."
+            )
+            rules_per_sample_ = np.sum(rule_matrix)
+            missed_values_index_ = np.where(rules_per_sample_ == 0)[0]
+
+            # Sıfır olmayan elemanların maskesini oluştur
+            non_zero_mask = rule_matrix != 0
+            rule_length_per_sample_ = np.sum(
+                rule_matrix * rule_lengths, axis=1, where=non_zero_mask
+            ) / np.sum(non_zero_mask, axis=1)
+            return missed_values_index_, rules_per_sample_, rule_length_per_sample_
+
+        else:
+            return sum_class_weights_arr
+
+    def predict(
+        self,
+        x: ArrayLike,
+        indices: list | None = None,
+        threshold: float = 0.0,
+        *,
+        predict_info: bool = False,
+    ) -> np.ndarray:
+        """
+        Predicts class labels for the given data, optionally returning
+        additional prediction info.
+
+        Parameters
+        ----------
+        x : array-like of shape (n_samples, n_features)
+            The training input samples. Internally, it will be converted to dtype=np.float32.
+        indices : list or None, default=None
+            Specific indices of rules to use for prediction. If None,
+            all rules are used.
+        threshold : float, default=0
+            The threshold for selecting rules based on their weights.
+        predict_info : bool, default=False
+            If True, returns additional information about the prediction
+            process including indices of samples with missed values, number
+            of rules applied per sample, and average rule length per sample.
+            Otherwise, returns only the predicted class labels.
+
+        Returns
+        -------
+        np.ndarray
+            An array of predicted class labels for each instance in `x`.
+            If predict_info is True, also returns arrays containing indices
+            of samples with missed values, number of rules applied per sample,
+            and average rule length per sample.
+        """
+
+        if indices is None:
+            indices = []
+
+        x = check_inputs(x)
+
+        if predict_info:
+            missed_values_index_, rules_per_sample_, rule_length_per_sample_ = (
+                self._predict_base(x, indices, threshold, predict_info=predict_info)
+            )
+            return missed_values_index_, rules_per_sample_, rule_length_per_sample_
+
+        else:
+            sum_class_weights_arr = self._predict_base(
+                x, indices, threshold, predict_info=predict_info
+            )
+
+            near_zero_mask = np.sum(sum_class_weights_arr, axis=1) <= 1e-6
+            predictions = np.argmax(sum_class_weights_arr, axis=1)
+            predictions[near_zero_mask] = self.majority_class_
+            predictions = self.classes_[predictions]
+
+            return predictions
+
+    def predict_proba(
+        self,
+        x: ArrayLike,
+        indices: list | None = None,
+        threshold: float = 0.0,
+        *,
+        predict_info: bool = False,
+    ):
+        """
+        Predicts class probabilities for the given data, optionally
+        returning additional prediction info.
+
+        Parameters
+        ----------
+        x : array-like of shape (n_samples, n_features)
+            The training input samples. Internally, it will be converted to dtype=np.float32.
+        indices : list or None, default=None
+            Specific indices of rules to use for calculating probabilities.
+            If None, all rules are used.
+        threshold : float, default=0
+            The threshold for selecting rules based on their weights.
+        predict_info : bool, default=False
+            If True, returns additional information about the prediction process
+            including indices of samples with missed values, number of rules applied
+            per sample, and average rule length per sample. Otherwise, returns only
+            the probabilities of each class for each sample.
+
+        Returns
+        -------
+        np.ndarray
+            An array where each row corresponds to a sample in `x` and each column
+            to a class, containing the probability of each class for each sample.
+            If predict_info is True, also returns arrays containing indices of samples
+            with missed values, number of rules applied per sample, and average rule
+            length per sample.
+        """
+
+        if indices is None:
+            indices = []
+
+        x = check_inputs(x)
+
+        if predict_info:
+            missed_values_index_, rules_per_sample_, rule_length_per_sample_ = (
+                self._predict_base(x, indices, threshold, predict_info=predict_info)
+            )
+            return missed_values_index_, rules_per_sample_, rule_length_per_sample_
+
+        else:
+            sum_class_weights_arr = self._predict_base(
+                x, indices, threshold, predict_info=predict_info
+            )
+
+            total_weights = np.sum(sum_class_weights_arr, axis=1)
+            near_zero_total_weight = total_weights <= 1e-6
+            predictions = np.divide(sum_class_weights_arr, total_weights.reshape(-1, 1))
+            predictions[near_zero_total_weight, :] = self.majority_probability_
+
+            return predictions
+
+    def _get_sample_weight(self, sample_weight, class_weight, y):
+        """
+        Calculates the final sample weights based on initial sample weights, class weights and
+        target values.
+
+        Parameters
+        ----------
+        sample_weight : array-like, shape (n_samples,) or None
+            Initial weights of samples. If None, all samples are assumed to have weight one.
+        class_weight : dict, "balanced" or None
+            Weights associated with classes in the form {class_label: weight}. Can be "balanced"
+            to automatically adjust weights inversely proportional to class frequencies in the input data
+            or None for equal weights.
+        y : array-like, shape (n_samples,)
+            Array of target values (class labels).
+
+        Returns
+        -------
+        final_sample_weights : array-like, shape (n_samples,) or None
+            The computed array of weights for each sample in the dataset. Returns None if all computed
+            weights are equal to one, indicating no weighting is necessary.
+        """
+        if class_weight is None and sample_weight is None:
+            return None
+
+        if sample_weight is not None:
+            if (
+                not isinstance(sample_weight, np.ndarray)
+                or sample_weight.shape != y.shape
+            ):
+                raise ValueError(
+                    "sample_weight must be a numpy array of the same shape as y."
+                )
+
+        final_sample_weights = np.ones_like(y, dtype=np.float64)
+
+        if class_weight is not None:
+            if isinstance(class_weight, dict):
+                if len(class_weight.keys()) != np.unique(y).size:
+                    raise ValueError(
+                        "The class_weight dictionary must have a key for each unique value in y."
+                    )
+
+            final_sample_weights *= compute_sample_weight(class_weight, y)
+        if sample_weight is not None:
+            final_sample_weights *= sample_weight
+
+        if np.min(final_sample_weights) != 1:
+            warnings.warn(
+                "Minimum sample weight automatically scaled to 1 for consistency."
+            )
+            final_sample_weights /= np.min(final_sample_weights)
+
+        return final_sample_weights
+
+    def _validate_rug_parameters(
+        self,
+        threshold: float,
+        solver: OptimizationSolver,
+        class_weight: Dict[int:float],
+        random_state: int | None,
+        rule_cost,
+    ):
+        if not isinstance(threshold, (float, int)) or threshold < 0:
+            raise TypeError("threshold must be a non-negative float or integer.")
+
+        if not isinstance(solver, (OptimizationSolver)):
+            raise TypeError("solver should be inherited from OptimizationSolver.")
+
+        if not (isinstance(random_state, int) or random_state is None):
+            raise TypeError("random_state must be an integer or None.")
+
+        if not isinstance(rule_cost, (int, RuleCost)):
+            raise TypeError("rule_cost must be an instance of RuleCost or an integer.")
+
+        if isinstance(rule_cost, int) and rule_cost < 0:
+            raise ValueError("If rule_cost is an integer, it must be non-negative.")
+
+        if not (
+            isinstance(class_weight, dict)
+            and all(
+                isinstance(k, int) and isinstance(v, float)
+                for k, v in class_weight.items()
+            )
+            or class_weight in ["balanced", None]
+        ):
+            raise TypeError(
+                "class_weight must be a dictionary with integer keys and float values, 'balanced', or None."
+            )
+
+    @property
+    def is_fitted(self):
+        """
+        Indicates whether the model is fitted.
+
+        Returns
+        -------
+        bool
+            True if the model is fitted, False otherwise.
+        """
+
+        return self._is_fitted
+
+    @property
+    def decision_rules(self):
+        """
+        Returns the rules extracted from the decision trees, after optimization.
+
+        Returns
+        -------
+        Dict[int, Rule]
+            A dictionary where keys are rule indices and values are Rule objects.
+        """
+
+        return self.decision_rules_
+
+    @property
+    def decision_trees(self):
+        """
+        Returns dictionary that stores the decision tree models.
+
+        Returns
+        -------
+        Dict[int, Any]
+            A dictionary containing decision tree models, with identifiers as keys
+            and decision
+            tree instances as values.
+        """
+        return self.decision_trees_
+
+    @property
+    def rule_info(self):
+        """
+        Returns information about each rule.
+
+        Returns
+        -------
+        Dict[int, Tuple[int, int, int, np.ndarray]]
+            A dictionary with rule indices as keys and tuples containing information about
+            each rule as values. The tuple structure is (rule_id, feature_index, threshold,
+            values_array).
+        """
+
+        return self.rule_info_
+
+    @property
+    def coefficients(self):
+        """
+        Stores coefficients associated with the rules during optimization.
+
+        Returns
+        -------
+        Coefficients
+            An object or array-like structure storing coefficients related to each rule.
+        """
+
+        return self.coefficients_
+
+    @property
+    def majority_class(self):
+        """
+        Returns the class label of the majority class in the dataset.
+
+        Returns
+        -------
+        int
+            The label of the majority class.
+        """
+
+        return self.majority_class_
+
+    @property
+    def majority_probability(self):
+        """
+        Returns the probability of the majority class in the dataset.
+
+        Returns
+        -------
+        float
+            The probability of encountering the majority class in the dataset.
+        """
+
+        return self.majority_probability_
+
+    @property
+    def k(self):
+        """
+        Returns the total number of unique classes in the dataset.
+
+        Returns
+        -------
+        float
+            The total number of unique classes.
+        """
+
+        return self.k_
+
+    @property
+    def classes(self):
+        """
+        Returns unique class labels in the dataset.
+
+        Returns
+        -------
+        np.ndarray
+            An array containing the unique class labels of the dataset.
+        """
+
+        return self.classes_
+
+    @property
+    def rule_columns(self):
+        """
+        Returns indices of rules selected as part of the model.
+
+        Returns
+        -------
+        np.ndarray
+            An array of indices corresponding to the rules included in the model.
+        """
+
+        return self.rule_columns_
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## ruleopt/estimator/lightgbm_.py

```diff
@@ -1,327 +1,327 @@
-from __future__ import annotations
-import numpy as np
-from numpy.typing import ArrayLike
-
-from .base import _RUGBASE
-from ..aux_classes import Rule
-from ..rule_cost import Gini
-from ..utils import check_module_available, check_inputs
-
-LIGHTGBM_AVAILABLE = check_module_available("lightgbm")
-
-
-class RUXLGBMClassifier(_RUGBASE):
-    """
-    A classifier that extracts and optimizes decision rules from a trained
-    LightGBM ensemble model to create a compact and interpretable model.
-    This process involves translating the ensemble's trees into a set of rules and
-    using optimization to balance model accuracy and interpretability. The complexity
-    of the resulting rule-based model is  controlled through a penalty parameter.
-    """
-
-    def __new__(cls, *args, **kwargs):
-        if not LIGHTGBM_AVAILABLE:
-            raise ImportError(
-                "LightGBM is required for this class but is not installed.",
-                "Please install it with 'pip install lightgbm'",
-            )
-        instance = super(RUXLGBMClassifier, cls).__new__(cls)
-        return instance
-
-    def __init__(
-        self,
-        trained_ensemble,
-        solver,
-        *,
-        rule_cost=Gini(),
-        class_weight: dict | str | None = None,
-        threshold: float = 1.0e-6,
-        random_state: int | None = None,
-    ):
-        """
-        Parameters
-        ----------
-        trained_ensemble : lightgbm.LGBMClassifier or lightgbm.Booster
-            The trained LightGBM ensemble model from which the rules will be extracted.
-
-        solver : OptimizationSolver
-            An instance of a derived class inherits from the 'Optimization Solver' base class.
-            The solver is responsible for optimizing the rule set based on the cost function
-            and constraints.
-
-        rule_cost : RuleCost or int, default=Gini()
-            Defines the cost of rules, either as a specific calculation method (RuleCost instance)
-            or a fixed cost
-
-        class_weight: dict, "balanced" or None, default=None
-            A dictionary mapping class labels to their respective weights, the string "balanced"
-            to automatically adjust weights inversely proportional to class frequencies,
-            or None for no weights. Used to adjust the model in favor of certain classes.
-
-        threshold : float, default=1.0e-6
-            The minimum weight threshold for including a rule in the final model.
-
-        random_state : int or None, default=None
-            Seed for the random number generator to ensure reproducible results.
-            Defaults to None.
-        """
-        ### LAZY INIT
-        from lightgbm import Booster
-
-        if not isinstance(trained_ensemble, Booster):
-            if hasattr(trained_ensemble, "booster_"):
-                if not isinstance(trained_ensemble.booster_, Booster):
-                    raise TypeError("trained_ensemble is not an instance of LightGBM.")
-                else:
-                    self.trained_ensemble = trained_ensemble.booster_
-            else:
-                raise TypeError("trained_ensemble is not an instance of LightGBM")
-        else:
-            self.trained_ensemble = trained_ensemble
-
-        super().__init__(
-            threshold=threshold,
-            random_state=random_state,
-            rule_cost=rule_cost,
-            solver=solver,
-            class_weight=class_weight,
-        )
-
-    def _find_leaf_index(self, fit_tree: dict, leaf_index: int, path=None) -> list:
-        """
-        Recursively finds the path from the root to the specified leaf node in a LightGBM tree.
-
-        Parameters
-        ----------
-        fit_tree : dict
-            The tree dictionary from a LightGBM model.
-        leaf_index : int
-            The target leaf node's ID.
-        path : list, optional
-            The path taken to reach the current node, used in recursive calls. Defaults to None.
-
-        Returns
-        -------
-        list
-            The path from the root to the leaf node, represented as a list of node IDs.
-        """
-        if path is None:
-            path = []
-        if isinstance(fit_tree, dict):
-            # If the current fit_tree is the leaf node we're looking for
-            if "leaf_index" in fit_tree and fit_tree["leaf_index"] == leaf_index:
-                return path
-            # Recursively look in the items of the fit_tree
-            for key, value in fit_tree.items():
-                result = self._find_leaf_index(value, leaf_index, path + [key])
-                if result:
-                    return result
-
-    def _get_rule(self, fit_tree: dict, nodeid: int) -> Rule:
-        """
-        Constructs a rule corresponding to the path leading to a specific leaf node in
-        a LightGBM tree.
-
-        Parameters
-        ----------
-        fit_tree : dict
-            The tree structure extracted from a LightGBM model, typically in JSON format.
-        nodeid : int
-            The unique identifier of the leaf node for which to construct the rule.
-
-        Returns
-        -------
-        Rule
-            An object representing the decision rule leading to the specified leaf node.
-        """
-        # Get the path to the leaf node
-        leaf_path = self._find_leaf_index(fit_tree, nodeid)
-
-        # Initialize the rule
-        return_rule = Rule()
-
-        # child variable is used to track the last traversed node
-        child = None
-
-        # While there are still nodes in the path to the leaf node
-        while leaf_path:
-            fit_tree_ = fit_tree.copy()
-            for path in leaf_path:
-                fit_tree_ = fit_tree_[path]
-
-            # If the current node is a split node, add a clause to the rule
-            if "split_feature" in fit_tree_.keys():
-                # Check which child node we're coming from
-                is_left = child == "left_child"
-                # Get the threshold for the split
-                threshold = fit_tree_["threshold"]
-                # Get the default path in case of missing values
-                missing = (is_left and fit_tree_["default_left"]) or (
-                    child == "right_child" and not fit_tree_["default_left"]
-                )
-
-                feature = fit_tree_["split_feature"]
-                ub = threshold if is_left else np.inf
-                lb = -np.inf if is_left else threshold
-                na = missing
-
-                return_rule.add_clause(feature, ub, lb, na)
-
-            # Move to the next node in the path
-            child = leaf_path.pop()
-
-        return return_rule
-
-    def _get_matrix(
-        self,
-        y: np.ndarray,
-        vec_y: np.ndarray,
-        fit_tree: dict,
-        treeno: int,
-        y_rules: np.ndarray,
-    ):
-        """
-        Populates the matrices for the optimization problem based on a single LightGBM tree.
-
-        Parameters
-        ----------
-        y : np.ndarray
-            The target vector of the training data.
-        vec_y : np.ndarray
-            The preprocessed target vector, suitable for the optimization problem.
-        fit_tree : dict
-            A single decision tree's structure from LightGBM, in dictionary form.
-        treeno : int
-            The index of the current tree within the ensemble.
-        y_rules : np.ndarray
-            The array of leaf indices for each sample in the training data, determined by
-            the current tree.
-        """
-        # If the coefficients matrix is empty, start from the first column
-        if self.coefficients_.cols.shape[0] == 0:
-            col = 0
-        else:
-            # Otherwise, start from the next available column
-            col = np.max(self.coefficients_.cols) + 1
-
-        # Get the leaf node for each sample in x
-        y_rules = y_rules[:, treeno]
-
-        # Iterate over unique leaf nodes
-        for leafno in np.unique(y_rules):
-            # Get the samples in the leaf
-            covers = np.where(y_rules == leafno)[0]
-            leaf_y_vals = y[covers]  # y values of the samples in the leaf
-
-            # Get unique labels in the leaf and their counts
-            unique_labels, counts = np.unique(leaf_y_vals, return_counts=True)
-
-            # Identify the majority class in the leaf
-            label = unique_labels[np.argmax(counts)]
-
-            # Create a vector for this label
-            label_vector = np.full((self.k_,), -1 / (self.k_ - 1))
-            label_vector[label] = 1
-
-            # Calculate fill_ahat, which will be used to update yvals in the coefficients matrix
-            fill_ahat = np.dot(vec_y[covers, :], label_vector)
-
-            # Update the coefficients matrix with the new information
-            self.coefficients_.rows = np.concatenate((self.coefficients_.rows, covers))
-            self.coefficients_.cols = np.concatenate(
-                (self.coefficients_.cols, [col] * covers.shape[0])
-            )
-            self.coefficients_.yvals = np.concatenate(
-                (self.coefficients_.yvals, np.full(covers.shape[0], fill_ahat))
-            )
-
-            cost = self._get_rule_cost(
-                temp_rule=self._get_rule(fit_tree, leafno),
-                covers=covers,
-                counts=counts,
-                y=y,
-            )
-
-            # Append the cost to the costs in the coefficients matrix
-            self.coefficients_.costs = np.concatenate(
-                (self.coefficients_.costs, [cost])
-            )
-
-            # Calculate the distribution of the samples in the leaf across the classess
-            sdist = counts
-            self.rule_info_[col] = (treeno, leafno, label, sdist)
-            col += 1
-
-    def _get_matrices(self, x: np.ndarray, y: np.ndarray, vec_y: np.ndarray):
-        """
-        Generates the coefficient matrices for the optimization problem from all
-        trees in the LightGBM ensemble.
-
-        Parameters
-        ----------
-        x : np.ndarray
-            The feature matrix of the training data.
-        y : np.ndarray
-            The target vector of the training data.
-        vec_y : np.ndarray
-            The preprocessed target vector.
-        """
-        y_rules = self.trained_ensemble.predict(x, pred_leaf=True).astype(np.intp)
-        for treeno, fit_tree in enumerate(self.decision_trees_.values()):
-            self._get_matrix(y, vec_y, fit_tree, treeno, y_rules)
-
-    def fit(self, x: ArrayLike, y: ArrayLike, sample_weight: ArrayLike | None = None):
-        """
-        Fits the RUXLGBMClassifier to the training data, optimizing the
-        extracted rules for a balance between accuracy and interpretability.
-
-        Parameters
-        ----------
-        x : array-like of shape (n_samples, n_features)
-            The training input samples. Internally, it will be converted to dtype=np.float32.
-        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
-            The target values (class labels) as integers
-        sample_weight : array-like of shape (n_samples,), default=None
-            Sample weights. If None, then samples are equally weighted.
-
-        Returns
-        -------
-        RUXLGBMClassifier
-            The fitted model, ready for making predictions.
-        """
-
-        x, y = check_inputs(x, y)
-
-        # If the model has been fitted before, clean it up
-        if self.coefficients_.cols.shape[0] != 0:
-            self._cleanup()
-
-        # Fills the fitted decision trees.
-        tree_infos = self.trained_ensemble.dump_model()["tree_info"]
-        for treeno, fit_tree in enumerate(tree_infos):
-            self.decision_trees_[treeno] = fit_tree
-
-        # Extract and set properties of the target variable
-        self._get_class_infos(y)
-
-        # Preprocess the target values
-        vec_y = self._preprocess(y)
-
-        # Calculate the coefficients and other parameters for the optimization problem
-        self._get_matrices(x=x, y=y, vec_y=vec_y)
-
-        sample_weight = self._get_sample_wight(sample_weight, self.class_weight, y)
-
-        # Solve the optimization problem again with the new rules
-        ws, *_ = self.solver(
-            coefficients=self.coefficients_, k=self.k_, sample_weight=sample_weight
-        )
-
-        # Fill the decision rules based on the weights obtained from the optimization problem
-        self._fill_rules(ws)
-
-        # Mark the model as fitted
-        self._is_fitted = True
-
-        # Return the fitted model
-        return self
+from __future__ import annotations
+import numpy as np
+from numpy.typing import ArrayLike
+
+from .base import _RUGBASE
+from ..aux_classes import Rule
+from ..rule_cost import Gini
+from ..utils import check_module_available, check_inputs
+
+LIGHTGBM_AVAILABLE = check_module_available("lightgbm")
+
+
+class RUXLGBMClassifier(_RUGBASE):
+    """
+    A classifier that extracts and optimizes decision rules from a trained
+    LightGBM ensemble model to create a compact and interpretable model.
+    This process involves translating the ensemble's trees into a set of rules and
+    using optimization to balance model accuracy and interpretability. The complexity
+    of the resulting rule-based model is  controlled through a penalty parameter.
+    """
+
+    def __new__(cls, *args, **kwargs):
+        if not LIGHTGBM_AVAILABLE:
+            raise ImportError(
+                "LightGBM is required for this class but is not installed.",
+                "Please install it with 'pip install lightgbm'",
+            )
+        instance = super(RUXLGBMClassifier, cls).__new__(cls)
+        return instance
+
+    def __init__(
+        self,
+        trained_ensemble,
+        solver,
+        *,
+        rule_cost=Gini(),
+        class_weight: dict | str | None = None,
+        threshold: float = 1.0e-6,
+        random_state: int | None = None,
+    ):
+        """
+        Parameters
+        ----------
+        trained_ensemble : lightgbm.LGBMClassifier or lightgbm.Booster
+            The trained LightGBM ensemble model from which the rules will be extracted.
+
+        solver : OptimizationSolver
+            An instance of a derived class inherits from the 'Optimization Solver' base class.
+            The solver is responsible for optimizing the rule set based on the cost function
+            and constraints.
+
+        rule_cost : RuleCost or int, default=Gini()
+            Defines the cost of rules, either as a specific calculation method (RuleCost instance)
+            or a fixed cost
+
+        class_weight: dict, "balanced" or None, default=None
+            A dictionary mapping class labels to their respective weights, the string "balanced"
+            to automatically adjust weights inversely proportional to class frequencies,
+            or None for no weights. Used to adjust the model in favor of certain classes.
+
+        threshold : float, default=1.0e-6
+            The minimum weight threshold for including a rule in the final model.
+
+        random_state : int or None, default=None
+            Seed for the random number generator to ensure reproducible results.
+            Defaults to None.
+        """
+        ### LAZY INIT
+        from lightgbm import Booster
+
+        if not isinstance(trained_ensemble, Booster):
+            if hasattr(trained_ensemble, "booster_"):
+                if not isinstance(trained_ensemble.booster_, Booster):
+                    raise TypeError("trained_ensemble is not an instance of LightGBM.")
+                else:
+                    self.trained_ensemble = trained_ensemble.booster_
+            else:
+                raise TypeError("trained_ensemble is not an instance of LightGBM")
+        else:
+            self.trained_ensemble = trained_ensemble
+
+        super().__init__(
+            threshold=threshold,
+            random_state=random_state,
+            rule_cost=rule_cost,
+            solver=solver,
+            class_weight=class_weight,
+        )
+
+    def _find_leaf_index(self, fit_tree: dict, leaf_index: int, path=None) -> list:
+        """
+        Recursively finds the path from the root to the specified leaf node in a LightGBM tree.
+
+        Parameters
+        ----------
+        fit_tree : dict
+            The tree dictionary from a LightGBM model.
+        leaf_index : int
+            The target leaf node's ID.
+        path : list, optional
+            The path taken to reach the current node, used in recursive calls. Defaults to None.
+
+        Returns
+        -------
+        list
+            The path from the root to the leaf node, represented as a list of node IDs.
+        """
+        if path is None:
+            path = []
+        if isinstance(fit_tree, dict):
+            # If the current fit_tree is the leaf node we're looking for
+            if "leaf_index" in fit_tree and fit_tree["leaf_index"] == leaf_index:
+                return path
+            # Recursively look in the items of the fit_tree
+            for key, value in fit_tree.items():
+                result = self._find_leaf_index(value, leaf_index, path + [key])
+                if result:
+                    return result
+
+    def _get_rule(self, fit_tree: dict, nodeid: int) -> Rule:
+        """
+        Constructs a rule corresponding to the path leading to a specific leaf node in
+        a LightGBM tree.
+
+        Parameters
+        ----------
+        fit_tree : dict
+            The tree structure extracted from a LightGBM model, typically in JSON format.
+        nodeid : int
+            The unique identifier of the leaf node for which to construct the rule.
+
+        Returns
+        -------
+        Rule
+            An object representing the decision rule leading to the specified leaf node.
+        """
+        # Get the path to the leaf node
+        leaf_path = self._find_leaf_index(fit_tree, nodeid)
+
+        # Initialize the rule
+        return_rule = Rule()
+
+        # child variable is used to track the last traversed node
+        child = None
+
+        # While there are still nodes in the path to the leaf node
+        while leaf_path:
+            fit_tree_ = fit_tree.copy()
+            for path in leaf_path:
+                fit_tree_ = fit_tree_[path]
+
+            # If the current node is a split node, add a clause to the rule
+            if "split_feature" in fit_tree_.keys():
+                # Check which child node we're coming from
+                is_left = child == "left_child"
+                # Get the threshold for the split
+                threshold = fit_tree_["threshold"]
+                # Get the default path in case of missing values
+                missing = (is_left and fit_tree_["default_left"]) or (
+                    child == "right_child" and not fit_tree_["default_left"]
+                )
+
+                feature = fit_tree_["split_feature"]
+                ub = threshold if is_left else np.inf
+                lb = -np.inf if is_left else threshold
+                na = missing
+
+                return_rule.add_clause(feature, ub, lb, na)
+
+            # Move to the next node in the path
+            child = leaf_path.pop()
+
+        return return_rule
+
+    def _get_matrix(
+        self,
+        y: np.ndarray,
+        vec_y: np.ndarray,
+        fit_tree: dict,
+        treeno: int,
+        y_rules: np.ndarray,
+    ):
+        """
+        Populates the matrices for the optimization problem based on a single LightGBM tree.
+
+        Parameters
+        ----------
+        y : np.ndarray
+            The target vector of the training data.
+        vec_y : np.ndarray
+            The preprocessed target vector, suitable for the optimization problem.
+        fit_tree : dict
+            A single decision tree's structure from LightGBM, in dictionary form.
+        treeno : int
+            The index of the current tree within the ensemble.
+        y_rules : np.ndarray
+            The array of leaf indices for each sample in the training data, determined by
+            the current tree.
+        """
+        # If the coefficients matrix is empty, start from the first column
+        if self.coefficients_.cols.shape[0] == 0:
+            col = 0
+        else:
+            # Otherwise, start from the next available column
+            col = np.max(self.coefficients_.cols) + 1
+
+        # Get the leaf node for each sample in x
+        y_rules = y_rules[:, treeno]
+
+        # Iterate over unique leaf nodes
+        for leafno in np.unique(y_rules):
+            # Get the samples in the leaf
+            covers = np.where(y_rules == leafno)[0]
+            leaf_y_vals = y[covers]  # y values of the samples in the leaf
+
+            # Get unique labels in the leaf and their counts
+            unique_labels, counts = np.unique(leaf_y_vals, return_counts=True)
+
+            # Identify the majority class in the leaf
+            label = unique_labels[np.argmax(counts)]
+
+            # Create a vector for this label
+            label_vector = np.full((self.k_,), -1 / (self.k_ - 1))
+            label_vector[label] = 1
+
+            # Calculate fill_ahat, which will be used to update yvals in the coefficients matrix
+            fill_ahat = np.dot(vec_y[covers, :], label_vector)
+
+            # Update the coefficients matrix with the new information
+            self.coefficients_.rows = np.concatenate((self.coefficients_.rows, covers))
+            self.coefficients_.cols = np.concatenate(
+                (self.coefficients_.cols, [col] * covers.shape[0])
+            )
+            self.coefficients_.yvals = np.concatenate(
+                (self.coefficients_.yvals, np.full(covers.shape[0], fill_ahat))
+            )
+
+            cost = self._get_rule_cost(
+                temp_rule=self._get_rule(fit_tree, leafno),
+                covers=covers,
+                counts=counts,
+                y=y,
+            )
+
+            # Append the cost to the costs in the coefficients matrix
+            self.coefficients_.costs = np.concatenate(
+                (self.coefficients_.costs, [cost])
+            )
+
+            # Calculate the distribution of the samples in the leaf across the classess
+            sdist = counts
+            self.rule_info_[col] = (treeno, leafno, label, sdist)
+            col += 1
+
+    def _get_matrices(self, x: np.ndarray, y: np.ndarray, vec_y: np.ndarray):
+        """
+        Generates the coefficient matrices for the optimization problem from all
+        trees in the LightGBM ensemble.
+
+        Parameters
+        ----------
+        x : np.ndarray
+            The feature matrix of the training data.
+        y : np.ndarray
+            The target vector of the training data.
+        vec_y : np.ndarray
+            The preprocessed target vector.
+        """
+        y_rules = self.trained_ensemble.predict(x, pred_leaf=True).astype(np.intp)
+        for treeno, fit_tree in enumerate(self.decision_trees_.values()):
+            self._get_matrix(y, vec_y, fit_tree, treeno, y_rules)
+
+    def fit(self, x: ArrayLike, y: ArrayLike, sample_weight: ArrayLike | None = None):
+        """
+        Fits the RUXLGBMClassifier to the training data, optimizing the
+        extracted rules for a balance between accuracy and interpretability.
+
+        Parameters
+        ----------
+        x : array-like of shape (n_samples, n_features)
+            The training input samples. Internally, it will be converted to dtype=np.float32.
+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
+            The target values (class labels) as integers
+        sample_weight : array-like of shape (n_samples,), default=None
+            Sample weights. If None, then samples are equally weighted.
+
+        Returns
+        -------
+        RUXLGBMClassifier
+            The fitted model, ready for making predictions.
+        """
+
+        x, y = check_inputs(x, y)
+
+        # If the model has been fitted before, clean it up
+        if self.coefficients_.cols.shape[0] != 0:
+            self._cleanup()
+
+        # Fills the fitted decision trees.
+        tree_infos = self.trained_ensemble.dump_model()["tree_info"]
+        for treeno, fit_tree in enumerate(tree_infos):
+            self.decision_trees_[treeno] = fit_tree
+
+        # Extract and set properties of the target variable
+        self._get_class_infos(y)
+
+        # Preprocess the target values
+        vec_y = self._preprocess(y)
+
+        # Calculate the coefficients and other parameters for the optimization problem
+        self._get_matrices(x=x, y=y, vec_y=vec_y)
+
+        sample_weight = self._get_sample_weight(sample_weight, self.class_weight, y)
+
+        # Solve the optimization problem again with the new rules
+        ws, *_ = self.solver(
+            coefficients=self.coefficients_, k=self.k_, sample_weight=sample_weight
+        )
+
+        # Fill the decision rules based on the weights obtained from the optimization problem
+        self._fill_rules(ws)
+
+        # Mark the model as fitted
+        self._is_fitted = True
+
+        # Return the fitted model
+        return self
```

## ruleopt/estimator/xgboost_.py

```diff
@@ -1,306 +1,306 @@
-from __future__ import annotations
-import re
-import numpy as np
-import pandas as pd
-from numpy.typing import ArrayLike
-
-from .base import _RUGBASE
-from ..aux_classes import Rule
-from ..rule_cost import Gini
-from ..utils import check_module_available, check_inputs
-
-
-XGBOOST_AVAILABLE = check_module_available("xgboost")
-
-
-class RUXXGBClassifier(_RUGBASE):
-    """
-    A classifier that extracts and optimizes decision rules from a trained
-    XGBoost ensemble model to create a compact and interpretable model.
-    This process involves translating the ensemble's trees into a set of rules and
-    using optimization to balance model accuracy and interpretability. The complexity
-    of the resulting rule-based model is  controlled through a penalty parameter.
-    """
-
-    def __new__(cls, *args, **kwargs):
-        if not XGBOOST_AVAILABLE:
-            raise ImportError(
-                "XGBoost is required for this class but is not installed.",
-                "Please install it with 'pip install xgboost'",
-            )
-        instance = super(RUXXGBClassifier, cls).__new__(cls)
-        return instance
-
-    def __init__(
-        self,
-        trained_ensemble,
-        solver,
-        *,
-        rule_cost=Gini(),
-        class_weight: dict | str | None = None,
-        threshold: float = 1.0e-6,
-        random_state: int | None = None,
-    ):
-        """
-        Parameters
-        ----------
-        trained_ensemble : xgboost.XGBClassifier
-            The trained XGBoost ensemble model from which rules will be extracted.
-            The model should already be trained on the dataset.
-
-        solver : OptimizationSolver
-            An instance of a derived class inherits from the 'Optimization Solver' base class.
-            The solver is responsible for optimizing the rule set based on the cost function
-            and constraints.
-
-        rule_cost : RuleCost or int, default=Gini()
-            Defines the cost of rules, either as a specific calculation method (RuleCost instance)
-            or a fixed cost
-
-        class_weight: dict, "balanced" or None, default=None
-            A dictionary mapping class labels to their respective weights, the string "balanced"
-            to automatically adjust weights inversely proportional to class frequencies,
-            or None for no weights. Used to adjust the model in favor of certain classes.
-
-        threshold : float, default=1.0e-6
-            The minimum weight threshold for including a rule in the final model
-
-        random_state : int or None, default=None
-            Seed for the random number generator to ensure reproducible results.
-            Defaults to None.
-        """
-        ### LAZY IMPORT
-        from xgboost import Booster
-
-        if not isinstance(trained_ensemble, Booster):
-            if hasattr(trained_ensemble, "get_booster"):
-                if not isinstance(trained_ensemble.get_booster(), Booster):
-                    raise TypeError(
-                        "trained_ensemble is not an instance of XGBClassifier."
-                    )
-                else:
-                    self.trained_ensemble = trained_ensemble
-            else:
-                raise TypeError("trained_ensemble is not an instance of XGBoost")
-        else:
-            raise TypeError(
-                "XGBoost Booster instance not supported yet. Use XGBClassifier."
-            )
-
-        super().__init__(
-            threshold=threshold,
-            random_state=random_state,
-            rule_cost=rule_cost,
-            solver=solver,
-            class_weight=class_weight,
-        )
-
-    def _get_rule(self, fit_tree: pd.DataFrame, leaf_index: int) -> Rule:
-        """
-        Extracts a decision rule leading to a specified leaf node from an XGBoost tree.
-
-        Parameters
-        ----------
-        fit_tree : pd.DataFrame
-            The decision trees represented as a DataFrame extracted from the trained XGBoost model.
-        leaf_index : int
-            The ID of the leaf node for which to construct the decision rule.
-
-        Returns
-        -------
-        Rule
-            An object representing the decision rule leading to the specified leaf node,
-            composed of clauses that define the decision path.
-        """
-        # Initialize the rule
-        return_rule = Rule()
-
-        if fit_tree.shape[0] <= 1:
-            return return_rule
-
-        while True:
-            # Find the parent node of the current leaf
-            parent = fit_tree.loc[
-                np.any(fit_tree.loc[:, ["Yes", "No"]] == leaf_index, axis=1), "Node"
-            ].values[0]
-
-            # Extract information about the decision at the parent node
-            feature = int(fit_tree.loc[fit_tree.Node == parent, "Feature"].values[0])
-
-            threshold = fit_tree.loc[fit_tree.Node == parent, "Split"].values[0]
-            is_left = (
-                fit_tree.loc[fit_tree.Node == parent, "Yes"].values[0] == leaf_index
-            )
-            missing = (
-                fit_tree.loc[fit_tree.Node == parent, "Missing"].values[0] == leaf_index
-            )
-
-            ub = threshold if is_left else np.inf
-            lb = -np.inf if is_left else threshold
-            na = missing
-
-            return_rule.add_clause(feature, ub, lb, na)
-
-            # If we reached the root of the tree, break the loop
-            if parent == 0:
-                break
-
-            # Move up the tree
-            leaf_index = parent
-
-        return return_rule
-
-    def _get_matrix(
-        self,
-        y: np.ndarray,
-        vec_y: np.ndarray,
-        fit_tree: pd.DataFrame,
-        treeno: int,
-        y_rules: np.ndarray,
-    ):
-        """
-        Populates the matrices for the optimization problem based on a single XGBoost tree.
-
-        Parameters
-        ----------
-        y : np.ndarray
-            The target vector of the training data.
-        vec_y : np.ndarray
-            The preprocessed target vector, suitable for the optimization problem.
-        fit_tree : pd.DataFrame
-            A single decision tree's structure from XGBoost, represented as a DataFrame.
-        treeno : int
-            The index of the current tree within the ensemble.
-        y_rules : np.ndarray
-            The array of leaf indices for each sample in the training data, determined
-            by the current tree.
-        """
-        # If the coefficients matrix is empty, start from the first column
-        if self.coefficients_.cols.shape[0] == 0:
-            col = 0
-        else:
-            # Otherwise, start from the next available column
-            col = np.max(self.coefficients_.cols) + 1
-
-        # Get the leaf node for each sample in x
-        y_rules = y_rules[:, treeno]
-
-        # Iterate over unique leaf nodes
-        for leafno in np.unique(y_rules):
-            # Get the samples in the leaf
-            covers = np.where(y_rules == leafno)[0]
-            leaf_y_vals = y[covers]  # y values of the samples in the leaf
-
-            # Get unique labels in the leaf and their counts
-            unique_labels, counts = np.unique(leaf_y_vals, return_counts=True)
-
-            # Identify the majority class in the leaf
-            label = unique_labels[np.argmax(counts)]
-
-            # Create a vector for this label
-            label_vector = np.full((self.k_,), -1 / (self.k_ - 1))
-            label_vector[label] = 1
-
-            # Calculate fill_ahat, which will be used to update yvals in the coefficients matrix
-            fill_ahat = np.dot(vec_y[covers, :], label_vector)
-
-            # Update the coefficients matrix with the new information
-            self.coefficients_.rows = np.concatenate((self.coefficients_.rows, covers))
-            self.coefficients_.cols = np.concatenate(
-                (self.coefficients_.cols, [col] * covers.shape[0])
-            )
-            self.coefficients_.yvals = np.concatenate(
-                (self.coefficients_.yvals, np.full(covers.shape[0], fill_ahat))
-            )
-
-            cost = self._get_rule_cost(
-                temp_rule=self._get_rule(fit_tree, leafno),
-                covers=covers,
-                counts=counts,
-                y=y,
-            )
-
-            # Append the cost to the costs in the coefficients matrix
-            self.coefficients_.costs = np.concatenate(
-                (self.coefficients_.costs, [cost])
-            )
-
-            # Calculate the distribution of the samples in the leaf across the classes
-            sdist = counts
-            self.rule_info_[col] = (treeno, leafno, label, sdist)
-            col += 1
-
-    def _get_matrices(self, x: np.ndarray, y: np.ndarray, vec_y: np.ndarray):
-        """
-        Generates the coefficient matrices for the optimization problem from
-        all trees in the XGBoost ensemble.
-
-        Parameters
-        ----------
-        x : np.ndarray
-            The feature matrix of the training data.
-        y : np.ndarray
-            The target vector of the training data.
-        vec_y : np.ndarray
-            The preprocessed target vector.
-        """
-        y_rules = self.trained_ensemble.apply(x).astype(np.intp)
-        for treeno, fit_tree in enumerate(self.decision_trees_.values()):
-            self._get_matrix(y, vec_y, fit_tree, treeno, y_rules)
-
-    def fit(self, x: ArrayLike, y: ArrayLike, sample_weight: ArrayLike | None = None):
-        """
-        Fits the RUXXGBClassifier to the training data, optimizing
-        the extracted rules for a balance between accuracy and interpretability.
-
-        Parameters
-        ----------
-        x : array-like of shape (n_samples, n_features)
-            The training input samples. Internally, it will be converted to dtype=np.float32.
-        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
-            The target values (class labels) as integers
-        sample_weight : array-like of shape (n_samples,), default=None
-            Sample weights. If None, then samples are equally weighted.
-
-        Returns
-        -------
-        RUXXGBClassifier
-            The fitted model, ready for making predictions.
-        """
-        x, y = check_inputs(x, y)
-
-        # If the model has been fitted before, clean it up
-        if self.coefficients_.cols.shape[0] != 0:
-            self._cleanup()
-
-        # Fills the fitted decision trees.
-        out = self.trained_ensemble.get_booster().trees_to_dataframe()
-        pattern = re.compile(r"-(\d+)")
-        columns = ["Yes", "No", "Missing"]
-        out[columns] = out[columns].map(
-            lambda x: (
-                int(pattern.search(x).group(1))
-                if pd.notna(x) and pattern.search(x)
-                else None
-            )
-        )
-        out.Feature = out.Feature.str.lstrip("f")
-        self.decision_trees_ = {
-            treeno: out.loc[out.Tree == treeno] for treeno in out.Tree.unique()
-        }
-
-        self._get_class_infos(y)
-        vec_y = self._preprocess(y)
-
-        self._get_matrices(x=x, y=y, vec_y=vec_y)
-
-        sample_weight = self._get_sample_wight(sample_weight, self.class_weight, y)
-
-        ws, *_ = self.solver(
-            coefficients=self.coefficients_, k=self.k_, sample_weight=sample_weight
-        )
-
-        self._fill_rules(ws)
-        self._is_fitted = True
-
-        return self
+from __future__ import annotations
+import re
+import numpy as np
+import pandas as pd
+from numpy.typing import ArrayLike
+
+from .base import _RUGBASE
+from ..aux_classes import Rule
+from ..rule_cost import Gini
+from ..utils import check_module_available, check_inputs
+
+
+XGBOOST_AVAILABLE = check_module_available("xgboost")
+
+
+class RUXXGBClassifier(_RUGBASE):
+    """
+    A classifier that extracts and optimizes decision rules from a trained
+    XGBoost ensemble model to create a compact and interpretable model.
+    This process involves translating the ensemble's trees into a set of rules and
+    using optimization to balance model accuracy and interpretability. The complexity
+    of the resulting rule-based model is  controlled through a penalty parameter.
+    """
+
+    def __new__(cls, *args, **kwargs):
+        if not XGBOOST_AVAILABLE:
+            raise ImportError(
+                "XGBoost is required for this class but is not installed.",
+                "Please install it with 'pip install xgboost'",
+            )
+        instance = super(RUXXGBClassifier, cls).__new__(cls)
+        return instance
+
+    def __init__(
+        self,
+        trained_ensemble,
+        solver,
+        *,
+        rule_cost=Gini(),
+        class_weight: dict | str | None = None,
+        threshold: float = 1.0e-6,
+        random_state: int | None = None,
+    ):
+        """
+        Parameters
+        ----------
+        trained_ensemble : xgboost.XGBClassifier
+            The trained XGBoost ensemble model from which rules will be extracted.
+            The model should already be trained on the dataset.
+
+        solver : OptimizationSolver
+            An instance of a derived class inherits from the 'Optimization Solver' base class.
+            The solver is responsible for optimizing the rule set based on the cost function
+            and constraints.
+
+        rule_cost : RuleCost or int, default=Gini()
+            Defines the cost of rules, either as a specific calculation method (RuleCost instance)
+            or a fixed cost
+
+        class_weight: dict, "balanced" or None, default=None
+            A dictionary mapping class labels to their respective weights, the string "balanced"
+            to automatically adjust weights inversely proportional to class frequencies,
+            or None for no weights. Used to adjust the model in favor of certain classes.
+
+        threshold : float, default=1.0e-6
+            The minimum weight threshold for including a rule in the final model
+
+        random_state : int or None, default=None
+            Seed for the random number generator to ensure reproducible results.
+            Defaults to None.
+        """
+        ### LAZY IMPORT
+        from xgboost import Booster
+
+        if not isinstance(trained_ensemble, Booster):
+            if hasattr(trained_ensemble, "get_booster"):
+                if not isinstance(trained_ensemble.get_booster(), Booster):
+                    raise TypeError(
+                        "trained_ensemble is not an instance of XGBClassifier."
+                    )
+                else:
+                    self.trained_ensemble = trained_ensemble
+            else:
+                raise TypeError("trained_ensemble is not an instance of XGBoost")
+        else:
+            raise TypeError(
+                "XGBoost Booster instance not supported yet. Use XGBClassifier."
+            )
+
+        super().__init__(
+            threshold=threshold,
+            random_state=random_state,
+            rule_cost=rule_cost,
+            solver=solver,
+            class_weight=class_weight,
+        )
+
+    def _get_rule(self, fit_tree: pd.DataFrame, leaf_index: int) -> Rule:
+        """
+        Extracts a decision rule leading to a specified leaf node from an XGBoost tree.
+
+        Parameters
+        ----------
+        fit_tree : pd.DataFrame
+            The decision trees represented as a DataFrame extracted from the trained XGBoost model.
+        leaf_index : int
+            The ID of the leaf node for which to construct the decision rule.
+
+        Returns
+        -------
+        Rule
+            An object representing the decision rule leading to the specified leaf node,
+            composed of clauses that define the decision path.
+        """
+        # Initialize the rule
+        return_rule = Rule()
+
+        if fit_tree.shape[0] <= 1:
+            return return_rule
+
+        while True:
+            # Find the parent node of the current leaf
+            parent = fit_tree.loc[
+                np.any(fit_tree.loc[:, ["Yes", "No"]] == leaf_index, axis=1), "Node"
+            ].values[0]
+
+            # Extract information about the decision at the parent node
+            feature = int(fit_tree.loc[fit_tree.Node == parent, "Feature"].values[0])
+
+            threshold = fit_tree.loc[fit_tree.Node == parent, "Split"].values[0]
+            is_left = (
+                fit_tree.loc[fit_tree.Node == parent, "Yes"].values[0] == leaf_index
+            )
+            missing = (
+                fit_tree.loc[fit_tree.Node == parent, "Missing"].values[0] == leaf_index
+            )
+
+            ub = threshold if is_left else np.inf
+            lb = -np.inf if is_left else threshold
+            na = missing
+
+            return_rule.add_clause(feature, ub, lb, na)
+
+            # If we reached the root of the tree, break the loop
+            if parent == 0:
+                break
+
+            # Move up the tree
+            leaf_index = parent
+
+        return return_rule
+
+    def _get_matrix(
+        self,
+        y: np.ndarray,
+        vec_y: np.ndarray,
+        fit_tree: pd.DataFrame,
+        treeno: int,
+        y_rules: np.ndarray,
+    ):
+        """
+        Populates the matrices for the optimization problem based on a single XGBoost tree.
+
+        Parameters
+        ----------
+        y : np.ndarray
+            The target vector of the training data.
+        vec_y : np.ndarray
+            The preprocessed target vector, suitable for the optimization problem.
+        fit_tree : pd.DataFrame
+            A single decision tree's structure from XGBoost, represented as a DataFrame.
+        treeno : int
+            The index of the current tree within the ensemble.
+        y_rules : np.ndarray
+            The array of leaf indices for each sample in the training data, determined
+            by the current tree.
+        """
+        # If the coefficients matrix is empty, start from the first column
+        if self.coefficients_.cols.shape[0] == 0:
+            col = 0
+        else:
+            # Otherwise, start from the next available column
+            col = np.max(self.coefficients_.cols) + 1
+
+        # Get the leaf node for each sample in x
+        y_rules = y_rules[:, treeno]
+
+        # Iterate over unique leaf nodes
+        for leafno in np.unique(y_rules):
+            # Get the samples in the leaf
+            covers = np.where(y_rules == leafno)[0]
+            leaf_y_vals = y[covers]  # y values of the samples in the leaf
+
+            # Get unique labels in the leaf and their counts
+            unique_labels, counts = np.unique(leaf_y_vals, return_counts=True)
+
+            # Identify the majority class in the leaf
+            label = unique_labels[np.argmax(counts)]
+
+            # Create a vector for this label
+            label_vector = np.full((self.k_,), -1 / (self.k_ - 1))
+            label_vector[label] = 1
+
+            # Calculate fill_ahat, which will be used to update yvals in the coefficients matrix
+            fill_ahat = np.dot(vec_y[covers, :], label_vector)
+
+            # Update the coefficients matrix with the new information
+            self.coefficients_.rows = np.concatenate((self.coefficients_.rows, covers))
+            self.coefficients_.cols = np.concatenate(
+                (self.coefficients_.cols, [col] * covers.shape[0])
+            )
+            self.coefficients_.yvals = np.concatenate(
+                (self.coefficients_.yvals, np.full(covers.shape[0], fill_ahat))
+            )
+
+            cost = self._get_rule_cost(
+                temp_rule=self._get_rule(fit_tree, leafno),
+                covers=covers,
+                counts=counts,
+                y=y,
+            )
+
+            # Append the cost to the costs in the coefficients matrix
+            self.coefficients_.costs = np.concatenate(
+                (self.coefficients_.costs, [cost])
+            )
+
+            # Calculate the distribution of the samples in the leaf across the classes
+            sdist = counts
+            self.rule_info_[col] = (treeno, leafno, label, sdist)
+            col += 1
+
+    def _get_matrices(self, x: np.ndarray, y: np.ndarray, vec_y: np.ndarray):
+        """
+        Generates the coefficient matrices for the optimization problem from
+        all trees in the XGBoost ensemble.
+
+        Parameters
+        ----------
+        x : np.ndarray
+            The feature matrix of the training data.
+        y : np.ndarray
+            The target vector of the training data.
+        vec_y : np.ndarray
+            The preprocessed target vector.
+        """
+        y_rules = self.trained_ensemble.apply(x).astype(np.intp)
+        for treeno, fit_tree in enumerate(self.decision_trees_.values()):
+            self._get_matrix(y, vec_y, fit_tree, treeno, y_rules)
+
+    def fit(self, x: ArrayLike, y: ArrayLike, sample_weight: ArrayLike | None = None):
+        """
+        Fits the RUXXGBClassifier to the training data, optimizing
+        the extracted rules for a balance between accuracy and interpretability.
+
+        Parameters
+        ----------
+        x : array-like of shape (n_samples, n_features)
+            The training input samples. Internally, it will be converted to dtype=np.float32.
+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
+            The target values (class labels) as integers
+        sample_weight : array-like of shape (n_samples,), default=None
+            Sample weights. If None, then samples are equally weighted.
+
+        Returns
+        -------
+        RUXXGBClassifier
+            The fitted model, ready for making predictions.
+        """
+        x, y = check_inputs(x, y)
+
+        # If the model has been fitted before, clean it up
+        if self.coefficients_.cols.shape[0] != 0:
+            self._cleanup()
+
+        # Fills the fitted decision trees.
+        out = self.trained_ensemble.get_booster().trees_to_dataframe()
+        pattern = re.compile(r"-(\d+)")
+        columns = ["Yes", "No", "Missing"]
+        out[columns] = out[columns].map(
+            lambda x: (
+                int(pattern.search(x).group(1))
+                if pd.notna(x) and pattern.search(x)
+                else None
+            )
+        )
+        out.Feature = out.Feature.str.lstrip("f")
+        self.decision_trees_ = {
+            treeno: out.loc[out.Tree == treeno] for treeno in out.Tree.unique()
+        }
+
+        self._get_class_infos(y)
+        vec_y = self._preprocess(y)
+
+        self._get_matrices(x=x, y=y, vec_y=vec_y)
+
+        sample_weight = self._get_sample_weight(sample_weight, self.class_weight, y)
+
+        ws, *_ = self.solver(
+            coefficients=self.coefficients_, k=self.k_, sample_weight=sample_weight
+        )
+
+        self._fill_rules(ws)
+        self._is_fitted = True
+
+        return self
```

## ruleopt/estimator/sklearn_/__init__.py

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-from .rug import RUGClassifier
-from .rux import RUXClassifier
-
-__all__ = [
-    "RUGClassifier",
-    "RUXClassifier",
-]
+from .rug import RUGClassifier
+from .rux import RUXClassifier
+
+__all__ = [
+    "RUGClassifier",
+    "RUXClassifier",
+]
```

## ruleopt/estimator/sklearn_/base_sklearn.py

 * *Ordering differences only*

```diff
@@ -1,196 +1,196 @@
-from typing import Union, Dict
-from sklearn.tree import DecisionTreeClassifier
-import numpy as np
-from ..base import _RUGBASE
-from ...aux_classes import Rule
-from ...rule_cost import RuleCost
-from ...solver.base import OptimizationSolver
-
-
-class _RUGSKLEARN(_RUGBASE):
-    """
-    The base class specialized for use with scikit-learn.
-
-    This subclass implements the rule extraction process specifically for decision trees
-    trained using scikit-learn.
-    """
-
-    def __init__(
-        self,
-        solver,
-        rule_cost,
-        class_weight,
-        threshold,
-        random_state,
-    ):
-        """
-        Parameters
-        ----------
-        solver : OptimizationSolver
-            An instance of a derived class inherits from the 'Optimization Solver' base class.
-            The solver is responsible for optimizing the rule set based on the cost function
-            and constraints.
-
-        rule_cost : RuleCost or int
-            Defines the cost of rules, either as a specific calculation method (RuleCost instance)
-            or a fixed cost
-
-        class_weight: dict, "balanced" or None
-            A dictionary mapping class labels to their respective weights, the string "balanced"
-            to automatically adjust weights inversely proportional to class frequencies,
-            or None for no weights. Used to adjust the model in favor of certain classes.
-
-        threshold : floa
-            The minimum weight threshold for including a rule in the final model
-
-        random_state : int or None
-            Seed for the random number generator to ensure reproducible results.
-            Defaults to None.
-        """
-        super().__init__(
-            threshold=threshold,
-            random_state=random_state,
-            rule_cost=rule_cost,
-            solver=solver,
-            class_weight=class_weight,
-        )
-
-    def _get_rule(self, fit_tree: DecisionTreeClassifier, nodeid: int) -> Rule:
-        """
-        Constructs a rule from a given node in a decision tree.
-
-        Parameters
-        ----------
-        fit_tree : DecisionTreeClassifier
-            The fitted decision tree from which to extract the rule.
-        nodeid : int
-            The ID of the node from which the rule is to be extracted.
-
-        Returns
-        -------
-        Rule
-            An object representing the extracted rule.
-        """
-
-        # Initializing the rule to be returned
-        return_rule = Rule()
-
-        # If the first feature of the tree is -2, the rule is empty
-        if fit_tree.tree_.feature[0] == -2:
-            return Rule()
-
-        # Extracting information from the tree
-        tree = fit_tree.tree_
-        left = tree.children_left
-        right = tree.children_right
-        threshold = tree.threshold
-        missing_left = tree.missing_go_to_left
-
-        # Building dictionaries to hold node information
-        node_info = {
-            node_id: (parent, True, bool(missing_left[parent]))
-            for parent, node_id in enumerate(left)
-        }
-        node_info.update(
-            {
-                node_id: (parent, False, not bool(missing_left[parent]))
-                for parent, node_id in enumerate(right)
-            }
-        )
-
-        # Traversing up the tree to build the rule
-        while nodeid != 0:
-            parent, is_left, missing = node_info[nodeid]
-
-            feature = tree.feature[parent]
-            ub = threshold[parent] if is_left else np.inf
-            lb = -np.inf if is_left else threshold[parent]
-            na = missing
-
-            return_rule.add_clause(feature, ub, lb, na)
-            nodeid = parent
-
-        return return_rule
-
-    def _get_matrix(
-        self,
-        x: np.ndarray,
-        y: np.ndarray,
-        vec_y: np.ndarray,
-        fit_tree: DecisionTreeClassifier,
-        treeno: int,
-    ) -> None:
-        """
-        Generates matrices for optimization based on a decision tree and training data.
-
-        Parameters
-        ----------
-        x : np.ndarray
-            The feature matrix of the training data.
-        y : np.ndarray
-            The target vector of the training data.
-        vec_y : np.ndarray
-            The preprocessed target vector, adjusted for optimization.
-        fit_tree : DecisionTreeClassifier
-            A fitted decision tree model from which to extract rules.
-        treeno : int
-            An identifier for the decision tree within an ensemble.
-        """
-        # If the coefficients matrix is empty, start from the first column
-        if self.coefficients_.cols.shape[0] == 0:
-            col = 0
-        else:
-            # Otherwise, start from the next available column
-            col = np.max(self.coefficients_.cols) + 1
-
-        # Get the leaf node for each sample in x
-        y_rules = fit_tree.apply(x)
-        preds = fit_tree.predict(x)
-
-        # Iterate over unique leaf nodes
-        for leafno in np.unique(y_rules):
-            # Get the samples in the leaf
-            covers = np.where(y_rules == leafno)[0]
-            leaf_y_vals = y[covers]  # y values of the samples in the leaf
-            
-            if hasattr(self, "_temp_rules"):
-                self._temp_rules.append(self._get_rule(fit_tree, leafno))
-
-            # Get unique labels in the leaf and their counts
-            unique_labels, counts = np.unique(leaf_y_vals, return_counts=True)
-
-            # Identify the majority class in the leaf
-            label = preds[covers][0]
-
-            # Create a vector for this label
-            label_vector = np.full((self.k_,), -1 / (self.k_ - 1))
-            label_vector[label] = 1
-
-            # Calculate fill_ahat, which will be used to update yvals in the coefficients matrix
-            fill_ahat = np.dot(vec_y[covers, :], label_vector)
-
-            # Update the coefficients matrix with the new information
-            self.coefficients_.rows = np.concatenate((self.coefficients_.rows, covers))
-            self.coefficients_.cols = np.concatenate(
-                (self.coefficients_.cols, [col] * covers.shape[0])
-            )
-            self.coefficients_.yvals = np.concatenate(
-                (self.coefficients_.yvals, np.full(covers.shape[0], fill_ahat))
-            )
-
-            cost = self._get_rule_cost(
-                temp_rule=self._get_rule(fit_tree, leafno),
-                covers=covers,
-                counts=counts,
-                y=y,
-            )
-
-            # Append the cost to the costs in the coefficients matrix
-            self.coefficients_.costs = np.concatenate(
-                (self.coefficients_.costs, [cost])
-            )
-
-            # Calculate the distribution of the samples in the leaf across the classes
-            sdist = counts
-            self.rule_info_[col] = (treeno, leafno, label, sdist)
-            col += 1
+from typing import Union, Dict
+from sklearn.tree import DecisionTreeClassifier
+import numpy as np
+from ..base import _RUGBASE
+from ...aux_classes import Rule
+from ...rule_cost import RuleCost
+from ...solver.base import OptimizationSolver
+
+
+class _RUGSKLEARN(_RUGBASE):
+    """
+    The base class specialized for use with scikit-learn.
+
+    This subclass implements the rule extraction process specifically for decision trees
+    trained using scikit-learn.
+    """
+
+    def __init__(
+        self,
+        solver,
+        rule_cost,
+        class_weight,
+        threshold,
+        random_state,
+    ):
+        """
+        Parameters
+        ----------
+        solver : OptimizationSolver
+            An instance of a derived class inherits from the 'Optimization Solver' base class.
+            The solver is responsible for optimizing the rule set based on the cost function
+            and constraints.
+
+        rule_cost : RuleCost or int
+            Defines the cost of rules, either as a specific calculation method (RuleCost instance)
+            or a fixed cost
+
+        class_weight: dict, "balanced" or None
+            A dictionary mapping class labels to their respective weights, the string "balanced"
+            to automatically adjust weights inversely proportional to class frequencies,
+            or None for no weights. Used to adjust the model in favor of certain classes.
+
+        threshold : floa
+            The minimum weight threshold for including a rule in the final model
+
+        random_state : int or None
+            Seed for the random number generator to ensure reproducible results.
+            Defaults to None.
+        """
+        super().__init__(
+            threshold=threshold,
+            random_state=random_state,
+            rule_cost=rule_cost,
+            solver=solver,
+            class_weight=class_weight,
+        )
+
+    def _get_rule(self, fit_tree: DecisionTreeClassifier, nodeid: int) -> Rule:
+        """
+        Constructs a rule from a given node in a decision tree.
+
+        Parameters
+        ----------
+        fit_tree : DecisionTreeClassifier
+            The fitted decision tree from which to extract the rule.
+        nodeid : int
+            The ID of the node from which the rule is to be extracted.
+
+        Returns
+        -------
+        Rule
+            An object representing the extracted rule.
+        """
+
+        # Initializing the rule to be returned
+        return_rule = Rule()
+
+        # If the first feature of the tree is -2, the rule is empty
+        if fit_tree.tree_.feature[0] == -2:
+            return Rule()
+
+        # Extracting information from the tree
+        tree = fit_tree.tree_
+        left = tree.children_left
+        right = tree.children_right
+        threshold = tree.threshold
+        missing_left = tree.missing_go_to_left
+
+        # Building dictionaries to hold node information
+        node_info = {
+            node_id: (parent, True, bool(missing_left[parent]))
+            for parent, node_id in enumerate(left)
+        }
+        node_info.update(
+            {
+                node_id: (parent, False, not bool(missing_left[parent]))
+                for parent, node_id in enumerate(right)
+            }
+        )
+
+        # Traversing up the tree to build the rule
+        while nodeid != 0:
+            parent, is_left, missing = node_info[nodeid]
+
+            feature = tree.feature[parent]
+            ub = threshold[parent] if is_left else np.inf
+            lb = -np.inf if is_left else threshold[parent]
+            na = missing
+
+            return_rule.add_clause(feature, ub, lb, na)
+            nodeid = parent
+
+        return return_rule
+
+    def _get_matrix(
+        self,
+        x: np.ndarray,
+        y: np.ndarray,
+        vec_y: np.ndarray,
+        fit_tree: DecisionTreeClassifier,
+        treeno: int,
+    ) -> None:
+        """
+        Generates matrices for optimization based on a decision tree and training data.
+
+        Parameters
+        ----------
+        x : np.ndarray
+            The feature matrix of the training data.
+        y : np.ndarray
+            The target vector of the training data.
+        vec_y : np.ndarray
+            The preprocessed target vector, adjusted for optimization.
+        fit_tree : DecisionTreeClassifier
+            A fitted decision tree model from which to extract rules.
+        treeno : int
+            An identifier for the decision tree within an ensemble.
+        """
+        # If the coefficients matrix is empty, start from the first column
+        if self.coefficients_.cols.shape[0] == 0:
+            col = 0
+        else:
+            # Otherwise, start from the next available column
+            col = np.max(self.coefficients_.cols) + 1
+
+        # Get the leaf node for each sample in x
+        y_rules = fit_tree.apply(x)
+        preds = fit_tree.predict(x)
+
+        # Iterate over unique leaf nodes
+        for leafno in np.unique(y_rules):
+            # Get the samples in the leaf
+            covers = np.where(y_rules == leafno)[0]
+            leaf_y_vals = y[covers]  # y values of the samples in the leaf
+            
+            if hasattr(self, "_temp_rules"):
+                self._temp_rules.append(self._get_rule(fit_tree, leafno))
+
+            # Get unique labels in the leaf and their counts
+            unique_labels, counts = np.unique(leaf_y_vals, return_counts=True)
+
+            # Identify the majority class in the leaf
+            label = preds[covers][0]
+
+            # Create a vector for this label
+            label_vector = np.full((self.k_,), -1 / (self.k_ - 1))
+            label_vector[label] = 1
+
+            # Calculate fill_ahat, which will be used to update yvals in the coefficients matrix
+            fill_ahat = np.dot(vec_y[covers, :], label_vector)
+
+            # Update the coefficients matrix with the new information
+            self.coefficients_.rows = np.concatenate((self.coefficients_.rows, covers))
+            self.coefficients_.cols = np.concatenate(
+                (self.coefficients_.cols, [col] * covers.shape[0])
+            )
+            self.coefficients_.yvals = np.concatenate(
+                (self.coefficients_.yvals, np.full(covers.shape[0], fill_ahat))
+            )
+
+            cost = self._get_rule_cost(
+                temp_rule=self._get_rule(fit_tree, leafno),
+                covers=covers,
+                counts=counts,
+                y=y,
+            )
+
+            # Append the cost to the costs in the coefficients matrix
+            self.coefficients_.costs = np.concatenate(
+                (self.coefficients_.costs, [cost])
+            )
+
+            # Calculate the distribution of the samples in the leaf across the classes
+            sdist = counts
+            self.rule_info_[col] = (treeno, leafno, label, sdist)
+            col += 1
```

## ruleopt/estimator/sklearn_/rug.py

```diff
@@ -1,473 +1,473 @@
-from __future__ import annotations
-import warnings
-
-import numpy as np
-from numpy.typing import ArrayLike
-from sklearn.tree import DecisionTreeClassifier
-
-from .base_sklearn import _RUGSKLEARN
-from ...rule_cost import Gini
-from ...utils import check_inputs
-
-
-class RUGClassifier(_RUGSKLEARN):
-    """
-    Rule Generation algorithm for multi-class classification. This algorithm aims at
-    producing a compact and interpretable model by employing optimization-bsed rule learning.
-    """
-
-    def __init__(
-        self,
-        solver,
-        *,
-        rule_cost=Gini(),
-        max_rmp_calls=20,
-        threshold: float = 1.0e-6,
-        random_state: int | None = None,
-        class_weight: dict | str | None = None,
-        criterion: str = "gini",
-        splitter: str = "best",
-        max_depth: int | None = None,
-        min_samples_split: int = 2,
-        min_samples_leaf: int = 1,
-        min_weight_fraction_leaf: float = 0.0,
-        max_features: int | float | str = None,
-        max_leaf_nodes: int | None = None,
-        min_impurity_decrease: float = 0.0,
-        ccp_alpha: float = 0.0,
-        monotonic_cst: ArrayLike | None = None,
-    ):
-        """
-        Parameters
-        ----------
-        solver : OptimizationSolver
-            An instance of a derived class inherits from the 'Optimization Solver' base class.
-            The solver is responsible for optimizing the rule set based on the cost function
-            and constraints.
-
-        rule_cost : RuleCost or int, default=Gini()
-            Defines the cost of rules, either as a specific calculation method (RuleCost instance)
-            or a fixed cost
-
-        max_rmp_calls : int, default=20
-            Maximum number of Restricted Master Problem (RMP) iterations allowed during fitting.
-
-        class_weight: dict, "balanced" or None, default=None
-            A dictionary mapping class labels to their respective weights, the string "balanced"
-            to automatically adjust weights inversely proportional to class frequencies,
-            or None for no weights. Used to adjust the model in favor of certain classes.
-
-        threshold : float, default=1.0e-6
-            The minimum weight threshold for including a rule in the final model.
-
-        random_state : int or None, default=None
-            Seed for the random number generator to ensure reproducible results.
-            Defaults to None.
-
-        criterion : {"gini", "entropy", "log_loss"}, default="gini"
-            The function to measure the quality of a split. Supported criteria are
-            "gini" for the Gini impurity and "log_loss" and "entropy" both for the
-            Shannon information gain, see :ref:`tree_mathematical_formulation`.
-
-        splitter : {"best", "random"}, default="best"
-            The strategy used to choose the split at each node. Supported
-            strategies are "best" to choose the best split and "random" to choose
-            the best random split.
-
-        max_depth : int, default=None
-            The maximum depth of the tree. If None, then nodes are expanded until
-            all leaves are pure or until all leaves contain less than
-            min_samples_split samples.
-
-        min_samples_split : int or float, default=2
-            The minimum number of samples required to split an internal node:
-
-        min_samples_leaf : int or float, default=1
-            The minimum number of samples required to be at a leaf node.
-            A split point at any depth will only be considered if it leaves at
-            least ``min_samples_leaf`` training samples in each of the left and
-            right branches.  This may have the effect of smoothing the model,
-            especially in regression.
-
-        min_weight_fraction_leaf : float, default=0.0
-            The minimum weighted fraction of the sum total of weights (of all
-            the input samples) required to be at a leaf node. Samples have
-            equal weight when sample_weight is not provided.
-
-        max_features : int, float or {"sqrt", "log2"}, default=None
-            The number of features to consider when looking for the best split:
-
-        max_leaf_nodes : int, default=None
-            Grow a tree with ``max_leaf_nodes`` in best-first fashion.
-            Best nodes are defined as relative reduction in impurity.
-            If None then unlimited number of leaf nodes.
-
-        min_impurity_decrease : float, default=0.0
-            A node will be split if this split induces a decrease of the impurity
-            greater than or equal to this value.
-
-        ccp_alpha : non-negative float, default=0.0
-            Complexity parameter used for Minimal Cost-Complexity Pruning. The
-            subtree with the largest cost complexity that is smaller than
-            ``ccp_alpha`` will be chosen. By default, no pruning is performed.
-
-        monotonic_cst : array-like of int of shape (n_features), default=None
-            Indicates the monotonicity constraint to enforce on each feature.
-            - 1: monotonic increase
-            - 0: no constraint
-            - -1: monotonic decrease
-        """
-        self._validate_parameters(
-            max_rmp_calls,
-            class_weight,
-            criterion,
-            splitter,
-            max_depth,
-            min_samples_split,
-            min_samples_leaf,
-            min_weight_fraction_leaf,
-            max_features,
-            max_leaf_nodes,
-            min_impurity_decrease,
-            ccp_alpha,
-            monotonic_cst,
-        )
-
-        super().__init__(
-            threshold=threshold,
-            random_state=random_state,
-            solver=solver,
-            rule_cost=rule_cost,
-            class_weight=class_weight,
-        )
-
-        self.max_rmp_calls = int(max_rmp_calls)
-
-        self.criterion = criterion
-        self.splitter = splitter
-        self.max_depth = max_depth
-        self.min_samples_split = min_samples_split
-        self.min_samples_leaf = min_samples_leaf
-        self.min_weight_fraction_leaf = min_weight_fraction_leaf
-        self.max_features = max_features
-        self.max_leaf_nodes = max_leaf_nodes
-        self.min_impurity_decrease = min_impurity_decrease
-        self.ccp_alpha = ccp_alpha
-        self.monotonic_cst = monotonic_cst
-
-        self._temp_rules = []
-
-    def _pspdt(
-        self,
-        x: np.ndarray,
-        y: np.ndarray,
-        vec_y: np.ndarray,
-        fit_tree: DecisionTreeClassifier,
-        treeno: int,
-        betas: np.ndarray,
-    ) -> bool:
-        """
-        Pricing SubProblem for Decision Trees (PSPDT) in the rule generation process.
-
-        Parameters
-        ----------
-        x : np.ndarray
-            Feature matrix of the training data.
-        y : np.ndarray
-            Target vector of the training data.
-        vec_y : np.ndarray
-            Preprocessed target vector, adjusted for optimization.
-        fit_tree : DecisionTreeClassifier
-            Fitted decision tree for rule extraction.
-        treeno : int
-            Identifier for the current tree in the iterative process.
-        betas : np.ndarray
-            Dual variables or sample weights from the latest master problem solution.
-
-        Returns
-        -------
-        bool
-            Indicates whether a new rule that improves the objective function was found.
-        """
-        no_improvement = True
-
-        n, col = x.shape[0], np.max(self.coefficients_.cols) + 1
-
-        # Apply the decision tree to the feature matrix
-        y_rules = fit_tree.apply(x)
-        preds = fit_tree.predict(x)
-
-        for leafno in np.unique(y_rules):
-            temp_rule = self._get_rule(fit_tree, leafno)
-
-            if temp_rule in self._temp_rules:
-                continue
-
-            # Get the samples that fall into this leaf
-            covers = np.where(y_rules == leafno)[0]
-            leaf_y_vals = y[covers]  # y values of the samples in the leaf
-
-            # Get the unique labels in the leaf and their counts
-            unique_labels = np.arange(self.k_, dtype=np.intp)
-            counts = np.zeros(self.k_, dtype=np.intp)
-            unique_labels_, counts_ = np.unique(leaf_y_vals, return_counts=True)
-            for i, j in enumerate(unique_labels_):
-                unique_labels[j] = unique_labels_[i]
-                counts[j] = counts_[i]
-
-            # Identify the majority class in the leaf
-            label = preds[covers][0]  # leaf_output
-
-            # Create a vector for this label
-            label_vector = np.full((self.k_,), -1 / (self.k_ - 1))
-            label_vector[label] = 1
-
-            # Calculate the y values for the optimization problem
-            fill_ahat = np.dot(vec_y[covers, :], label_vector)
-
-            # Prepare to check the reduced cost
-            aijhat = np.zeros(n)
-            aijhat[covers] = fill_ahat
-
-            cost = self._get_rule_cost(
-                temp_rule=self._get_rule(fit_tree, leafno),
-                covers=covers,
-                counts=counts,
-                y=y,
-            )
-
-            # Calculate the reduced cost
-            red_cost = np.dot(
-                np.multiply(((self.k_ - 1.0) / self.k_), aijhat),
-                betas,  # (betas if sample_weight is None else betas * sample_weight),
-            ) - (cost * self.solver.penalty)
-
-            # If the reduced cost is positive, update the coefficients
-            if red_cost > 0:  # only columns with proper reduced costs are added
-                covers_fill = np.full((covers.shape[0],), fill_ahat, dtype=np.intp)
-                covers_col = np.full((covers.shape[0],), col, dtype=np.intp)
-                self.coefficients_.rows = np.concatenate(
-                    (self.coefficients_.rows, covers)
-                )
-                self.coefficients_.cols = np.concatenate(
-                    (self.coefficients_.cols, covers_col)
-                )
-                self.coefficients_.yvals = np.concatenate(
-                    (self.coefficients_.yvals, covers_fill)
-                )
-                self.coefficients_.costs = np.concatenate(
-                    (self.coefficients_.costs, [cost])
-                )
-
-                # Calculate the distribution of the samples in the leaf across the classes
-                sdist = np.zeros(self.k_, dtype=np.intp)
-                sdist[unique_labels] = counts
-                self.rule_info_[col] = (treeno, leafno, label, sdist)
-                col += 1
-                no_improvement = False
-
-                self._temp_rules.append(temp_rule)
-
-        # Return whether there was any improvement
-        return no_improvement
-
-    def _fit_decision_tree(
-        self, x: np.ndarray, y: np.ndarray, sample_weight: np.ndarray
-    ) -> DecisionTreeClassifier:
-        """
-        Fits a decision tree to the data, taking into account sample weights.
-
-        Parameters
-        ----------
-        x : np.ndarray
-            Feature matrix of the training data.
-        y : np.ndarray
-            Target vector of the training data.
-        sample_weight : np.ndarray
-            Array of weights for the samples.
-
-        Returns
-        -------
-        DecisionTreeClassifier
-            A decision tree classifier fitted to the weighted data.
-        """
-        dt = DecisionTreeClassifier(
-            random_state=self._rng.integers(np.iinfo(np.int16).max),
-            criterion=self.criterion,
-            splitter=self.splitter,
-            class_weight=self.class_weight,
-            max_depth=self.max_depth,
-            min_samples_split=self.min_samples_split,
-            min_samples_leaf=self.min_samples_leaf,
-            min_weight_fraction_leaf=self.min_weight_fraction_leaf,
-            max_features=self.max_features,
-            max_leaf_nodes=self.max_leaf_nodes,
-            min_impurity_decrease=self.min_impurity_decrease,
-            ccp_alpha=self.ccp_alpha,
-            monotonic_cst=self.monotonic_cst,
-        )
-
-        if sample_weight is not None:
-            dt.class_weight = None
-
-        # Fit the decision tree to the data
-        return dt.fit(x, y, sample_weight=sample_weight)
-
-
-    def fit(self, x: ArrayLike, y: ArrayLike, sample_weight: ArrayLike | None = None):
-        """
-        Fits the RUGClassifier model to the training data using a rule generation approach.
-
-        Parameters
-        ----------
-        x : array-like of shape (n_samples, n_features)
-            The training input samples. Internally, it will be converted to dtype=np.float32.
-        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
-            The target values (class labels) as integers
-        sample_weight : array-like of shape (n_samples,), default=None
-            Sample weights. If None, then samples are equally weighted.
-
-        Returns
-        -------
-        RUGClassifier
-            The fitted model, ready for making predictions.
-        """
-        x, y = check_inputs(x, y)
-        if self._is_fitted:
-            self._cleanup()
-
-        treeno = 0
-        fit_tree = self._fit_decision_tree(x, y, sample_weight=None)
-        self.decision_trees_[treeno] = fit_tree
-
-        self._get_class_infos(y)
-        vec_y = self._preprocess(y)
-        self._get_matrix(x, y, vec_y, fit_tree, treeno)
-
-        sample_weight = self._get_sample_wight(sample_weight, self.class_weight, y)
-
-        ws, betas = self.solver(
-            coefficients=self.coefficients_, k=self.k_, sample_weight=sample_weight
-        )
-
-        # Rule generation
-        for _ in range(self.max_rmp_calls):
-            if np.all(betas == 0):
-                break
-
-            treeno += 1
-            fit_tree = self._fit_decision_tree(x, y, sample_weight=betas)
-            self.decision_trees_[treeno] = fit_tree
-
-            no_improvement = self._pspdt(x, y, vec_y, fit_tree, treeno, betas)
-
-            if no_improvement:
-                break
-
-            ws, betas = self.solver(
-                coefficients=self.coefficients_,
-                k=self.k_,
-                ws0=ws.copy(),
-                sample_weight=sample_weight,
-            )
-
-        self._fill_rules(ws)
-        self._is_fitted = True
-
-        return self
-
-    def _validate_parameters(
-        self,
-        max_rmp_calls,
-        class_weight,
-        criterion,
-        splitter,
-        max_depth,
-        min_samples_split,
-        min_samples_leaf,
-        min_weight_fraction_leaf,
-        max_features,
-        max_leaf_nodes,
-        min_impurity_decrease,
-        ccp_alpha,
-        monotonic_cst,
-    ):
-        # max_rmp_calls check
-        if not isinstance(max_rmp_calls, (float, int)):
-            raise TypeError("max_rmp_calls must be an integer.")
-
-        if max_rmp_calls < 0:
-            raise ValueError("max_rmp_calls must be a non-negative integer.")
-
-        # class_weight check
-        if not isinstance(class_weight, (dict, str, type(None))) or (
-            isinstance(class_weight, str) and class_weight != "balanced"
-        ):
-            raise ValueError("class_weight must be a dictionary, 'balanced', or None.")
-
-        # criterion check
-        if criterion not in {"gini", "entropy", "log_loss"}:
-            raise ValueError(
-                "criterion must be one of 'gini', 'entropy', or 'log_loss'."
-            )
-
-        # splitter check
-        if splitter not in {"best", "random"}:
-            raise ValueError("splitter must be 'best' or 'random'.")
-
-        # max_depth check
-        if max_depth is not None and not isinstance(max_depth, int):
-            raise TypeError("max_depth must be an integer or None.")
-        if isinstance(max_depth, int) and max_depth < 1:
-            raise ValueError("max_depth must be greater than 0.")
-
-        # min_samples_split check
-        if not isinstance(min_samples_split, (int, float)) or min_samples_split < 2:
-            raise ValueError(
-                "min_samples_split must be an integer or float greater than or equal to 2."
-            )
-
-        # min_samples_leaf check
-        if not isinstance(min_samples_leaf, (int, float)) or min_samples_leaf < 1:
-            raise ValueError(
-                "min_samples_leaf must be an integer or float greater than or equal to 1."
-            )
-
-        # min_weight_fraction_leaf check
-        if not isinstance(min_weight_fraction_leaf, float) or not (
-            0.0 <= min_weight_fraction_leaf <= 1.0
-        ):
-            raise ValueError(
-                "min_weight_fraction_leaf must be a float between 0.0 and 1.0."
-            )
-
-        # max_features check
-        if (
-            max_features is not None
-            and not isinstance(max_features, (int, float, str))
-            or (isinstance(max_features, str) and max_features not in {"sqrt", "log2"})
-        ):
-            raise ValueError(
-                "max_features must be an integer, float, 'sqrt', 'log2', or None."
-            )
-
-        # max_leaf_nodes check
-        if max_leaf_nodes is not None and (
-            not isinstance(max_leaf_nodes, int) or max_leaf_nodes < 1
-        ):
-            raise ValueError("max_leaf_nodes must be a positive integer or None.")
-
-        # min_impurity_decrease check
-        if not isinstance(min_impurity_decrease, float) or min_impurity_decrease < 0.0:
-            raise ValueError("min_impurity_decrease must be a non-negative float.")
-
-        # ccp_alpha check
-        if not isinstance(ccp_alpha, float) or ccp_alpha < 0.0:
-            raise ValueError("ccp_alpha must be a non-negative float.")
-
-        # monotonic_cst check
-        if monotonic_cst is not None and (
-            not isinstance(monotonic_cst, (list, tuple))
-            or not all(isinstance(item, int) for item in monotonic_cst)
-        ):
-            raise ValueError("monotonic_cst must be an array-like of integers or None.")
+from __future__ import annotations
+import warnings
+
+import numpy as np
+from numpy.typing import ArrayLike
+from sklearn.tree import DecisionTreeClassifier
+
+from .base_sklearn import _RUGSKLEARN
+from ...rule_cost import Gini
+from ...utils import check_inputs
+
+
+class RUGClassifier(_RUGSKLEARN):
+    """
+    Rule Generation algorithm for multi-class classification. This algorithm aims at
+    producing a compact and interpretable model by employing optimization-bsed rule learning.
+    """
+
+    def __init__(
+        self,
+        solver,
+        *,
+        rule_cost=Gini(),
+        max_rmp_calls=20,
+        threshold: float = 1.0e-6,
+        random_state: int | None = None,
+        class_weight: dict | str | None = None,
+        criterion: str = "gini",
+        splitter: str = "best",
+        max_depth: int | None = None,
+        min_samples_split: int = 2,
+        min_samples_leaf: int = 1,
+        min_weight_fraction_leaf: float = 0.0,
+        max_features: int | float | str = None,
+        max_leaf_nodes: int | None = None,
+        min_impurity_decrease: float = 0.0,
+        ccp_alpha: float = 0.0,
+        monotonic_cst: ArrayLike | None = None,
+    ):
+        """
+        Parameters
+        ----------
+        solver : OptimizationSolver
+            An instance of a derived class inherits from the 'Optimization Solver' base class.
+            The solver is responsible for optimizing the rule set based on the cost function
+            and constraints.
+
+        rule_cost : RuleCost or int, default=Gini()
+            Defines the cost of rules, either as a specific calculation method (RuleCost instance)
+            or a fixed cost
+
+        max_rmp_calls : int, default=20
+            Maximum number of Restricted Master Problem (RMP) iterations allowed during fitting.
+
+        class_weight: dict, "balanced" or None, default=None
+            A dictionary mapping class labels to their respective weights, the string "balanced"
+            to automatically adjust weights inversely proportional to class frequencies,
+            or None for no weights. Used to adjust the model in favor of certain classes.
+
+        threshold : float, default=1.0e-6
+            The minimum weight threshold for including a rule in the final model.
+
+        random_state : int or None, default=None
+            Seed for the random number generator to ensure reproducible results.
+            Defaults to None.
+
+        criterion : {"gini", "entropy", "log_loss"}, default="gini"
+            The function to measure the quality of a split. Supported criteria are
+            "gini" for the Gini impurity and "log_loss" and "entropy" both for the
+            Shannon information gain, see :ref:`tree_mathematical_formulation`.
+
+        splitter : {"best", "random"}, default="best"
+            The strategy used to choose the split at each node. Supported
+            strategies are "best" to choose the best split and "random" to choose
+            the best random split.
+
+        max_depth : int, default=None
+            The maximum depth of the tree. If None, then nodes are expanded until
+            all leaves are pure or until all leaves contain less than
+            min_samples_split samples.
+
+        min_samples_split : int or float, default=2
+            The minimum number of samples required to split an internal node:
+
+        min_samples_leaf : int or float, default=1
+            The minimum number of samples required to be at a leaf node.
+            A split point at any depth will only be considered if it leaves at
+            least ``min_samples_leaf`` training samples in each of the left and
+            right branches.  This may have the effect of smoothing the model,
+            especially in regression.
+
+        min_weight_fraction_leaf : float, default=0.0
+            The minimum weighted fraction of the sum total of weights (of all
+            the input samples) required to be at a leaf node. Samples have
+            equal weight when sample_weight is not provided.
+
+        max_features : int, float or {"sqrt", "log2"}, default=None
+            The number of features to consider when looking for the best split:
+
+        max_leaf_nodes : int, default=None
+            Grow a tree with ``max_leaf_nodes`` in best-first fashion.
+            Best nodes are defined as relative reduction in impurity.
+            If None then unlimited number of leaf nodes.
+
+        min_impurity_decrease : float, default=0.0
+            A node will be split if this split induces a decrease of the impurity
+            greater than or equal to this value.
+
+        ccp_alpha : non-negative float, default=0.0
+            Complexity parameter used for Minimal Cost-Complexity Pruning. The
+            subtree with the largest cost complexity that is smaller than
+            ``ccp_alpha`` will be chosen. By default, no pruning is performed.
+
+        monotonic_cst : array-like of int of shape (n_features), default=None
+            Indicates the monotonicity constraint to enforce on each feature.
+            - 1: monotonic increase
+            - 0: no constraint
+            - -1: monotonic decrease
+        """
+        self._validate_parameters(
+            max_rmp_calls,
+            class_weight,
+            criterion,
+            splitter,
+            max_depth,
+            min_samples_split,
+            min_samples_leaf,
+            min_weight_fraction_leaf,
+            max_features,
+            max_leaf_nodes,
+            min_impurity_decrease,
+            ccp_alpha,
+            monotonic_cst,
+        )
+
+        super().__init__(
+            threshold=threshold,
+            random_state=random_state,
+            solver=solver,
+            rule_cost=rule_cost,
+            class_weight=class_weight,
+        )
+
+        self.max_rmp_calls = int(max_rmp_calls)
+
+        self.criterion = criterion
+        self.splitter = splitter
+        self.max_depth = max_depth
+        self.min_samples_split = min_samples_split
+        self.min_samples_leaf = min_samples_leaf
+        self.min_weight_fraction_leaf = min_weight_fraction_leaf
+        self.max_features = max_features
+        self.max_leaf_nodes = max_leaf_nodes
+        self.min_impurity_decrease = min_impurity_decrease
+        self.ccp_alpha = ccp_alpha
+        self.monotonic_cst = monotonic_cst
+
+        self._temp_rules = []
+
+    def _pspdt(
+        self,
+        x: np.ndarray,
+        y: np.ndarray,
+        vec_y: np.ndarray,
+        fit_tree: DecisionTreeClassifier,
+        treeno: int,
+        betas: np.ndarray,
+    ) -> bool:
+        """
+        Pricing SubProblem for Decision Trees (PSPDT) in the rule generation process.
+
+        Parameters
+        ----------
+        x : np.ndarray
+            Feature matrix of the training data.
+        y : np.ndarray
+            Target vector of the training data.
+        vec_y : np.ndarray
+            Preprocessed target vector, adjusted for optimization.
+        fit_tree : DecisionTreeClassifier
+            Fitted decision tree for rule extraction.
+        treeno : int
+            Identifier for the current tree in the iterative process.
+        betas : np.ndarray
+            Dual variables or sample weights from the latest master problem solution.
+
+        Returns
+        -------
+        bool
+            Indicates whether a new rule that improves the objective function was found.
+        """
+        no_improvement = True
+
+        n, col = x.shape[0], np.max(self.coefficients_.cols) + 1
+
+        # Apply the decision tree to the feature matrix
+        y_rules = fit_tree.apply(x)
+        preds = fit_tree.predict(x)
+
+        for leafno in np.unique(y_rules):
+            temp_rule = self._get_rule(fit_tree, leafno)
+
+            if temp_rule in self._temp_rules:
+                continue
+
+            # Get the samples that fall into this leaf
+            covers = np.where(y_rules == leafno)[0]
+            leaf_y_vals = y[covers]  # y values of the samples in the leaf
+
+            # Get the unique labels in the leaf and their counts
+            unique_labels = np.arange(self.k_, dtype=np.intp)
+            counts = np.zeros(self.k_, dtype=np.intp)
+            unique_labels_, counts_ = np.unique(leaf_y_vals, return_counts=True)
+            for i, j in enumerate(unique_labels_):
+                unique_labels[j] = unique_labels_[i]
+                counts[j] = counts_[i]
+
+            # Identify the majority class in the leaf
+            label = preds[covers][0]  # leaf_output
+
+            # Create a vector for this label
+            label_vector = np.full((self.k_,), -1 / (self.k_ - 1))
+            label_vector[label] = 1
+
+            # Calculate the y values for the optimization problem
+            fill_ahat = np.dot(vec_y[covers, :], label_vector)
+
+            # Prepare to check the reduced cost
+            aijhat = np.zeros(n)
+            aijhat[covers] = fill_ahat
+
+            cost = self._get_rule_cost(
+                temp_rule=self._get_rule(fit_tree, leafno),
+                covers=covers,
+                counts=counts,
+                y=y,
+            )
+
+            # Calculate the reduced cost
+            red_cost = np.dot(
+                np.multiply(((self.k_ - 1.0) / self.k_), aijhat),
+                betas,  # (betas if sample_weight is None else betas * sample_weight),
+            ) - (cost * self.solver.penalty)
+
+            # If the reduced cost is positive, update the coefficients
+            if red_cost > 0:  # only columns with proper reduced costs are added
+                covers_fill = np.full((covers.shape[0],), fill_ahat, dtype=np.intp)
+                covers_col = np.full((covers.shape[0],), col, dtype=np.intp)
+                self.coefficients_.rows = np.concatenate(
+                    (self.coefficients_.rows, covers)
+                )
+                self.coefficients_.cols = np.concatenate(
+                    (self.coefficients_.cols, covers_col)
+                )
+                self.coefficients_.yvals = np.concatenate(
+                    (self.coefficients_.yvals, covers_fill)
+                )
+                self.coefficients_.costs = np.concatenate(
+                    (self.coefficients_.costs, [cost])
+                )
+
+                # Calculate the distribution of the samples in the leaf across the classes
+                sdist = np.zeros(self.k_, dtype=np.intp)
+                sdist[unique_labels] = counts
+                self.rule_info_[col] = (treeno, leafno, label, sdist)
+                col += 1
+                no_improvement = False
+
+                self._temp_rules.append(temp_rule)
+
+        # Return whether there was any improvement
+        return no_improvement
+
+    def _fit_decision_tree(
+        self, x: np.ndarray, y: np.ndarray, sample_weight: np.ndarray
+    ) -> DecisionTreeClassifier:
+        """
+        Fits a decision tree to the data, taking into account sample weights.
+
+        Parameters
+        ----------
+        x : np.ndarray
+            Feature matrix of the training data.
+        y : np.ndarray
+            Target vector of the training data.
+        sample_weight : np.ndarray
+            Array of weights for the samples.
+
+        Returns
+        -------
+        DecisionTreeClassifier
+            A decision tree classifier fitted to the weighted data.
+        """
+        dt = DecisionTreeClassifier(
+            random_state=self._rng.integers(np.iinfo(np.int16).max),
+            criterion=self.criterion,
+            splitter=self.splitter,
+            class_weight=self.class_weight,
+            max_depth=self.max_depth,
+            min_samples_split=self.min_samples_split,
+            min_samples_leaf=self.min_samples_leaf,
+            min_weight_fraction_leaf=self.min_weight_fraction_leaf,
+            max_features=self.max_features,
+            max_leaf_nodes=self.max_leaf_nodes,
+            min_impurity_decrease=self.min_impurity_decrease,
+            ccp_alpha=self.ccp_alpha,
+            monotonic_cst=self.monotonic_cst,
+        )
+
+        if sample_weight is not None:
+            dt.class_weight = None
+
+        # Fit the decision tree to the data
+        return dt.fit(x, y, sample_weight=sample_weight)
+
+
+    def fit(self, x: ArrayLike, y: ArrayLike, sample_weight: ArrayLike | None = None):
+        """
+        Fits the RUGClassifier model to the training data using a rule generation approach.
+
+        Parameters
+        ----------
+        x : array-like of shape (n_samples, n_features)
+            The training input samples. Internally, it will be converted to dtype=np.float32.
+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
+            The target values (class labels) as integers
+        sample_weight : array-like of shape (n_samples,), default=None
+            Sample weights. If None, then samples are equally weighted.
+
+        Returns
+        -------
+        RUGClassifier
+            The fitted model, ready for making predictions.
+        """
+        x, y = check_inputs(x, y)
+        if self._is_fitted:
+            self._cleanup()
+
+        treeno = 0
+        fit_tree = self._fit_decision_tree(x, y, sample_weight=None)
+        self.decision_trees_[treeno] = fit_tree
+
+        self._get_class_infos(y)
+        vec_y = self._preprocess(y)
+        self._get_matrix(x, y, vec_y, fit_tree, treeno)
+
+        sample_weight = self._get_sample_weight(sample_weight, self.class_weight, y)
+
+        ws, betas = self.solver(
+            coefficients=self.coefficients_, k=self.k_, sample_weight=sample_weight
+        )
+
+        # Rule generation
+        for _ in range(self.max_rmp_calls):
+            if np.all(betas == 0):
+                break
+
+            treeno += 1
+            fit_tree = self._fit_decision_tree(x, y, sample_weight=betas)
+            self.decision_trees_[treeno] = fit_tree
+
+            no_improvement = self._pspdt(x, y, vec_y, fit_tree, treeno, betas)
+
+            if no_improvement:
+                break
+
+            ws, betas = self.solver(
+                coefficients=self.coefficients_,
+                k=self.k_,
+                ws0=ws.copy(),
+                sample_weight=sample_weight,
+            )
+
+        self._fill_rules(ws)
+        self._is_fitted = True
+
+        return self
+
+    def _validate_parameters(
+        self,
+        max_rmp_calls,
+        class_weight,
+        criterion,
+        splitter,
+        max_depth,
+        min_samples_split,
+        min_samples_leaf,
+        min_weight_fraction_leaf,
+        max_features,
+        max_leaf_nodes,
+        min_impurity_decrease,
+        ccp_alpha,
+        monotonic_cst,
+    ):
+        # max_rmp_calls check
+        if not isinstance(max_rmp_calls, (float, int)):
+            raise TypeError("max_rmp_calls must be an integer.")
+
+        if max_rmp_calls < 0:
+            raise ValueError("max_rmp_calls must be a non-negative integer.")
+
+        # class_weight check
+        if not isinstance(class_weight, (dict, str, type(None))) or (
+            isinstance(class_weight, str) and class_weight != "balanced"
+        ):
+            raise ValueError("class_weight must be a dictionary, 'balanced', or None.")
+
+        # criterion check
+        if criterion not in {"gini", "entropy", "log_loss"}:
+            raise ValueError(
+                "criterion must be one of 'gini', 'entropy', or 'log_loss'."
+            )
+
+        # splitter check
+        if splitter not in {"best", "random"}:
+            raise ValueError("splitter must be 'best' or 'random'.")
+
+        # max_depth check
+        if max_depth is not None and not isinstance(max_depth, int):
+            raise TypeError("max_depth must be an integer or None.")
+        if isinstance(max_depth, int) and max_depth < 1:
+            raise ValueError("max_depth must be greater than 0.")
+
+        # min_samples_split check
+        if not isinstance(min_samples_split, (int, float)) or min_samples_split < 2:
+            raise ValueError(
+                "min_samples_split must be an integer or float greater than or equal to 2."
+            )
+
+        # min_samples_leaf check
+        if not isinstance(min_samples_leaf, (int, float)) or min_samples_leaf < 1:
+            raise ValueError(
+                "min_samples_leaf must be an integer or float greater than or equal to 1."
+            )
+
+        # min_weight_fraction_leaf check
+        if not isinstance(min_weight_fraction_leaf, float) or not (
+            0.0 <= min_weight_fraction_leaf <= 1.0
+        ):
+            raise ValueError(
+                "min_weight_fraction_leaf must be a float between 0.0 and 1.0."
+            )
+
+        # max_features check
+        if (
+            max_features is not None
+            and not isinstance(max_features, (int, float, str))
+            or (isinstance(max_features, str) and max_features not in {"sqrt", "log2"})
+        ):
+            raise ValueError(
+                "max_features must be an integer, float, 'sqrt', 'log2', or None."
+            )
+
+        # max_leaf_nodes check
+        if max_leaf_nodes is not None and (
+            not isinstance(max_leaf_nodes, int) or max_leaf_nodes < 1
+        ):
+            raise ValueError("max_leaf_nodes must be a positive integer or None.")
+
+        # min_impurity_decrease check
+        if not isinstance(min_impurity_decrease, float) or min_impurity_decrease < 0.0:
+            raise ValueError("min_impurity_decrease must be a non-negative float.")
+
+        # ccp_alpha check
+        if not isinstance(ccp_alpha, float) or ccp_alpha < 0.0:
+            raise ValueError("ccp_alpha must be a non-negative float.")
+
+        # monotonic_cst check
+        if monotonic_cst is not None and (
+            not isinstance(monotonic_cst, (list, tuple))
+            or not all(isinstance(item, int) for item in monotonic_cst)
+        ):
+            raise ValueError("monotonic_cst must be an array-like of integers or None.")
```

## ruleopt/estimator/sklearn_/rux.py

```diff
@@ -1,149 +1,149 @@
-from __future__ import annotations
-
-import numpy as np
-from sklearn.ensemble._forest import ForestClassifier
-from sklearn.ensemble import GradientBoostingClassifier
-from numpy.typing import ArrayLike
-
-from .base_sklearn import _RUGSKLEARN
-from ...rule_cost import Gini
-from ...utils import check_inputs
-
-
-class RUXClassifier(_RUGSKLEARN):
-    """
-    RUXClassifier aims to build a compact and interpretable model
-    by employing rule-based learning extracted from a trained scikit-learn
-    ensemble model (such as Gradient Boosting or Random Forest). It allows
-    a user to trade off between the complexity of the model (number of rules)
-    and the accuracy of the model.
-    """
-
-    def __init__(
-        self,
-        trained_ensemble,
-        solver,
-        *,
-        rule_cost=Gini(),
-        class_weight: dict | str | None = None,
-        threshold: float = 1.0e-6,
-        random_state: int | None = None,
-    ):
-        """
-        Parameters
-        ----------
-        trained_ensemble : sklearn.ensemble object
-            The trained scikit-learn ensemble model from which the rules will be
-            extracted.
-
-        solver : OptimizationSolver
-            An instance of a derived class inherits from the 'Optimization Solver' base class.
-            The solver is responsible for optimizing the rule set based on the cost function
-            and constraints.
-
-        rule_cost : RuleCost or int, default=Gini()
-            Defines the cost of rules, either as a specific calculation method (RuleCost instance)
-            or a fixed cost
-
-        class_weight: dict, "balanced" or None, default=None
-            A dictionary mapping class labels to their respective weights, the string "balanced"
-            to automatically adjust weights inversely proportional to class frequencies,
-            or None for no weights. Used to adjust the model in favor of certain classes.
-
-        threshold : float, default=1.0e-6
-            The minimum weight threshold for including a rule in the final model
-
-        random_state : int or None, default=None
-            Seed for the random number generator to ensure reproducible results.
-            Defaults to None.
-        """
-        if not (
-            isinstance(trained_ensemble, (GradientBoostingClassifier, ForestClassifier))
-        ):
-            raise TypeError(
-                "trained_ensemble must be an instance of ",
-                "sklearn.ensemble.GradientBoostingClassifier, ",
-                "sklearn.ensemble.RandomForestClassifier, ",
-                "or sklearn.ensemble.ExtraTreesClassifier.",
-            )
-
-        self.trained_ensemble = trained_ensemble
-        super().__init__(
-            threshold=threshold,
-            random_state=random_state,
-            rule_cost=rule_cost,
-            solver=solver,
-            class_weight=class_weight,
-        )
-
-    def _get_matrices(self, x: np.ndarray, y: np.ndarray, vec_y: np.ndarray) -> None:
-        """
-        Prepares the optimization problem matrices based on the ensemble of decision trees.
-
-        Parameters
-        ----------
-        x : np.ndarray
-            The feature matrix of the training data.
-        y : np.ndarray
-            The target vector of the training data.
-        vec_y : np.ndarray
-            The preprocessed target vector, adjusted for optimization.
-        """
-        for treeno, fit_tree in enumerate(self.decision_trees_.values()):
-            self._get_matrix(x, y, vec_y, fit_tree, treeno)
-
-    def fit(self, x: ArrayLike, y: ArrayLike, sample_weight: ArrayLike | None = None):
-        """
-        Fits the RUXClassifier to the training data.
-
-        Parameters
-        ----------
-        x : array-like of shape (n_samples, n_features)
-            The training input samples. Internally, it will be converted to dtype=np.float32.
-        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
-            The target values (class labels) as integers
-        sample_weight : array-like of shape (n_samples,), default=None
-            Sample weights. If None, then samples are equally weighted.
-
-        Returns
-        -------
-        RUXClassifier
-            The fitted model, ready for making predictions.
-        """
-        x, y = check_inputs(x, y)
-
-        # If the model has been fitted before, clean it up
-        if self.coefficients_.cols.shape[0] != 0:
-            self._cleanup()
-
-        # Fills the fitted decision trees.
-        tree_infos = self.trained_ensemble.estimators_
-        for treeno, fit_tree in enumerate(tree_infos):
-            self.decision_trees_[treeno] = (
-                fit_tree[0] if isinstance(fit_tree, np.ndarray) else fit_tree
-            )
-
-        # Extract and set properties of the target variable
-        self._get_class_infos(y)
-
-        # Preprocess the target values
-        vec_y = self._preprocess(y)
-
-        # Calculate the coefficients and other parameters for the optimization problem
-        self._get_matrices(x, y, vec_y)
-
-        sample_weight = self._get_sample_wight(sample_weight, self.class_weight, y)
-
-        # Solve the optimization problem again with the new rules
-        ws, *_ = self.solver(
-            coefficients=self.coefficients_, k=self.k_, sample_weight=sample_weight
-        )
-
-        # Fill the decision rules based on the weights obtained from the optimization problem
-        self._fill_rules(ws)
-
-        # Mark the model as fitted
-        self._is_fitted = True
-
-        # Return the fitted model
-        return self
+from __future__ import annotations
+
+import numpy as np
+from sklearn.ensemble._forest import ForestClassifier
+from sklearn.ensemble import GradientBoostingClassifier
+from numpy.typing import ArrayLike
+
+from .base_sklearn import _RUGSKLEARN
+from ...rule_cost import Gini
+from ...utils import check_inputs
+
+
+class RUXClassifier(_RUGSKLEARN):
+    """
+    RUXClassifier aims to build a compact and interpretable model
+    by employing rule-based learning extracted from a trained scikit-learn
+    ensemble model (such as Gradient Boosting or Random Forest). It allows
+    a user to trade off between the complexity of the model (number of rules)
+    and the accuracy of the model.
+    """
+
+    def __init__(
+        self,
+        trained_ensemble,
+        solver,
+        *,
+        rule_cost=Gini(),
+        class_weight: dict | str | None = None,
+        threshold: float = 1.0e-6,
+        random_state: int | None = None,
+    ):
+        """
+        Parameters
+        ----------
+        trained_ensemble : sklearn.ensemble object
+            The trained scikit-learn ensemble model from which the rules will be
+            extracted.
+
+        solver : OptimizationSolver
+            An instance of a derived class inherits from the 'Optimization Solver' base class.
+            The solver is responsible for optimizing the rule set based on the cost function
+            and constraints.
+
+        rule_cost : RuleCost or int, default=Gini()
+            Defines the cost of rules, either as a specific calculation method (RuleCost instance)
+            or a fixed cost
+
+        class_weight: dict, "balanced" or None, default=None
+            A dictionary mapping class labels to their respective weights, the string "balanced"
+            to automatically adjust weights inversely proportional to class frequencies,
+            or None for no weights. Used to adjust the model in favor of certain classes.
+
+        threshold : float, default=1.0e-6
+            The minimum weight threshold for including a rule in the final model
+
+        random_state : int or None, default=None
+            Seed for the random number generator to ensure reproducible results.
+            Defaults to None.
+        """
+        if not (
+            isinstance(trained_ensemble, (GradientBoostingClassifier, ForestClassifier))
+        ):
+            raise TypeError(
+                "trained_ensemble must be an instance of ",
+                "sklearn.ensemble.GradientBoostingClassifier, ",
+                "sklearn.ensemble.RandomForestClassifier, ",
+                "or sklearn.ensemble.ExtraTreesClassifier.",
+            )
+
+        self.trained_ensemble = trained_ensemble
+        super().__init__(
+            threshold=threshold,
+            random_state=random_state,
+            rule_cost=rule_cost,
+            solver=solver,
+            class_weight=class_weight,
+        )
+
+    def _get_matrices(self, x: np.ndarray, y: np.ndarray, vec_y: np.ndarray) -> None:
+        """
+        Prepares the optimization problem matrices based on the ensemble of decision trees.
+
+        Parameters
+        ----------
+        x : np.ndarray
+            The feature matrix of the training data.
+        y : np.ndarray
+            The target vector of the training data.
+        vec_y : np.ndarray
+            The preprocessed target vector, adjusted for optimization.
+        """
+        for treeno, fit_tree in enumerate(self.decision_trees_.values()):
+            self._get_matrix(x, y, vec_y, fit_tree, treeno)
+
+    def fit(self, x: ArrayLike, y: ArrayLike, sample_weight: ArrayLike | None = None):
+        """
+        Fits the RUXClassifier to the training data.
+
+        Parameters
+        ----------
+        x : array-like of shape (n_samples, n_features)
+            The training input samples. Internally, it will be converted to dtype=np.float32.
+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
+            The target values (class labels) as integers
+        sample_weight : array-like of shape (n_samples,), default=None
+            Sample weights. If None, then samples are equally weighted.
+
+        Returns
+        -------
+        RUXClassifier
+            The fitted model, ready for making predictions.
+        """
+        x, y = check_inputs(x, y)
+
+        # If the model has been fitted before, clean it up
+        if self.coefficients_.cols.shape[0] != 0:
+            self._cleanup()
+
+        # Fills the fitted decision trees.
+        tree_infos = self.trained_ensemble.estimators_
+        for treeno, fit_tree in enumerate(tree_infos):
+            self.decision_trees_[treeno] = (
+                fit_tree[0] if isinstance(fit_tree, np.ndarray) else fit_tree
+            )
+
+        # Extract and set properties of the target variable
+        self._get_class_infos(y)
+
+        # Preprocess the target values
+        vec_y = self._preprocess(y)
+
+        # Calculate the coefficients and other parameters for the optimization problem
+        self._get_matrices(x, y, vec_y)
+
+        sample_weight = self._get_sample_weight(sample_weight, self.class_weight, y)
+
+        # Solve the optimization problem again with the new rules
+        ws, *_ = self.solver(
+            coefficients=self.coefficients_, k=self.k_, sample_weight=sample_weight
+        )
+
+        # Fill the decision rules based on the weights obtained from the optimization problem
+        self._fill_rules(ws)
+
+        # Mark the model as fitted
+        self._is_fitted = True
+
+        # Return the fitted model
+        return self
```

## ruleopt/explainer/__init__.py

 * *Ordering differences only*

```diff
@@ -1,4 +1,4 @@
-from .explainer import (Explainer )
-
-__all__ = [
+from .explainer import (Explainer )
+
+__all__ = [
     "Explainer"]
```

## ruleopt/explainer/explainer.py

 * *Ordering differences only*

```diff
@@ -1,243 +1,243 @@
-from __future__ import annotations
-import warnings
-import numpy as np
-from sklearn.utils import check_array
-from sklearn.utils.validation import check_is_fitted
-from ..estimator.base import _RUGBASE
-
-from numpy.typing import ArrayLike
-
-
-class Explainer:
-    """
-    Initializes the Explainer with a given estimator. The estimator must be fitted and
-    of a type that inherits from _RUGBASE, such as RUGClassifier, RUXClassifier,
-    RUXLGBMClassifier, or RUXXGBClassifier.
-    """
-
-    def __init__(self, estimator: _RUGBASE) -> None:
-        """
-        Parameters
-        ----------
-        estimator : ruleopt.estimator instance
-            A fitted estimator of a type that inherits from _RUGBASE.
-        """
-        if not isinstance(estimator, _RUGBASE):
-            raise TypeError(
-                "Estimator should be an instance of a class inheriting from _RUGBASE, ",
-                f"not {type(estimator)}",
-            )
-
-        check_is_fitted(estimator, attributes=["_is_fitted"])
-
-        self.estimator = estimator
-
-    def retrieve_rule_details(
-        self,
-        feature_names: list | None = None,
-        indices: list | None = None,
-        info: bool = True,
-    ) -> dict:
-        """
-        Retrieves and optionally prints detailed information about the specified rules.
-        If indices are provided, information for those specific rules is returned.
-        Otherwise, information for all rules is returned.
-
-        Parameters
-        ----------
-        feature_names : list or None, default=None
-            List of feature names for more readable rule descriptions. If None, indices are used.
-
-        indices :list or None, default=None
-            Indices of the rules to retrieve. If None, retrieves all rules.
-
-        info : bool, default=True
-            If True, prints the rules' details in a human-readable format.
-
-        Returns
-        -------
-        dict
-            A dictionary mapping each rule index to its details, including label, weight,
-            rule description, and statistical distribution. If a rule has no conditions,
-            it sets the majority class.
-        """
-        rules = self.estimator.decision_rules_
-        if indices is not None:
-            # Validate indices
-            max_index = max(rules.keys())
-            indices = [i for i in indices if i <= max_index]
-            if len(indices) < len(set(indices)):
-                warnings.warn("Some specified indices are out of range.")
-        else:
-            indices = list(rules.keys())
-
-        return_dict = {}
-        for indx in indices:
-            rule = rules.get(indx, None)
-            if rule is None:
-                continue  # Skip if rule does not exist, alternative to raising an error
-            rule_details = {
-                "label": int(rule.label),
-                "weight": float(rule.weight),
-                "rule": (
-                    rule.to_dict(feature_names)
-                    if rule
-                    else "No Rule: Set Majority Class"
-                ),
-                "sdist": rule.sdist.tolist(),
-            }
-            return_dict[indx] = rule_details
-            if info:
-                self._display_rule_info(indx, rule, feature_names)
-
-        return return_dict
-
-    def find_applicable_rules_for_samples(
-        self,
-        x: ArrayLike,
-        threshold: float = 0.0,
-        feature_names: list | None = None,
-        info: bool = True,
-    ) -> list:
-        """
-        Identifies which rules apply to each instance in the provided input data
-        based on a given threshold.Optionally, prints detailed information about
-        each rule that applies to the instances.
-
-        Parameters
-        ----------
-        x : array-like of shape (n_samples, n_features)
-            The training input samples. Internally, it will be converted to dtype=np.float32.
-
-        threshold : float, default=0.0
-            Minimum rule weight threshold for considering a rule as covering an instance.
-
-        feature_names : list or None, default=None
-            List of feature names for more readable rule descriptions. If None, indices are used.
-
-        info : bool, default=True
-            If True, prints the details of the applicable rules for each instance.
-
-        Returns
-        -------
-        List[List[int]]
-            A list of lists, where each inner list contains the indices of rules that cover
-            the corresponding instance in `x`.
-        """
-        x = check_array(np.asarray(x, dtype=np.float32), force_all_finite="allow-nan")
-        x_to_rule_list = []
-        for i, x0 in enumerate(x):
-            if info:
-                print(f"Rules for instance {i}")
-            rule_list = []
-
-            for rule_id, rule in self.estimator.decision_rules_.items():
-                if rule.check_rule(x0) and rule.weight >= threshold:
-                    rule_list.append(rule_id)
-
-                    if info:
-                        self._display_rule_info(rule_id, rule, feature_names)
-
-            x_to_rule_list.append(rule_list)
-
-        return x_to_rule_list
-
-    def _display_rule_info(self, indx, rule, feature_names=None) -> None:
-        """
-        Prints the details of a specified rule in a human-readable format. This
-        method is designed for internal use to support other public methods that
-        may require detailed rule information to be printed.
-
-        Parameters
-        ----------
-        indx : int
-            Index of the rule being printed.
-
-        rule : Rule object
-            The rule whose details are to be printed.
-
-        feature_names : Optional[List[str]], default=None
-            List of feature names for more readable rule descriptions. If None,
-            feature indices are used.
-        """
-        print(f"RULE {indx}:")
-        rule_description = (
-            rule.to_text(feature_names) if feature_names else rule.to_text()
-        )
-        print(rule_description)
-        print(f"Class: {rule.label}")
-        print(f"Scaled rule weight: {rule.weight:.4f}\n")
-
-    def summarize_rule_metrics(self, info: bool = True) -> dict:
-        """
-        Calculates and optionally prints the total number of rules and the average
-        rule length within the model.
-
-        Parameters
-        ----------
-        info : bool, default=True
-            If True, prints the summary information.
-
-        Returns
-        -------
-        dict
-            A dictionary containing 'num_of_rules' (the total number of rules) and
-            'avg_rule_length' (the average length of the rules).
-        """
-        num_of_rules = len(self.estimator.decision_rules_)
-        avg_rule_length = np.mean(
-            [len(rule) for rule in self.estimator.decision_rules_.values()]
-        )
-
-        if info:
-            print(f"Total number of rules: {num_of_rules}")
-            print(f"Average rule length: {avg_rule_length:.2f}")
-
-        return {"num_of_rules": num_of_rules, "avg_rule_length": avg_rule_length}
-
-    def evaluate_rule_coverage_metrics(self, x: ArrayLike, info: bool = True) -> dict:
-        """
-        Calculates metrics including the number of instances not covered by any
-        rule ('num_of_missed'), the average number of rules per sample
-        ('avg_num_rules_per_sample'), and the average rule length per sample
-        ('avg_rule_length_per_sample'). Optionally, prints this information.
-
-        Parameters
-        ----------
-        x : array-like of shape (n_samples, n_features)
-            The training input samples. Internally, it will be converted to dtype=np.float32.
-
-        info : bool, default=True
-            If True, prints the calculated metrics.
-
-        Returns
-        -------
-        dict
-            A dictionary with calculated metrics: 'num_of_missed', 'avg_num_rules_per_sample',
-            and 'avg_rule_length_per_sample'.
-        """
-
-        with warnings.catch_warnings():
-            warnings.simplefilter("ignore")
-            missed_values_index_, rules_per_sample_, rule_length_per_sample_ = (
-                self.estimator.predict(x, predict_info=True)
-            )
-
-        results = {
-            "num_of_missed": len(missed_values_index_),
-            "avg_num_rules_per_sample": np.mean(rules_per_sample_),
-            "avg_rule_length_per_sample": np.mean(rule_length_per_sample_),
-        }
-
-        if info:
-            print(
-                f"Number of instances not covered by any rule: {results['num_of_missed']}"
-            )
-            print(
-                f"Average number of rules per sample: {results['avg_num_rules_per_sample']:.2f}"
-            )
-            print(
-                f"Average length of rules per sample: {results['avg_rule_length_per_sample']:.2f}"
-            )
-
-        return results
+from __future__ import annotations
+import warnings
+import numpy as np
+from sklearn.utils import check_array
+from sklearn.utils.validation import check_is_fitted
+from ..estimator.base import _RUGBASE
+
+from numpy.typing import ArrayLike
+
+
+class Explainer:
+    """
+    Initializes the Explainer with a given estimator. The estimator must be fitted and
+    of a type that inherits from _RUGBASE, such as RUGClassifier, RUXClassifier,
+    RUXLGBMClassifier, or RUXXGBClassifier.
+    """
+
+    def __init__(self, estimator: _RUGBASE) -> None:
+        """
+        Parameters
+        ----------
+        estimator : ruleopt.estimator instance
+            A fitted estimator of a type that inherits from _RUGBASE.
+        """
+        if not isinstance(estimator, _RUGBASE):
+            raise TypeError(
+                "Estimator should be an instance of a class inheriting from _RUGBASE, ",
+                f"not {type(estimator)}",
+            )
+
+        check_is_fitted(estimator, attributes=["_is_fitted"])
+
+        self.estimator = estimator
+
+    def retrieve_rule_details(
+        self,
+        feature_names: list | None = None,
+        indices: list | None = None,
+        info: bool = True,
+    ) -> dict:
+        """
+        Retrieves and optionally prints detailed information about the specified rules.
+        If indices are provided, information for those specific rules is returned.
+        Otherwise, information for all rules is returned.
+
+        Parameters
+        ----------
+        feature_names : list or None, default=None
+            List of feature names for more readable rule descriptions. If None, indices are used.
+
+        indices :list or None, default=None
+            Indices of the rules to retrieve. If None, retrieves all rules.
+
+        info : bool, default=True
+            If True, prints the rules' details in a human-readable format.
+
+        Returns
+        -------
+        dict
+            A dictionary mapping each rule index to its details, including label, weight,
+            rule description, and statistical distribution. If a rule has no conditions,
+            it sets the majority class.
+        """
+        rules = self.estimator.decision_rules_
+        if indices is not None:
+            # Validate indices
+            max_index = max(rules.keys())
+            indices = [i for i in indices if i <= max_index]
+            if len(indices) < len(set(indices)):
+                warnings.warn("Some specified indices are out of range.")
+        else:
+            indices = list(rules.keys())
+
+        return_dict = {}
+        for indx in indices:
+            rule = rules.get(indx, None)
+            if rule is None:
+                continue  # Skip if rule does not exist, alternative to raising an error
+            rule_details = {
+                "label": int(rule.label),
+                "weight": float(rule.weight),
+                "rule": (
+                    rule.to_dict(feature_names)
+                    if rule
+                    else "No Rule: Set Majority Class"
+                ),
+                "sdist": rule.sdist.tolist(),
+            }
+            return_dict[indx] = rule_details
+            if info:
+                self._display_rule_info(indx, rule, feature_names)
+
+        return return_dict
+
+    def find_applicable_rules_for_samples(
+        self,
+        x: ArrayLike,
+        threshold: float = 0.0,
+        feature_names: list | None = None,
+        info: bool = True,
+    ) -> list:
+        """
+        Identifies which rules apply to each instance in the provided input data
+        based on a given threshold.Optionally, prints detailed information about
+        each rule that applies to the instances.
+
+        Parameters
+        ----------
+        x : array-like of shape (n_samples, n_features)
+            The training input samples. Internally, it will be converted to dtype=np.float32.
+
+        threshold : float, default=0.0
+            Minimum rule weight threshold for considering a rule as covering an instance.
+
+        feature_names : list or None, default=None
+            List of feature names for more readable rule descriptions. If None, indices are used.
+
+        info : bool, default=True
+            If True, prints the details of the applicable rules for each instance.
+
+        Returns
+        -------
+        List[List[int]]
+            A list of lists, where each inner list contains the indices of rules that cover
+            the corresponding instance in `x`.
+        """
+        x = check_array(np.asarray(x, dtype=np.float32), force_all_finite="allow-nan")
+        x_to_rule_list = []
+        for i, x0 in enumerate(x):
+            if info:
+                print(f"Rules for instance {i}")
+            rule_list = []
+
+            for rule_id, rule in self.estimator.decision_rules_.items():
+                if rule.check_rule(x0) and rule.weight >= threshold:
+                    rule_list.append(rule_id)
+
+                    if info:
+                        self._display_rule_info(rule_id, rule, feature_names)
+
+            x_to_rule_list.append(rule_list)
+
+        return x_to_rule_list
+
+    def _display_rule_info(self, indx, rule, feature_names=None) -> None:
+        """
+        Prints the details of a specified rule in a human-readable format. This
+        method is designed for internal use to support other public methods that
+        may require detailed rule information to be printed.
+
+        Parameters
+        ----------
+        indx : int
+            Index of the rule being printed.
+
+        rule : Rule object
+            The rule whose details are to be printed.
+
+        feature_names : Optional[List[str]], default=None
+            List of feature names for more readable rule descriptions. If None,
+            feature indices are used.
+        """
+        print(f"RULE {indx}:")
+        rule_description = (
+            rule.to_text(feature_names) if feature_names else rule.to_text()
+        )
+        print(rule_description)
+        print(f"Class: {rule.label}")
+        print(f"Scaled rule weight: {rule.weight:.4f}\n")
+
+    def summarize_rule_metrics(self, info: bool = True) -> dict:
+        """
+        Calculates and optionally prints the total number of rules and the average
+        rule length within the model.
+
+        Parameters
+        ----------
+        info : bool, default=True
+            If True, prints the summary information.
+
+        Returns
+        -------
+        dict
+            A dictionary containing 'num_of_rules' (the total number of rules) and
+            'avg_rule_length' (the average length of the rules).
+        """
+        num_of_rules = len(self.estimator.decision_rules_)
+        avg_rule_length = np.mean(
+            [len(rule) for rule in self.estimator.decision_rules_.values()]
+        )
+
+        if info:
+            print(f"Total number of rules: {num_of_rules}")
+            print(f"Average rule length: {avg_rule_length:.2f}")
+
+        return {"num_of_rules": num_of_rules, "avg_rule_length": avg_rule_length}
+
+    def evaluate_rule_coverage_metrics(self, x: ArrayLike, info: bool = True) -> dict:
+        """
+        Calculates metrics including the number of instances not covered by any
+        rule ('num_of_missed'), the average number of rules per sample
+        ('avg_num_rules_per_sample'), and the average rule length per sample
+        ('avg_rule_length_per_sample'). Optionally, prints this information.
+
+        Parameters
+        ----------
+        x : array-like of shape (n_samples, n_features)
+            The training input samples. Internally, it will be converted to dtype=np.float32.
+
+        info : bool, default=True
+            If True, prints the calculated metrics.
+
+        Returns
+        -------
+        dict
+            A dictionary with calculated metrics: 'num_of_missed', 'avg_num_rules_per_sample',
+            and 'avg_rule_length_per_sample'.
+        """
+
+        with warnings.catch_warnings():
+            warnings.simplefilter("ignore")
+            missed_values_index_, rules_per_sample_, rule_length_per_sample_ = (
+                self.estimator.predict(x, predict_info=True)
+            )
+
+        results = {
+            "num_of_missed": len(missed_values_index_),
+            "avg_num_rules_per_sample": np.mean(rules_per_sample_),
+            "avg_rule_length_per_sample": np.mean(rule_length_per_sample_),
+        }
+
+        if info:
+            print(
+                f"Number of instances not covered by any rule: {results['num_of_missed']}"
+            )
+            print(
+                f"Average number of rules per sample: {results['avg_num_rules_per_sample']:.2f}"
+            )
+            print(
+                f"Average length of rules per sample: {results['avg_rule_length_per_sample']:.2f}"
+            )
+
+        return results
```

## ruleopt/rule_cost/__init__.py

 * *Ordering differences only*

```diff
@@ -1,3 +1,3 @@
-from .rule_cost import Length, Gini, Mixed, MixedSigmoid, RuleCost
-
-__all__ = ["Length", "Gini", "Mixed", "MixedSigmoid", "RuleCost"]
+from .rule_cost import Length, Gini, Mixed, MixedSigmoid, RuleCost
+
+__all__ = ["Length", "Gini", "Mixed", "MixedSigmoid", "RuleCost"]
```

## ruleopt/rule_cost/rule_cost.py

 * *Ordering differences only*

```diff
@@ -1,173 +1,173 @@
-from abc import abstractmethod, ABC
-import numpy as np
-from ..aux_classes import Rule
-
-
-class RuleCost(ABC):
-    """
-    Abstract base class representing the cost associated with a rule or a set of rules.
-
-    This class is designed to be subclassed by specific cost calculation implementations.
-    Each subclass should provide a specific cost calculation strategy by overriding the
-    `__call__` method. The `__call__` method allows instances of the subclass to be used
-    as if they were functions, directly invoking the cost calculation.
-    """
-
-    @abstractmethod
-    def __call__(self):
-        """
-        Abstract method to be implemented by subclasses to calculate and return the cost.
-
-        This method should take relevant parameters as input (e.g., the rule or set of
-        rules being evaluated) and return a numerical value representing the cost. The
-        exact implementation details and parameters are to be defined in the subclass.
-
-        Returns
-        -------
-        cost : float
-            The calculated cost of the rule or set of rules. This is a numerical value
-            indicating the cost or penalty associated with the rule(s) based on the
-            specific calculation strategy implemented in the subclass.
-        """
-
-
-class Length(RuleCost):
-    """
-    Calculate the cost of a rule based on its length.
-    """
-
-    def __call__(self, temp_rule: Rule, *args, **kwargs) -> int:
-        """
-        Calculate and return the cost of the rule.
-
-        Parameters
-        ----------
-        temp_rule : Rule
-            The rule to calculate the cost for.
-
-        Returns
-        -------
-        int
-            The cost of the rule, defined as its length.
-        """
-        cost = len(temp_rule)
-        return cost
-
-
-class Gini(RuleCost):
-    """
-    Calculate the Gini cost of a split.
-    """
-
-    def __call__(self, counts: np.ndarray, *args, **kwargs) -> float:
-        """
-        Calculate and return the Gini cost for a node.
-
-        Parameters
-        ----------
-        counts : np.ndarray
-            An array containing the counts of each class in a node.
-
-        Returns
-        -------
-        float
-            The Gini cost for the node.
-        """
-        probs = np.divide(counts, np.sum(counts))
-        cost = 1 - np.sum(np.square(probs))
-        return cost
-
-
-class Mixed(RuleCost):
-    """
-    Calculate the mixed cost, combining class separation and data selection terms with
-    a weighting parameter.
-    """
-
-    def __init__(self, w: float = 0.7) -> None:
-        """
-        Initialize the mixed cost calculation with a weighting parameter.
-
-        Parameters
-        ----------
-        w : float, optional, default=0.7
-            Weighting parameter to balance class separation and data selection terms.
-        """
-        self.w = w
-
-    def __call__(self, covers: np.ndarray, y: np.ndarray, *args, **kwargs) -> float:
-        """
-        Calculate and return the mixed cost for a rule.
-
-        Parameters
-        ----------
-        covers : np.ndarray
-            Array of cover sizes for the classes.
-
-        y : np.ndarray
-            The target array.
-
-        Returns
-        -------
-        float
-            The mixed cost for the rule.
-        """
-        class_separation_term = 1 - (1 - (np.min(covers) / covers.shape[0]))
-        data_selection_term = 1 - (covers.shape[0] / y.shape[0])
-        cost = self.w * class_separation_term + (1 - self.w) * data_selection_term
-        return cost
-
-
-class MixedSigmoid(RuleCost):
-    """
-    Calculate the mixed cost with a sigmoid adjustment based on weighting and alpha
-    parameters.
-    """
-
-    def __init__(self, w: float = 0.7, alpha: float = 10) -> None:
-        """
-        Initialize the sigmoid-adjusted mixed cost calculation with weighting and
-        alpha parameters.
-
-        Parameters
-        ----------
-        w : float, optional, default=0.7
-            Weighting parameter to balance class separation and data selection terms.
-
-        alpha : float, optional, default=10
-            Scaling parameter to adjust the steepness of the sigmoid function.
-        """
-        self.w = w
-        self.alpha = alpha
-
-    def __call__(self, covers: np.ndarray, y: np.ndarray, *args, **kwargs) -> float:
-        """
-        Calculate and return the sigmoid-adjusted mixed cost for a rule.
-
-        Parameters
-        ----------
-        covers : np.ndarray
-            Array of cover sizes for the classes.
-
-        y : np.ndarray
-            The target array.
-
-        Returns
-        -------
-        float
-            The sigmoid-adjusted mixed cost for the rule.
-        """
-        class_separation_term = 1 - (1 - (np.min(covers) / covers.shape[0]))
-        data_selection_term = 1 - (covers.shape[0] / y.shape[0])
-        cost = 1 / (
-            1
-            + np.exp(
-                -self.alpha
-                * (
-                    self.w * class_separation_term
-                    + (1 - self.w) * data_selection_term
-                    - 0.5
-                )
-            )
-        )
-        return cost
+from abc import abstractmethod, ABC
+import numpy as np
+from ..aux_classes import Rule
+
+
+class RuleCost(ABC):
+    """
+    Abstract base class representing the cost associated with a rule or a set of rules.
+
+    This class is designed to be subclassed by specific cost calculation implementations.
+    Each subclass should provide a specific cost calculation strategy by overriding the
+    `__call__` method. The `__call__` method allows instances of the subclass to be used
+    as if they were functions, directly invoking the cost calculation.
+    """
+
+    @abstractmethod
+    def __call__(self):
+        """
+        Abstract method to be implemented by subclasses to calculate and return the cost.
+
+        This method should take relevant parameters as input (e.g., the rule or set of
+        rules being evaluated) and return a numerical value representing the cost. The
+        exact implementation details and parameters are to be defined in the subclass.
+
+        Returns
+        -------
+        cost : float
+            The calculated cost of the rule or set of rules. This is a numerical value
+            indicating the cost or penalty associated with the rule(s) based on the
+            specific calculation strategy implemented in the subclass.
+        """
+
+
+class Length(RuleCost):
+    """
+    Calculate the cost of a rule based on its length.
+    """
+
+    def __call__(self, temp_rule: Rule, *args, **kwargs) -> int:
+        """
+        Calculate and return the cost of the rule.
+
+        Parameters
+        ----------
+        temp_rule : Rule
+            The rule to calculate the cost for.
+
+        Returns
+        -------
+        int
+            The cost of the rule, defined as its length.
+        """
+        cost = len(temp_rule)
+        return cost
+
+
+class Gini(RuleCost):
+    """
+    Calculate the Gini cost of a split.
+    """
+
+    def __call__(self, counts: np.ndarray, *args, **kwargs) -> float:
+        """
+        Calculate and return the Gini cost for a node.
+
+        Parameters
+        ----------
+        counts : np.ndarray
+            An array containing the counts of each class in a node.
+
+        Returns
+        -------
+        float
+            The Gini cost for the node.
+        """
+        probs = np.divide(counts, np.sum(counts))
+        cost = 1 - np.sum(np.square(probs))
+        return cost
+
+
+class Mixed(RuleCost):
+    """
+    Calculate the mixed cost, combining class separation and data selection terms with
+    a weighting parameter.
+    """
+
+    def __init__(self, w: float = 0.7) -> None:
+        """
+        Initialize the mixed cost calculation with a weighting parameter.
+
+        Parameters
+        ----------
+        w : float, optional, default=0.7
+            Weighting parameter to balance class separation and data selection terms.
+        """
+        self.w = w
+
+    def __call__(self, covers: np.ndarray, y: np.ndarray, *args, **kwargs) -> float:
+        """
+        Calculate and return the mixed cost for a rule.
+
+        Parameters
+        ----------
+        covers : np.ndarray
+            Array of cover sizes for the classes.
+
+        y : np.ndarray
+            The target array.
+
+        Returns
+        -------
+        float
+            The mixed cost for the rule.
+        """
+        class_separation_term = 1 - (1 - (np.min(covers) / covers.shape[0]))
+        data_selection_term = 1 - (covers.shape[0] / y.shape[0])
+        cost = self.w * class_separation_term + (1 - self.w) * data_selection_term
+        return cost
+
+
+class MixedSigmoid(RuleCost):
+    """
+    Calculate the mixed cost with a sigmoid adjustment based on weighting and alpha
+    parameters.
+    """
+
+    def __init__(self, w: float = 0.7, alpha: float = 10) -> None:
+        """
+        Initialize the sigmoid-adjusted mixed cost calculation with weighting and
+        alpha parameters.
+
+        Parameters
+        ----------
+        w : float, optional, default=0.7
+            Weighting parameter to balance class separation and data selection terms.
+
+        alpha : float, optional, default=10
+            Scaling parameter to adjust the steepness of the sigmoid function.
+        """
+        self.w = w
+        self.alpha = alpha
+
+    def __call__(self, covers: np.ndarray, y: np.ndarray, *args, **kwargs) -> float:
+        """
+        Calculate and return the sigmoid-adjusted mixed cost for a rule.
+
+        Parameters
+        ----------
+        covers : np.ndarray
+            Array of cover sizes for the classes.
+
+        y : np.ndarray
+            The target array.
+
+        Returns
+        -------
+        float
+            The sigmoid-adjusted mixed cost for the rule.
+        """
+        class_separation_term = 1 - (1 - (np.min(covers) / covers.shape[0]))
+        data_selection_term = 1 - (covers.shape[0] / y.shape[0])
+        cost = 1 / (
+            1
+            + np.exp(
+                -self.alpha
+                * (
+                    self.w * class_separation_term
+                    + (1 - self.w) * data_selection_term
+                    - 0.5
+                )
+            )
+        )
+        return cost
```

## ruleopt/solver/__init__.py

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-from .unc_solver import UNCSolver
-from .ortools_solver import ORToolsSolver
-from .base import OptimizationSolver
-from .gurobi_solver import GurobiSolver
-from .cplex_solver import CPLEXSolver
-
-__all__ = ["UNCSolver", "ORToolsSolver", "GurobiSolver", "CPLEXSolver", "OptimizationSolver"]
+from .unc_solver import UNCSolver
+from .ortools_solver import ORToolsSolver
+from .base import OptimizationSolver
+from .gurobi_solver import GurobiSolver
+from .cplex_solver import CPLEXSolver
+
+__all__ = ["UNCSolver", "ORToolsSolver", "GurobiSolver", "CPLEXSolver", "OptimizationSolver"]
```

## ruleopt/solver/base.py

 * *Ordering differences only*

```diff
@@ -1,82 +1,82 @@
-from abc import ABC, abstractmethod
-from typing import Any
-import warnings
-
-
-class OptimizationSolver(ABC):
-    """
-    This abstract base class defines the interface for a generic solver.
-    Implementations of this class must provide the `__call__` method, 
-    allowing the solver to be invoked as if it were a function.
-    """
-
-    def __init__(self) -> None:
-        super().__init__()
-        if not hasattr(self, "penalty"):
-            raise AttributeError("Subclasses must define 'penalty'")
-        if not hasattr(self, "use_sparse"):
-            raise AttributeError("Subclasses must define 'use_sparse'")
-
-    @abstractmethod
-    def __call__(self, *args: Any, **kwds: Any) -> Any:
-        """
-        Executes the solver using the provided arguments and keyword arguments.
-
-        Parameters:
-            *args (Any): Positional arguments required for solving the problem.
-            **kwds (Any): Keyword arguments required for solving the problem.
-
-        Returns:
-            Any: The result of the solving process.
-        """
-        pass
-
-    def _check_params(self):
-        if not isinstance(self.penalty, (float, int)) or self.penalty <= 0:
-            raise TypeError("penalty must be a positive float.")
-
-        if not isinstance(self.use_sparse, bool):
-            raise TypeError(f"use_sparse must be True or False.")
-
-        if hasattr(self, "solver_type"):
-            valid_solvers = [
-                "CLP",
-                "GLOP",
-                "GUROBI_LP",
-                "CPLEX_LP",
-                "XPRESS_LP",
-                "GLPK_LP",
-                "HiGHS",
-            ]
-            if (
-                not isinstance(self.solver_type, str)
-                or self.solver_type not in valid_solvers
-            ):
-                raise ValueError(f"solver_type must be one of {valid_solvers}.")
-
-        if hasattr(self, "lr"):
-            if not isinstance(self.lr, (float, int)) or self.lr <= 0:
-                raise TypeError("lr must be a positive float.")
-            
-        if hasattr(self, "constraint_cost"):
-            if not isinstance(self.constraint_cost, (float, int)) or self.constraint_cost <= 0:
-                raise TypeError("constraint_cost must be a positive float.")
-
-        if hasattr(self, "weight_decay"):
-            if not isinstance(self.weight_decay, (int, float)) or self.weight_decay < 0:
-                raise TypeError("weight_decay must be a non-negative float.")
-
-        if hasattr(self, "patience"):
-            if not isinstance(self.patience, int) or self.patience <= 0:
-                raise TypeError("patience must be a positive integer.")
-
-        if hasattr(self, "device"):
-            valid_devices = ["cuda", "cpu"]
-            if not isinstance(self.device, str) or self.device not in valid_devices:
-                raise ValueError(f"solver_type must be one of {valid_devices}.")
-
-        if self.use_sparse:
-            warnings.warn(
-                "A sparse data format is being used. If your dataset is not sufficiently "
-                "large, using a sparse format could lead to performance issues.",
-            )
+from abc import ABC, abstractmethod
+from typing import Any
+import warnings
+
+
+class OptimizationSolver(ABC):
+    """
+    This abstract base class defines the interface for a generic solver.
+    Implementations of this class must provide the `__call__` method, 
+    allowing the solver to be invoked as if it were a function.
+    """
+
+    def __init__(self) -> None:
+        super().__init__()
+        if not hasattr(self, "penalty"):
+            raise AttributeError("Subclasses must define 'penalty'")
+        if not hasattr(self, "use_sparse"):
+            raise AttributeError("Subclasses must define 'use_sparse'")
+
+    @abstractmethod
+    def __call__(self, *args: Any, **kwds: Any) -> Any:
+        """
+        Executes the solver using the provided arguments and keyword arguments.
+
+        Parameters:
+            *args (Any): Positional arguments required for solving the problem.
+            **kwds (Any): Keyword arguments required for solving the problem.
+
+        Returns:
+            Any: The result of the solving process.
+        """
+        pass
+
+    def _check_params(self):
+        if not isinstance(self.penalty, (float, int)) or self.penalty <= 0:
+            raise TypeError("penalty must be a positive float.")
+
+        if not isinstance(self.use_sparse, bool):
+            raise TypeError(f"use_sparse must be True or False.")
+
+        if hasattr(self, "solver_type"):
+            valid_solvers = [
+                "CLP",
+                "GLOP",
+                "GUROBI_LP",
+                "CPLEX_LP",
+                "XPRESS_LP",
+                "GLPK_LP",
+                "HiGHS",
+            ]
+            if (
+                not isinstance(self.solver_type, str)
+                or self.solver_type not in valid_solvers
+            ):
+                raise ValueError(f"solver_type must be one of {valid_solvers}.")
+
+        if hasattr(self, "lr"):
+            if not isinstance(self.lr, (float, int)) or self.lr <= 0:
+                raise TypeError("lr must be a positive float.")
+            
+        if hasattr(self, "constraint_cost"):
+            if not isinstance(self.constraint_cost, (float, int)) or self.constraint_cost <= 0:
+                raise TypeError("constraint_cost must be a positive float.")
+
+        if hasattr(self, "weight_decay"):
+            if not isinstance(self.weight_decay, (int, float)) or self.weight_decay < 0:
+                raise TypeError("weight_decay must be a non-negative float.")
+
+        if hasattr(self, "patience"):
+            if not isinstance(self.patience, int) or self.patience <= 0:
+                raise TypeError("patience must be a positive integer.")
+
+        if hasattr(self, "device"):
+            valid_devices = ["cuda", "cpu"]
+            if not isinstance(self.device, str) or self.device not in valid_devices:
+                raise ValueError(f"solver_type must be one of {valid_devices}.")
+
+        if self.use_sparse:
+            warnings.warn(
+                "A sparse data format is being used. If your dataset is not sufficiently "
+                "large, using a sparse format could lead to performance issues.",
+            )
```

## ruleopt/solver/cplex_solver.py

 * *Ordering differences only*

```diff
@@ -1,129 +1,129 @@
-from typing import Tuple
-import numpy as np
-from scipy.sparse import csr_matrix
-from ..utils import check_module_available
-from .base import OptimizationSolver
-
-CPLEX_AVAILABLE = check_module_available("docplex")
-
-
-class CPLEXSolver(OptimizationSolver):
-    """
-    A solver wrapper function for linear optimization using the proprietary CPLEX optimizer.
-
-    The solver supports both dense and sparse matrix representations.
-    """
-
-    def __new__(cls, *args, **kwargs):
-        if not CPLEX_AVAILABLE:
-            raise ImportError(
-                "CPLEX is required for this class but is not installed.",
-                "Please install it with 'pip install docplex cplex'",
-            )
-        instance = super(CPLEXSolver, cls).__new__(cls)
-        return instance
-
-    def __init__(
-        self,
-        penalty: float = 2.0,
-        use_sparse: bool = False,
-    ) -> None:
-        """
-        Parameters
-        ----------
-        penalty : float, default=2.0
-            Penalty parameter for the cost in the objective function.
-        use_sparse : bool, default=False
-            Determines whether to use a sparse matrix representation for the optimization
-            problem. Using sparse matrices can significantly reduce memory usage and improve
-            performance for large-scale problems with many zeros in the data.
-        """
-        self.penalty = penalty
-        self.use_sparse = use_sparse
-        super().__init__()
-        super()._check_params()
-
-    def __call__(
-        self,
-        coefficients,
-        k: int,
-        sample_weight,
-        ws0: np.ndarray = None,
-        *args,
-        **kwargs,
-    ) -> Tuple[np.ndarray, np.ndarray]:
-        """
-        Parameters
-        ----------
-        coefficients : object
-            An object containing the sparse matrix coefficients ('yvals', 'rows', 'cols'),
-            and costs associated with each rule ('costs').
-        k : float
-            A scaling factor for the coefficients.
-        ws0 : array-like, optional
-            Initial weights for the optimization process. If provided, should have the same
-            length as the number of rules. Otherwise, weights are initialized to ones.
-
-        Returns
-        -------
-        ws : numpy.ndarray
-            The optimal weights for each rule.
-        betas : numpy.ndarray
-            The optimal dual solution.
-        """
-        ### LAZY IMPORT
-        from docplex.mp.model import Model
-
-        a_hat = csr_matrix(
-            (
-                coefficients.yvals,
-                (coefficients.rows, coefficients.cols),
-            ),
-            dtype=np.float64,
-        ) * ((k - 1.0) / k)
-
-        if not self.use_sparse:
-            a_hat = a_hat.toarray()
-
-        costs = np.array(coefficients.costs, copy=False)
-
-        n, m = a_hat.shape
-        # Primal Model
-        modprimal = Model("RUXG Primal")
-
-        # Variables
-        vs = modprimal.continuous_var_list(n, name="vs")
-        ws = modprimal.continuous_var_list(m, name="ws")
-
-        # Set initial values
-        initial_values = []
-
-        if ws0 is not None:
-            initial_values += [(ws[i], ws0[i]) for i in range(len(ws0))]
-
-        # Assign initial solution
-        modprimal.start = initial_values
-
-        # Objective
-        modprimal.minimize(
-            (
-                modprimal.sum(vs)
-                if sample_weight is None
-                else modprimal.sum(vs * sample_weight)
-            )
-            + modprimal.scal_prod(ws, costs * self.penalty)
-        )
-        # Constraints
-        for i in range(n):
-            modprimal.add_constraint(
-                modprimal.sum(a_hat[i, j] * ws[j] for j in range(m)) + vs[i] >= 1.0
-            )
-
-        modprimal.solve()
-
-        betas = np.array(
-            [c.dual_value for c in modprimal.iter_constraints()], dtype=np.float64
-        )
-        ws = np.array([v.solution_value for v in ws], dtype=np.float64)
-
-        return ws, betas
+from typing import Tuple
+import numpy as np
+from scipy.sparse import csr_matrix
+from ..utils import check_module_available
+from .base import OptimizationSolver
+
+CPLEX_AVAILABLE = check_module_available("docplex")
+
+
+class CPLEXSolver(OptimizationSolver):
+    """
+    A solver wrapper function for linear optimization using the proprietary CPLEX optimizer.
+
+    The solver supports both dense and sparse matrix representations.
+    """
+
+    def __new__(cls, *args, **kwargs):
+        if not CPLEX_AVAILABLE:
+            raise ImportError(
+                "CPLEX is required for this class but is not installed.",
+                "Please install it with 'pip install docplex cplex'",
+            )
+        instance = super(CPLEXSolver, cls).__new__(cls)
+        return instance
+
+    def __init__(
+        self,
+        penalty: float = 2.0,
+        use_sparse: bool = False,
+    ) -> None:
+        """
+        Parameters
+        ----------
+        penalty : float, default=2.0
+            Penalty parameter for the cost in the objective function.
+        use_sparse : bool, default=False
+            Determines whether to use a sparse matrix representation for the optimization
+            problem. Using sparse matrices can significantly reduce memory usage and improve
+            performance for large-scale problems with many zeros in the data.
+        """
+        self.penalty = penalty
+        self.use_sparse = use_sparse
+        super().__init__()
+        super()._check_params()
+
+    def __call__(
+        self,
+        coefficients,
+        k: int,
+        sample_weight,
+        ws0: np.ndarray = None,
+        *args,
+        **kwargs,
+    ) -> Tuple[np.ndarray, np.ndarray]:
+        """
+        Parameters
+        ----------
+        coefficients : object
+            An object containing the sparse matrix coefficients ('yvals', 'rows', 'cols'),
+            and costs associated with each rule ('costs').
+        k : float
+            A scaling factor for the coefficients.
+        ws0 : array-like, optional
+            Initial weights for the optimization process. If provided, should have the same
+            length as the number of rules. Otherwise, weights are initialized to ones.
+
+        Returns
+        -------
+        ws : numpy.ndarray
+            The optimal weights for each rule.
+        betas : numpy.ndarray
+            The optimal dual solution.
+        """
+        ### LAZY IMPORT
+        from docplex.mp.model import Model
+
+        a_hat = csr_matrix(
+            (
+                coefficients.yvals,
+                (coefficients.rows, coefficients.cols),
+            ),
+            dtype=np.float64,
+        ) * ((k - 1.0) / k)
+
+        if not self.use_sparse:
+            a_hat = a_hat.toarray()
+
+        costs = np.array(coefficients.costs, copy=False)
+
+        n, m = a_hat.shape
+        # Primal Model
+        modprimal = Model("RUXG Primal")
+
+        # Variables
+        vs = modprimal.continuous_var_list(n, name="vs")
+        ws = modprimal.continuous_var_list(m, name="ws")
+
+        # Set initial values
+        initial_values = []
+
+        if ws0 is not None:
+            initial_values += [(ws[i], ws0[i]) for i in range(len(ws0))]
+
+        # Assign initial solution
+        modprimal.start = initial_values
+
+        # Objective
+        modprimal.minimize(
+            (
+                modprimal.sum(vs)
+                if sample_weight is None
+                else modprimal.sum(vs * sample_weight)
+            )
+            + modprimal.scal_prod(ws, costs * self.penalty)
+        )
+        # Constraints
+        for i in range(n):
+            modprimal.add_constraint(
+                modprimal.sum(a_hat[i, j] * ws[j] for j in range(m)) + vs[i] >= 1.0
+            )
+
+        modprimal.solve()
+
+        betas = np.array(
+            [c.dual_value for c in modprimal.iter_constraints()], dtype=np.float64
+        )
+        ws = np.array([v.solution_value for v in ws], dtype=np.float64)
+
+        return ws, betas
```

## ruleopt/solver/gurobi_solver.py

 * *Ordering differences only*

```diff
@@ -1,119 +1,119 @@
-from typing import Tuple
-import numpy as np
-from scipy.sparse import csr_matrix
-from ..utils import check_module_available
-from .base import OptimizationSolver
-
-GUROBI_AVAILABLE = check_module_available("gurobipy")
-
-
-class GurobiSolver(OptimizationSolver):
-    """
-    A solver wrapper function for linear optimization using the proprietary Gurobi optimizer.
-
-    The solver supports both dense and sparse matrix representations.
-    """
-
-    def __new__(cls, *args, **kwargs):
-        if not GUROBI_AVAILABLE:
-            raise ImportError(
-                "Gurobi is required for this class but is not installed.",
-                "Please install it with 'pip install gurobipy'",
-            )
-        instance = super(GurobiSolver, cls).__new__(cls)
-        return instance
-
-    def __init__(
-        self,
-        penalty: float = 2.0,
-        use_sparse: bool = False,
-    ) -> None:
-        """
-        Parameters
-        ----------
-        penalty : float, default=2.0
-            Penalty parameter for the cost in the objective function.
-
-        use_sparse : bool, default=False
-            Determines whether to use a sparse matrix representation for the optimization
-            problem. Using sparse matrices can significantly reduce memory usage and improve
-            performance for large-scale problems with many zeros in the data.
-        """
-        self.penalty = penalty
-        self.use_sparse = use_sparse
-        super().__init__()
-        super()._check_params()
-
-    def __call__(
-        self,
-        coefficients,
-        k: int,
-        sample_weight,
-        ws0: np.ndarray = None,
-        *args,
-        **kwargs,
-    ) -> Tuple[np.ndarray, np.ndarray]:
-        """
-        Parameters
-        ----------
-        coefficients : object
-            An object containing the sparse matrix coefficients ('yvals', 'rows', 'cols'),
-            and costs associated with each rule ('costs').
-        k : float
-            A scaling factor for the coefficients.
-        ws0 : array-like, optional
-            Initial weights for the optimization process. If provided, should have the same
-            length as the number of rules. Otherwise, weights are initialized to ones.
-
-        Returns
-        -------
-        ws : numpy.ndarray
-            The optimized weights for each rule after the optimization process.
-        betas : numpy.ndarray
-            The betas values indicating constraint violations for the optimized solution.
-        """
-        ### LAZY IMPORT
-        from gurobipy import Model, GRB
-
-        a_hat = csr_matrix(
-            (
-                coefficients.yvals,
-                (coefficients.rows, coefficients.cols),
-            ),
-            dtype=np.float64,
-        ) * ((k - 1.0) / k)
-
-        if not self.use_sparse:
-            a_hat = a_hat.toarray()
-
-        costs = np.array(coefficients.costs, copy=False)
-
-        n, m = a_hat.shape
-
-        modprimal = Model("RUG Primal")
-        modprimal.setParam("OutputFlag", False)
-        # Variables
-        vs = modprimal.addMVar(shape=int(n), name="vs")
-        ws = modprimal.addMVar(shape=int(m), name="ws")
-
-        if ws0 is not None:
-            tempws = np.zeros(m)
-            tempws[: len(ws0)] = ws0
-            ws.setAttr("Start", tempws)
-            modprimal.update()
-        # Objective
-        modprimal.setObjective(
-            (
-                (np.ones(n) @ vs if sample_weight is None else sample_weight @ vs)
-                + (costs * self.penalty) @ ws
-            ),
-            GRB.MINIMIZE,
-        )
-        # Constraints
-        modprimal.addConstr(a_hat @ ws + vs >= 1.0, name="a_hat Constraints")
-
-        modprimal.optimize()
-
-        betas = np.array(modprimal.getAttr(GRB.Attr.Pi)[:n])
-
-        return ws.X, betas
+from typing import Tuple
+import numpy as np
+from scipy.sparse import csr_matrix
+from ..utils import check_module_available
+from .base import OptimizationSolver
+
+GUROBI_AVAILABLE = check_module_available("gurobipy")
+
+
+class GurobiSolver(OptimizationSolver):
+    """
+    A solver wrapper function for linear optimization using the proprietary Gurobi optimizer.
+
+    The solver supports both dense and sparse matrix representations.
+    """
+
+    def __new__(cls, *args, **kwargs):
+        if not GUROBI_AVAILABLE:
+            raise ImportError(
+                "Gurobi is required for this class but is not installed.",
+                "Please install it with 'pip install gurobipy'",
+            )
+        instance = super(GurobiSolver, cls).__new__(cls)
+        return instance
+
+    def __init__(
+        self,
+        penalty: float = 2.0,
+        use_sparse: bool = False,
+    ) -> None:
+        """
+        Parameters
+        ----------
+        penalty : float, default=2.0
+            Penalty parameter for the cost in the objective function.
+
+        use_sparse : bool, default=False
+            Determines whether to use a sparse matrix representation for the optimization
+            problem. Using sparse matrices can significantly reduce memory usage and improve
+            performance for large-scale problems with many zeros in the data.
+        """
+        self.penalty = penalty
+        self.use_sparse = use_sparse
+        super().__init__()
+        super()._check_params()
+
+    def __call__(
+        self,
+        coefficients,
+        k: int,
+        sample_weight,
+        ws0: np.ndarray = None,
+        *args,
+        **kwargs,
+    ) -> Tuple[np.ndarray, np.ndarray]:
+        """
+        Parameters
+        ----------
+        coefficients : object
+            An object containing the sparse matrix coefficients ('yvals', 'rows', 'cols'),
+            and costs associated with each rule ('costs').
+        k : float
+            A scaling factor for the coefficients.
+        ws0 : array-like, optional
+            Initial weights for the optimization process. If provided, should have the same
+            length as the number of rules. Otherwise, weights are initialized to ones.
+
+        Returns
+        -------
+        ws : numpy.ndarray
+            The optimized weights for each rule after the optimization process.
+        betas : numpy.ndarray
+            The betas values indicating constraint violations for the optimized solution.
+        """
+        ### LAZY IMPORT
+        from gurobipy import Model, GRB
+
+        a_hat = csr_matrix(
+            (
+                coefficients.yvals,
+                (coefficients.rows, coefficients.cols),
+            ),
+            dtype=np.float64,
+        ) * ((k - 1.0) / k)
+
+        if not self.use_sparse:
+            a_hat = a_hat.toarray()
+
+        costs = np.array(coefficients.costs, copy=False)
+
+        n, m = a_hat.shape
+
+        modprimal = Model("RUG Primal")
+        modprimal.setParam("OutputFlag", False)
+        # Variables
+        vs = modprimal.addMVar(shape=int(n), name="vs")
+        ws = modprimal.addMVar(shape=int(m), name="ws")
+
+        if ws0 is not None:
+            tempws = np.zeros(m)
+            tempws[: len(ws0)] = ws0
+            ws.setAttr("Start", tempws)
+            modprimal.update()
+        # Objective
+        modprimal.setObjective(
+            (
+                (np.ones(n) @ vs if sample_weight is None else sample_weight @ vs)
+                + (costs * self.penalty) @ ws
+            ),
+            GRB.MINIMIZE,
+        )
+        # Constraints
+        modprimal.addConstr(a_hat @ ws + vs >= 1.0, name="a_hat Constraints")
+
+        modprimal.optimize()
+
+        betas = np.array(modprimal.getAttr(GRB.Attr.Pi)[:n])
+
+        return ws.X, betas
```

## ruleopt/solver/ortools_solver.py

 * *Ordering differences only*

```diff
@@ -1,145 +1,145 @@
-from typing import Any
-from scipy.sparse import csr_matrix
-import numpy as np
-from ..utils import check_module_available
-from .base import OptimizationSolver
-
-
-ORTOOLS_AVAILABLE = check_module_available("ortools")
-
-
-class ORToolsSolver(OptimizationSolver):
-    """
-    A solver for linear optimization problems using the Google OR-Tools linear solver.
-
-    This solver can handle large-scale linear programming problems by interfacing with
-    various backend solvers such as CLP, GLOP, and proprietary solvers like Gurobi and CPLEX.
-    """
-
-    def __new__(cls, *args, **kwargs):
-        if not ORTOOLS_AVAILABLE:
-            raise ImportError(
-                "OR-Tools is required for this class but is not installed.",
-                "Please install it with 'pip install ortools'",
-            )
-        instance = super(ORToolsSolver, cls).__new__(cls)
-        return instance
-
-    def __init__(
-        self,
-        penalty: float = 2.0,
-        solver_type: str = "GLOP",
-        use_sparse: bool = False,
-    ) -> None:
-        """
-        Parameters
-        ----------
-        penalty : float, default=2.0
-            Penalty parameter for the cost in the objective function.
-
-        solver_type : {"CLP", "GLOP", "GUROBI_LP", "CPLEX_LP", "XPRESS_LP", "GLPK_LP", "HiGHS"}, default="GLOP"
-            The type of Linear Programming solver to use.
-
-        use_sparse : bool, default=False
-            Determines whether to use a sparse matrix representation for the optimization
-            problem. Using sparse matrices can significantly reduce memory usage and improve
-            performance for large-scale problems with many zeros in the data.
-        """
-        self.solver_type = solver_type
-        self.penalty = penalty
-        self.use_sparse = use_sparse
-        super().__init__()
-        super()._check_params()
-
-    def __call__(self, coefficients, k, sample_weight, *args, **kwargs) -> Any:
-        """
-        Solves a linear optimization problem with the given coefficients and penalty.
-
-        Parameters
-        ----------
-        coefficients : object
-            An object containing the sparse matrix coefficients ('yvals', 'rows', 'cols'),
-            and costs associated with each rule ('costs').
-        k : float
-            A scaling factor for the coefficients.
-
-        Returns
-        -------
-        ws : numpy.ndarray
-            The optimized weights for each rule after the optimization process.
-        betas : numpy.ndarray
-            The betas values indicating constraint violations for the optimized solution.
-
-        Raises
-        ------
-        ValueError
-            If the specified solver type is not supported or not linked correctly.
-        """
-        ### LAZY IMPORT
-        from ortools.linear_solver.pywraplp import Solver
-
-        a_hat = csr_matrix(
-            (
-                coefficients.yvals,
-                (coefficients.rows, coefficients.cols),
-            ),
-            dtype=np.float64,
-        ) * ((k - 1.0) / k)
-
-        if not self.use_sparse:
-            a_hat = a_hat.toarray()
-
-        n, m = a_hat.shape
-
-        costs = np.array(coefficients.costs, dtype=np.float32)
-
-        solver = Solver.CreateSolver(self.solver_type)
-
-        if solver is None:
-            raise ValueError(
-                f"Support for {self.solver_type} not linked in, or the license ",
-                "was not found.",
-            )
-
-        # Variables
-        vs = [solver.NumVar(0, solver.infinity(), f"vs[{i}]") for i in range(n)]
-        ws = [solver.NumVar(0, solver.infinity(), f"ws[{i}]") for i in range(m)]
-
-        # Objective
-        objective_terms = (
-            [vs[i] for i in range(n)]
-            if sample_weight is None
-            else [vs[i] * sample_weight[i] for i in range(n)]
-        ) + [self.penalty * costs[i] * ws[i] for i in range(m)]
-        solver.Minimize(solver.Sum(objective_terms))
-
-        # Constraints and storing them for dual value access
-        constraints = []
-        for i in range(n):
-            constraint = solver.Add(
-                solver.Sum([a_hat[i, j] * ws[j] for j in range(m)]) + vs[i] >= 1
-            )
-            constraints.append(constraint)
-
-        solver.Solve()
-
-        ws = np.array([ws[j].solution_value() for j in range(m)])
-        betas = np.array([constraint.dual_value() for constraint in constraints])
-
-        return ws, betas
-
-    def _validate_parameters(self, solver_type, penalty_parameter):
-        valid_solvers = [
-            "CLP",
-            "GLOP",
-            "GUROBI_LP",
-            "CPLEX_LP",
-            "XPRESS_LP",
-            "GLPK_LP",
-            "HiGHS",
-        ]
-        if not isinstance(solver_type, str) or solver_type not in valid_solvers:
-            raise ValueError(f"solver_type must be one of {valid_solvers}.")
-
-        if not isinstance(penalty_parameter, (float, int)) or penalty_parameter <= 0:
-            raise ValueError("penalty_parameter must be a positive float or integer.")
+from typing import Any
+from scipy.sparse import csr_matrix
+import numpy as np
+from ..utils import check_module_available
+from .base import OptimizationSolver
+
+
+ORTOOLS_AVAILABLE = check_module_available("ortools")
+
+
+class ORToolsSolver(OptimizationSolver):
+    """
+    A solver for linear optimization problems using the Google OR-Tools linear solver.
+
+    This solver can handle large-scale linear programming problems by interfacing with
+    various backend solvers such as CLP, GLOP, and proprietary solvers like Gurobi and CPLEX.
+    """
+
+    def __new__(cls, *args, **kwargs):
+        if not ORTOOLS_AVAILABLE:
+            raise ImportError(
+                "OR-Tools is required for this class but is not installed.",
+                "Please install it with 'pip install ortools'",
+            )
+        instance = super(ORToolsSolver, cls).__new__(cls)
+        return instance
+
+    def __init__(
+        self,
+        penalty: float = 2.0,
+        solver_type: str = "GLOP",
+        use_sparse: bool = False,
+    ) -> None:
+        """
+        Parameters
+        ----------
+        penalty : float, default=2.0
+            Penalty parameter for the cost in the objective function.
+
+        solver_type : {"CLP", "GLOP", "GUROBI_LP", "CPLEX_LP", "XPRESS_LP", "GLPK_LP", "HiGHS"}, default="GLOP"
+            The type of Linear Programming solver to use.
+
+        use_sparse : bool, default=False
+            Determines whether to use a sparse matrix representation for the optimization
+            problem. Using sparse matrices can significantly reduce memory usage and improve
+            performance for large-scale problems with many zeros in the data.
+        """
+        self.solver_type = solver_type
+        self.penalty = penalty
+        self.use_sparse = use_sparse
+        super().__init__()
+        super()._check_params()
+
+    def __call__(self, coefficients, k, sample_weight, *args, **kwargs) -> Any:
+        """
+        Solves a linear optimization problem with the given coefficients and penalty.
+
+        Parameters
+        ----------
+        coefficients : object
+            An object containing the sparse matrix coefficients ('yvals', 'rows', 'cols'),
+            and costs associated with each rule ('costs').
+        k : float
+            A scaling factor for the coefficients.
+
+        Returns
+        -------
+        ws : numpy.ndarray
+            The optimized weights for each rule after the optimization process.
+        betas : numpy.ndarray
+            The betas values indicating constraint violations for the optimized solution.
+
+        Raises
+        ------
+        ValueError
+            If the specified solver type is not supported or not linked correctly.
+        """
+        ### LAZY IMPORT
+        from ortools.linear_solver.pywraplp import Solver
+
+        a_hat = csr_matrix(
+            (
+                coefficients.yvals,
+                (coefficients.rows, coefficients.cols),
+            ),
+            dtype=np.float64,
+        ) * ((k - 1.0) / k)
+
+        if not self.use_sparse:
+            a_hat = a_hat.toarray()
+
+        n, m = a_hat.shape
+
+        costs = np.array(coefficients.costs, dtype=np.float32)
+
+        solver = Solver.CreateSolver(self.solver_type)
+
+        if solver is None:
+            raise ValueError(
+                f"Support for {self.solver_type} not linked in, or the license ",
+                "was not found.",
+            )
+
+        # Variables
+        vs = [solver.NumVar(0, solver.infinity(), f"vs[{i}]") for i in range(n)]
+        ws = [solver.NumVar(0, solver.infinity(), f"ws[{i}]") for i in range(m)]
+
+        # Objective
+        objective_terms = (
+            [vs[i] for i in range(n)]
+            if sample_weight is None
+            else [vs[i] * sample_weight[i] for i in range(n)]
+        ) + [self.penalty * costs[i] * ws[i] for i in range(m)]
+        solver.Minimize(solver.Sum(objective_terms))
+
+        # Constraints and storing them for dual value access
+        constraints = []
+        for i in range(n):
+            constraint = solver.Add(
+                solver.Sum([a_hat[i, j] * ws[j] for j in range(m)]) + vs[i] >= 1
+            )
+            constraints.append(constraint)
+
+        solver.Solve()
+
+        ws = np.array([ws[j].solution_value() for j in range(m)])
+        betas = np.array([constraint.dual_value() for constraint in constraints])
+
+        return ws, betas
+
+    def _validate_parameters(self, solver_type, penalty_parameter):
+        valid_solvers = [
+            "CLP",
+            "GLOP",
+            "GUROBI_LP",
+            "CPLEX_LP",
+            "XPRESS_LP",
+            "GLPK_LP",
+            "HiGHS",
+        ]
+        if not isinstance(solver_type, str) or solver_type not in valid_solvers:
+            raise ValueError(f"solver_type must be one of {valid_solvers}.")
+
+        if not isinstance(penalty_parameter, (float, int)) or penalty_parameter <= 0:
+            raise ValueError("penalty_parameter must be a positive float or integer.")
```

## ruleopt/solver/unc_solver.py

 * *Ordering differences only*

```diff
@@ -1,284 +1,284 @@
-import numpy as np
-from scipy.sparse import csr_matrix
-from ..utils import check_module_available
-from .base import OptimizationSolver
-
-TORCH_AVAILABLE = check_module_available("torch")
-
-
-class _TorchSolverManager:
-    _torch_solver_class = None
-
-    @staticmethod
-    def _initialize_solver():
-        from torch import ones, relu, sum, matmul, sub, mul, float32, dot
-        from torch.nn import Module, Parameter
-
-        class _TorchSolver(Module):
-            """
-            A solver module for using PyTorch.
-
-            This module is designed to solve optimization problems with a specific structure,
-            leveraging the PyTorch framework for gradient-based optimization.
-
-            Attributes:
-                max_rule (int): The maximum number of rules to be selected.
-                ws (torch.nn.Parameter): Weights associated with each rule.
-                gates (torch.nn.Parameter): Gate parameters to control rule selection.
-                vs (torch.nn.Parameter): Slack variables for handling constraints.
-                penalty (float): Penalty parameter for the objective function.
-                a_hat (torch.Tensor): Coefficient matrix after processing.
-                costs (torch.Tensor): Cost associated with each rule.
-            """
-
-            def __init__(
-                self, m, n, penalty, costs, k, constraint_cost, sample_weight=None
-            ):
-                """
-                Initializes the Solver with given parameters and coefficients.
-
-                Parameters:
-                    m (int): Number of rules.
-                    penalty (float): Penalty parameter for the cost in the objective function.
-                    coefficients (object): An object containing the sparse matrix coefficients ('yvals', 'rows', 'cols'),
-                                            and costs associated with each rule ('costs').
-                    k (float): A scaling factor for the coefficients.
-                    max_rule (int): The maximum number of rules to be selected.
-                """
-                super().__init__()
-
-                self.ws = Parameter(ones(m, dtype=float32, requires_grad=True) * 0.5)
-
-                self.betas = Parameter(ones(n, dtype=float32, requires_grad=True) * 0.5)
-
-                self.sample_weight = sample_weight
-                self.penalty = penalty
-                self.constraint_cost = constraint_cost
-                self.costs = costs
-                self.k = k
-                self.m = m
-
-            def forward(self, a_hat):
-                """
-                Defines the forward pass for the optimization problem.
-
-                Performs the selection of rules based on the weighted gates, calculates the
-                objective function, and applies penalties for constraint violations.
-
-                Returns:
-                    torch.Tensor: The total loss comprising the objective function and penalties for constraint violations.
-                """
-                betas = relu(self.betas)
-                ws = relu(self.ws)
-
-                dual_constraint_violation = (
-                    sum(
-                        (
-                            sub(
-                                matmul(a_hat.t(), betas.unsqueeze(-1)).squeeze(),
-                                mul(self.costs, self.penalty),
-                            )
-                        ).relu_(),
-                        dim=0,
-                    )
-                    + sum(
-                        (
-                            sub(
-                                self.betas,
-                                (
-                                    1
-                                    if self.sample_weight is None
-                                    else self.sample_weight
-                                ),
-                            )
-                        ).relu_(),
-                        dim=0,
-                    )
-                ) * self.constraint_cost
-
-                dual_objective = sum(betas, dim=0)
-
-                vs = sub(1, matmul(a_hat, ws.unsqueeze(-1)).squeeze()).relu_()
-
-                primal_objective = self.penalty * dot(self.costs, ws) + (
-                    sum(vs, dim=0)
-                    if self.sample_weight is None
-                    else dot(vs, self.sample_weight)
-                )
-
-                total_loss = (
-                    -dual_objective
-                    + primal_objective
-                    + (primal_objective - dual_objective).pow(2)
-                    + dual_constraint_violation
-                )
-
-                return total_loss
-
-        return _TorchSolver
-
-    @classmethod
-    def get_solver(cls, *args, **kwargs):
-        if cls._torch_solver_class is None:
-            cls._torch_solver_class = cls._initialize_solver()
-        return cls._torch_solver_class(*args, **kwargs)
-
-
-class UNCSolver(OptimizationSolver):
-    """
-    A gradient descent solver for optimization problems, leveraging PyTorch for gradient-based optimization.
-
-    This solver iteratively updates parameters to minimize an objective function, applying penalties
-    for constraint violations. It utilizes a gradient descent approach with early stopping based on
-    a patience parameter to prevent overfitting.
-    """
-
-    def __new__(cls, *args, **kwargs):
-        if not TORCH_AVAILABLE:
-            raise ImportError(
-                "PyTorch is required for this class but is not installed.",
-                "Please install it with 'pip install torch'",
-            )
-        instance = super(UNCSolver, cls).__new__(cls)
-        return instance
-
-    def __init__(
-        self,
-        penalty: float = 2.0,
-        constraint_cost: float = 100.0,
-        lr: float = 0.1,
-        weight_decay: float = 0.1,
-        patience: int = 100,
-        device: str = "cpu",
-        use_sparse: bool = False,
-    ) -> None:
-        """
-        Parameters
-        ----------
-        penalty : float, default=2.0
-            Penalty parameter for the cost in the objective function.
-
-        lr : float, default=0.1
-            Learning rate for the Adam optimizer.
-            
-        constraint_cost : float, default=100
-            Penalty parameter for constraint violations. 
-
-        weight_decay : float, default=0.01
-            Weight decay (L2 penalty) for the optimizer.
-
-        patience : int, default=100
-            Number of iterations to wait for an improvement before stopping the optimization.
-
-        device : {"cuda", "cpu"}, default="cpu"
-            The device on which to perform computations.
-
-        use_sparse : bool, default=False
-            Determines whether to use a sparse matrix representation for the optimization
-            problem. Using sparse matrices can significantly reduce memory usage and improve
-            performance for large-scale problems with many zeros in the data.
-        """
-        self.penalty = penalty
-        self.lr = lr
-        self.weight_decay = weight_decay
-        self.patience = patience
-        self.device = device
-        self.use_sparse = use_sparse
-        self.constraint_cost = constraint_cost
-        super().__init__()
-        super()._check_params()
-
-    def __call__(self, coefficients, k, sample_weight=None, *args, **kwargs):
-        """
-        Executes the heuristic optimization process on given problem coefficients.
-
-        Parameters
-        ----------
-        coefficients : object
-            An object containing the sparse matrix coefficients ('yvals', 'rows', 'cols'),
-            and costs associated with each rule ('costs').
-
-        k : float
-            A scaling factor for the coefficients.
-
-        Returns
-        -------
-        ws : numpy.ndarray
-            The optimized weights for each rule after the optimization process.
-        vs : numpy.ndarray
-            The vs values indicating constraint violations for the optimized solution.
-        """
-        ### LAZY IMPORT
-        from torch import tensor, sparse_csr_tensor, from_numpy, float32, clip
-        from torch.optim import Adam
-        from torch.optim.lr_scheduler import ReduceLROnPlateau
-
-        a_hat = csr_matrix(
-            (
-                coefficients.yvals,
-                (coefficients.rows, coefficients.cols),
-            ),
-            dtype=np.float64,
-        ) * ((k - 1.0) / k)
-
-        if not self.use_sparse:
-            a_hat = tensor(a_hat.toarray(), dtype=float32, device=self.device)
-
-        else:
-            a_hat = sparse_csr_tensor(
-                from_numpy(a_hat.indptr),
-                from_numpy(a_hat.indices),
-                from_numpy(a_hat.data),
-                dtype=float32,
-                device=self.device,
-            )
-
-        if sample_weight is not None:
-            sample_weight = tensor(sample_weight, dtype=float32, device=self.device)
-
-        costs = tensor(coefficients.costs, dtype=float32, device=self.device)
-
-        n, m = a_hat.shape
-
-        solver = _TorchSolverManager.get_solver(
-            m=m,
-            n=n,
-            k=k,
-            penalty=self.penalty,
-            constraint_cost=self.constraint_cost,
-            costs=costs,
-            sample_weight=sample_weight,
-        ).to(self.device)
-
-        optimizer = Adam(
-            solver.parameters(), lr=self.lr, weight_decay=self.weight_decay
-        )
-        scheduler = ReduceLROnPlateau(
-            optimizer, "min", factor=0.9, patience=self.patience // 5
-        )
-
-        best_loss = float("inf")
-        counter = 0
-
-        while True:
-            optimizer.zero_grad(set_to_none=True)
-            loss = solver(a_hat)
-            loss.backward()
-            optimizer.step()
-            
-            scheduler.step(loss)
-
-            if loss.item() < best_loss:
-                best_loss = loss.item()
-                ws = solver.ws.clone()
-                betas = solver.betas.clone()
-                counter = 0
-            else:
-                counter += 1
-                if counter > self.patience:
-                    break
-
-        return (
-            clip(ws, min=0).detach().cpu().numpy(),
-            clip(betas, min=0).detach().cpu().numpy(),
-        )
+import numpy as np
+from scipy.sparse import csr_matrix
+from ..utils import check_module_available
+from .base import OptimizationSolver
+
+TORCH_AVAILABLE = check_module_available("torch")
+
+
+class _TorchSolverManager:
+    _torch_solver_class = None
+
+    @staticmethod
+    def _initialize_solver():
+        from torch import ones, relu, sum, matmul, sub, mul, float32, dot
+        from torch.nn import Module, Parameter
+
+        class _TorchSolver(Module):
+            """
+            A solver module for using PyTorch.
+
+            This module is designed to solve optimization problems with a specific structure,
+            leveraging the PyTorch framework for gradient-based optimization.
+
+            Attributes:
+                max_rule (int): The maximum number of rules to be selected.
+                ws (torch.nn.Parameter): Weights associated with each rule.
+                gates (torch.nn.Parameter): Gate parameters to control rule selection.
+                vs (torch.nn.Parameter): Slack variables for handling constraints.
+                penalty (float): Penalty parameter for the objective function.
+                a_hat (torch.Tensor): Coefficient matrix after processing.
+                costs (torch.Tensor): Cost associated with each rule.
+            """
+
+            def __init__(
+                self, m, n, penalty, costs, k, constraint_cost, sample_weight=None
+            ):
+                """
+                Initializes the Solver with given parameters and coefficients.
+
+                Parameters:
+                    m (int): Number of rules.
+                    penalty (float): Penalty parameter for the cost in the objective function.
+                    coefficients (object): An object containing the sparse matrix coefficients ('yvals', 'rows', 'cols'),
+                                            and costs associated with each rule ('costs').
+                    k (float): A scaling factor for the coefficients.
+                    max_rule (int): The maximum number of rules to be selected.
+                """
+                super().__init__()
+
+                self.ws = Parameter(ones(m, dtype=float32, requires_grad=True) * 0.5)
+
+                self.betas = Parameter(ones(n, dtype=float32, requires_grad=True) * 0.5)
+
+                self.sample_weight = sample_weight
+                self.penalty = penalty
+                self.constraint_cost = constraint_cost
+                self.costs = costs
+                self.k = k
+                self.m = m
+
+            def forward(self, a_hat):
+                """
+                Defines the forward pass for the optimization problem.
+
+                Performs the selection of rules based on the weighted gates, calculates the
+                objective function, and applies penalties for constraint violations.
+
+                Returns:
+                    torch.Tensor: The total loss comprising the objective function and penalties for constraint violations.
+                """
+                betas = relu(self.betas)
+                ws = relu(self.ws)
+
+                dual_constraint_violation = (
+                    sum(
+                        (
+                            sub(
+                                matmul(a_hat.t(), betas.unsqueeze(-1)).squeeze(),
+                                mul(self.costs, self.penalty),
+                            )
+                        ).relu_(),
+                        dim=0,
+                    )
+                    + sum(
+                        (
+                            sub(
+                                self.betas,
+                                (
+                                    1
+                                    if self.sample_weight is None
+                                    else self.sample_weight
+                                ),
+                            )
+                        ).relu_(),
+                        dim=0,
+                    )
+                ) * self.constraint_cost
+
+                dual_objective = sum(betas, dim=0)
+
+                vs = sub(1, matmul(a_hat, ws.unsqueeze(-1)).squeeze()).relu_()
+
+                primal_objective = self.penalty * dot(self.costs, ws) + (
+                    sum(vs, dim=0)
+                    if self.sample_weight is None
+                    else dot(vs, self.sample_weight)
+                )
+
+                total_loss = (
+                    -dual_objective
+                    + primal_objective
+                    + (primal_objective - dual_objective).pow(2)
+                    + dual_constraint_violation
+                )
+
+                return total_loss
+
+        return _TorchSolver
+
+    @classmethod
+    def get_solver(cls, *args, **kwargs):
+        if cls._torch_solver_class is None:
+            cls._torch_solver_class = cls._initialize_solver()
+        return cls._torch_solver_class(*args, **kwargs)
+
+
+class UNCSolver(OptimizationSolver):
+    """
+    A gradient descent solver for optimization problems, leveraging PyTorch for gradient-based optimization.
+
+    This solver iteratively updates parameters to minimize an objective function, applying penalties
+    for constraint violations. It utilizes a gradient descent approach with early stopping based on
+    a patience parameter to prevent overfitting.
+    """
+
+    def __new__(cls, *args, **kwargs):
+        if not TORCH_AVAILABLE:
+            raise ImportError(
+                "PyTorch is required for this class but is not installed.",
+                "Please install it with 'pip install torch'",
+            )
+        instance = super(UNCSolver, cls).__new__(cls)
+        return instance
+
+    def __init__(
+        self,
+        penalty: float = 2.0,
+        constraint_cost: float = 100.0,
+        lr: float = 0.1,
+        weight_decay: float = 0.1,
+        patience: int = 100,
+        device: str = "cpu",
+        use_sparse: bool = False,
+    ) -> None:
+        """
+        Parameters
+        ----------
+        penalty : float, default=2.0
+            Penalty parameter for the cost in the objective function.
+
+        lr : float, default=0.1
+            Learning rate for the Adam optimizer.
+            
+        constraint_cost : float, default=100
+            Penalty parameter for constraint violations. 
+
+        weight_decay : float, default=0.01
+            Weight decay (L2 penalty) for the optimizer.
+
+        patience : int, default=100
+            Number of iterations to wait for an improvement before stopping the optimization.
+
+        device : {"cuda", "cpu"}, default="cpu"
+            The device on which to perform computations.
+
+        use_sparse : bool, default=False
+            Determines whether to use a sparse matrix representation for the optimization
+            problem. Using sparse matrices can significantly reduce memory usage and improve
+            performance for large-scale problems with many zeros in the data.
+        """
+        self.penalty = penalty
+        self.lr = lr
+        self.weight_decay = weight_decay
+        self.patience = patience
+        self.device = device
+        self.use_sparse = use_sparse
+        self.constraint_cost = constraint_cost
+        super().__init__()
+        super()._check_params()
+
+    def __call__(self, coefficients, k, sample_weight=None, *args, **kwargs):
+        """
+        Executes the heuristic optimization process on given problem coefficients.
+
+        Parameters
+        ----------
+        coefficients : object
+            An object containing the sparse matrix coefficients ('yvals', 'rows', 'cols'),
+            and costs associated with each rule ('costs').
+
+        k : float
+            A scaling factor for the coefficients.
+
+        Returns
+        -------
+        ws : numpy.ndarray
+            The optimized weights for each rule after the optimization process.
+        vs : numpy.ndarray
+            The vs values indicating constraint violations for the optimized solution.
+        """
+        ### LAZY IMPORT
+        from torch import tensor, sparse_csr_tensor, from_numpy, float32, clip
+        from torch.optim import Adam
+        from torch.optim.lr_scheduler import ReduceLROnPlateau
+
+        a_hat = csr_matrix(
+            (
+                coefficients.yvals,
+                (coefficients.rows, coefficients.cols),
+            ),
+            dtype=np.float64,
+        ) * ((k - 1.0) / k)
+
+        if not self.use_sparse:
+            a_hat = tensor(a_hat.toarray(), dtype=float32, device=self.device)
+
+        else:
+            a_hat = sparse_csr_tensor(
+                from_numpy(a_hat.indptr),
+                from_numpy(a_hat.indices),
+                from_numpy(a_hat.data),
+                dtype=float32,
+                device=self.device,
+            )
+
+        if sample_weight is not None:
+            sample_weight = tensor(sample_weight, dtype=float32, device=self.device)
+
+        costs = tensor(coefficients.costs, dtype=float32, device=self.device)
+
+        n, m = a_hat.shape
+
+        solver = _TorchSolverManager.get_solver(
+            m=m,
+            n=n,
+            k=k,
+            penalty=self.penalty,
+            constraint_cost=self.constraint_cost,
+            costs=costs,
+            sample_weight=sample_weight,
+        ).to(self.device)
+
+        optimizer = Adam(
+            solver.parameters(), lr=self.lr, weight_decay=self.weight_decay
+        )
+        scheduler = ReduceLROnPlateau(
+            optimizer, "min", factor=0.9, patience=self.patience // 5
+        )
+
+        best_loss = float("inf")
+        counter = 0
+
+        while True:
+            optimizer.zero_grad(set_to_none=True)
+            loss = solver(a_hat)
+            loss.backward()
+            optimizer.step()
+            
+            scheduler.step(loss)
+
+            if loss.item() < best_loss:
+                best_loss = loss.item()
+                ws = solver.ws.clone()
+                betas = solver.betas.clone()
+                counter = 0
+            else:
+                counter += 1
+                if counter > self.patience:
+                    break
+
+        return (
+            clip(ws, min=0).detach().cpu().numpy(),
+            clip(betas, min=0).detach().cpu().numpy(),
+        )
```

## ruleopt/utils/__init__.py

 * *Ordering differences only*

```diff
@@ -1,5 +1,5 @@
-from .utils import (check_inputs, check_module_available)
-
-__all__ = [
-    "check_inputs",
+from .utils import (check_inputs, check_module_available)
+
+__all__ = [
+    "check_inputs",
     "check_module_available"]
```

## ruleopt/utils/utils.py

 * *Ordering differences only*

```diff
@@ -1,34 +1,34 @@
-from typing import Tuple
-from sklearn.utils import check_array
-import numpy as np
-import importlib.util
-    
-def check_inputs(x, y = None) -> Tuple[np.ndarray, np.ndarray]:
-    """
-    Validates and preprocesses input data for the model.
-
-    Parameters
-    ----------
-    x : array-like of shape (n_samples, n_features)
-        The training input samples. Internally, it will be converted to dtype=np.float32.
-    y : array-like of shape (n_samples,) or (n_samples, n_outputs)
-        The target values (class labels) as integers
-
-    Returns
-    -------
-    Tuple[np.ndarray, np.ndarray]
-        The validated and preprocessed input matrix `x` and target vector `y`.
-    """
-    x = check_array(np.asarray(x, dtype=np.float32), force_all_finite="allow-nan")
-    if y is not None:
-        y = check_array(np.asarray(y, dtype=np.intp), ensure_2d=False)
-        return x, y
-    return x
-
-
-def check_module_available(module_name):
-    """
-    Checks module is installed.
-    """
-    spec = importlib.util.find_spec(module_name)
+from typing import Tuple
+from sklearn.utils import check_array
+import numpy as np
+import importlib.util
+    
+def check_inputs(x, y = None) -> Tuple[np.ndarray, np.ndarray]:
+    """
+    Validates and preprocesses input data for the model.
+
+    Parameters
+    ----------
+    x : array-like of shape (n_samples, n_features)
+        The training input samples. Internally, it will be converted to dtype=np.float32.
+    y : array-like of shape (n_samples,) or (n_samples, n_outputs)
+        The target values (class labels) as integers
+
+    Returns
+    -------
+    Tuple[np.ndarray, np.ndarray]
+        The validated and preprocessed input matrix `x` and target vector `y`.
+    """
+    x = check_array(np.asarray(x, dtype=np.float32), force_all_finite="allow-nan")
+    if y is not None:
+        y = check_array(np.asarray(y, dtype=np.intp), ensure_2d=False)
+        return x, y
+    return x
+
+
+def check_module_available(module_name):
+    """
+    Checks module is installed.
+    """
+    spec = importlib.util.find_spec(module_name)
     return spec is not None
```

## Comparing `ruleopt-0.1.4.dist-info/LICENSE` & `ruleopt-0.1.5.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-MIT License
-
-Copyright (c) 2024
-
-Ilker Birbil, Nursen Aydin, Ozgür Martin, Samet Copur
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-SOFTWARE.
+MIT License
+
+Copyright (c) 2024
+
+Ilker Birbil, Nursen Aydin, Ozgür Martin, Samet Copur
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
```

## Comparing `ruleopt-0.1.4.dist-info/METADATA` & `ruleopt-0.1.5.dist-info/METADATA`

 * *Files 16% similar despite different names*

```diff
@@ -1,93 +1,93 @@
-Metadata-Version: 2.1
-Name: ruleopt
-Version: 0.1.4
-Summary: Optimization Based Rule Learning for Classification
-Author-email: Ilker Birbil <sibirbil@gmail.com>, Nursen Aydin <nursenaydin@gmail.com>, Ozgür Martin <ozgurmartin@gmail.com>, Samet Copur <sametcopur@yahoo.com>
-License: MIT License
-Project-URL: Documentation, https://ruleopt.readthedocs.io/
-Project-URL: Repository, https://github.com/sametcopur/ruleopt
-Project-URL: Tracker, https://github.com/sametcopur/ruleopt/issues
-Keywords: python,data-science,machine-learning,linear-programming,machine-learning-library,explainable-ai
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Operating System :: OS Independent
-Requires-Python: >=3.9
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: scikit-learn >=1.4.1
-Requires-Dist: numpy >=1.26.4
-Requires-Dist: pandas >=2.2.1
-Requires-Dist: scipy >=1.11.4
-
-# RuleOpt: Optimization-Based Rule Learning for Classification
-
-RuleOpt is a rule-based machine learning algorithm designed for classification problems. Focusing on scalability and interpretability, RuleOpt utilizes optimization algorithms for rule generation and extraction. An earlier version of this work is available in [our manuscript](https://arxiv.org/abs/2104.10751).
-
- The Python library `ruleopt` is capable to extract rules from ensemble models, and it also implements a novel rule generation scheme. The library ensures compatibility with existing machine learning pipelines, and it is especially efficient for tackling large-scale problems.
-
-Here are a few highlights of `ruleopt`:
-
-- **Efficient Rule Generation and Extraction**: Leverages linear programming and gradient descent for scalable rule generation (standalone machine learning method) and rule extraction from trained random forest and boosting models.
-- **Interpretability**: Prioritizes model transparency by assigning costs to rules in order to achieve a desirable balance with accuracy.
-- **Integration with Machine Learning Libraries**: Facilitates smooth integration with well-known Python libraries `scikit-learn`, `LightGBM`, and `XGBoost`, and existing machine learning pipelines.
-- **Extensive Solver Support**: Supports a wide array of solvers, including _Gurobi_, _CPLEX_, _GLPK_, and a `PyTorch`-based _UNCSolver_ designed to scale the algorithm.
-
-### Installation 
-To install `ruleopt`, clone this repository and use pip to install the package:
-
-```bash
-pip install ruleopt
-```
-### Usage
-
-To use `ruleopt`, you need to initialize the `ruleopt` class with your specific parameters and fit it to your data. Here's a basic example:
-
-
-```python
-from sklearn.model_selection import train_test_split
-from sklearn.datasets import load_iris
-
-from ruleopt import RUGClassifier
-from ruleopt.rule_cost import Gini
-from ruleopt.solver import UNCSolver
-
-# Set a random state for reproducibility
-random_state = 42
-
-# Load the Iris dataset
-X, y = load_iris(return_X_y=True)
-
-# Split the dataset into training and testing sets
-X_train, X_test, y_train, y_test = train_test_split(
-    X, y, test_size=0.2, random_state=random_state
-)
-
-# Define tree parameters
-tree_parameters = {"max_depth": 3, "class_weight": "balanced"}
-
-solver = UNCSolver()
-rule_cost = Gini()
-
-# Initialize the RUGClassifier with specific parameters
-rug = RUGClassifier(
-    solver=solver,
-    random_state=random_state,
-    max_rmp_calls=20,
-    rule_cost=rule_cost,
-    **tree_parameters,
-)
-
-# Fit the RUGClassifier to the training data
-rug.fit(X_train, y_train)
-
-# Predict the labels of the testing set
-y_pred = rug.predict(X_test)
-```
-### Documentation
-For more detailed information about the API and advanced usage, please refer to the full  [documentation](https://ruleopt.readthedocs.io/en/latest/).
-
-### Contributing
-Contributions are welcome! If you'd like to improve `ruleopt` or suggest new features, feel free to fork the repository and submit a pull request.
-
-### License
-`ruleopt` is released under the MIT License. See the LICENSE file for more details.
+Metadata-Version: 2.1
+Name: ruleopt
+Version: 0.1.5
+Summary: Optimization Based Rule Learning for Classification
+Author-email: Ilker Birbil <sibirbil@gmail.com>, Nursen Aydin <nursenaydin@gmail.com>, Ozgür Martin <ozgurmartin@gmail.com>, Samet Copur <sametcopur@yahoo.com>
+License: MIT License
+Project-URL: Documentation, https://ruleopt.readthedocs.io/
+Project-URL: Repository, https://github.com/sametcopur/ruleopt
+Project-URL: Tracker, https://github.com/sametcopur/ruleopt/issues
+Keywords: python,data-science,machine-learning,linear-programming,machine-learning-library,explainable-ai
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: OS Independent
+Requires-Python: >=3.9
+Description-Content-Type: text/markdown
+License-File: LICENSE
+Requires-Dist: scikit-learn >=1.4.1
+Requires-Dist: numpy >=1.26.4
+Requires-Dist: pandas >=2.2.1
+Requires-Dist: scipy >=1.11.4
+
+# RuleOpt: Optimization-Based Rule Learning for Classification
+
+RuleOpt is a rule-based machine learning algorithm designed for classification problems. Focusing on scalability and interpretability, RuleOpt utilizes optimization algorithms for rule generation and extraction. An earlier version of this work is available in [our manuscript](https://arxiv.org/abs/2104.10751).
+
+ The Python library `ruleopt` is capable to extract rules from ensemble models, and it also implements a novel rule generation scheme. The library ensures compatibility with existing machine learning pipelines, and it is especially efficient for tackling large-scale problems.
+
+Here are a few highlights of `ruleopt`:
+
+- **Efficient Rule Generation and Extraction**: Leverages linear programming and gradient descent for scalable rule generation (standalone machine learning method) and rule extraction from trained random forest and boosting models.
+- **Interpretability**: Prioritizes model transparency by assigning costs to rules in order to achieve a desirable balance with accuracy.
+- **Integration with Machine Learning Libraries**: Facilitates smooth integration with well-known Python libraries `scikit-learn`, `LightGBM`, and `XGBoost`, and existing machine learning pipelines.
+- **Extensive Solver Support**: Supports a wide array of solvers, including _Gurobi_, _CPLEX_, _GLPK_, and a `PyTorch`-based _UNCSolver_ designed to scale the algorithm.
+
+### Installation 
+To install `ruleopt`, clone this repository and use pip to install the package:
+
+```bash
+pip install ruleopt
+```
+### Usage
+
+To use `ruleopt`, you need to initialize the `ruleopt` class with your specific parameters and fit it to your data. Here's a basic example:
+
+
+```python
+from sklearn.model_selection import train_test_split
+from sklearn.datasets import load_iris
+
+from ruleopt import RUGClassifier
+from ruleopt.rule_cost import Gini
+from ruleopt.solver import UNCSolver
+
+# Set a random state for reproducibility
+random_state = 42
+
+# Load the Iris dataset
+X, y = load_iris(return_X_y=True)
+
+# Split the dataset into training and testing sets
+X_train, X_test, y_train, y_test = train_test_split(
+    X, y, test_size=0.2, random_state=random_state
+)
+
+# Define tree parameters
+tree_parameters = {"max_depth": 3, "class_weight": "balanced"}
+
+solver = UNCSolver()
+rule_cost = Gini()
+
+# Initialize the RUGClassifier with specific parameters
+rug = RUGClassifier(
+    solver=solver,
+    random_state=random_state,
+    max_rmp_calls=20,
+    rule_cost=rule_cost,
+    **tree_parameters,
+)
+
+# Fit the RUGClassifier to the training data
+rug.fit(X_train, y_train)
+
+# Predict the labels of the testing set
+y_pred = rug.predict(X_test)
+```
+### Documentation
+For more detailed information about the API and advanced usage, please refer to the full  [documentation](https://ruleopt.readthedocs.io/en/latest/).
+
+### Contributing
+Contributions are welcome! If you'd like to improve `ruleopt` or suggest new features, feel free to fork the repository and submit a pull request.
+
+### License
+`ruleopt` is released under the MIT License. See the LICENSE file for more details.
```

## Comparing `ruleopt-0.1.4.dist-info/RECORD` & `ruleopt-0.1.5.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-ruleopt/__init__.py,sha256=4IfJMKA1Ej57JEvfEktQXc0zMXgkqpmWSMDKOe7cfyE,257
-ruleopt/aux_classes/__init__.py,sha256=UGkt4gSK-CxEorj-BOxOVGu36HvPbUfYN5OeZSy2vp4,96
-ruleopt/aux_classes/aux_classes.c,sha256=g-aQY0rXgGl5Lit03-6Vwld8b72V_vhNEmx6WMsRaKI,1401393
-ruleopt/aux_classes/aux_classes.cp39-win_amd64.pyd,sha256=xDkYR61AoosifivpTmo1SwAt3Hb5cOTAi6d6nM57_aE,211968
-ruleopt/estimator/__init__.py,sha256=8SIFO2ba0bwOdMUxQjdnwHlbsj_OhnZ0cGipMq8XMV0,280
-ruleopt/estimator/base.py,sha256=X7YYnTJgf9EUpG0YfH4u4i20-Zh_lovzjtpzXXOELOs,26086
-ruleopt/estimator/lightgbm_.py,sha256=xlF0ylat2NgKg3QL4488YFrfALTTgWDGDBCxzlUewpc,12714
-ruleopt/estimator/xgboost_.py,sha256=9L9ormW460jLWcr37P8R33_TGifPLQcCMsVExGk3NbM,11558
-ruleopt/estimator/sklearn_/__init__.py,sha256=vpPehVjHJuFZgWiq2vuI6WADan_84Vqnw2CcmAYXYQQ,126
-ruleopt/estimator/sklearn_/base_sklearn.py,sha256=Mjd0Y6GMSgokHiD_pPFCQcpBuCt4iifSIT3y3TImyeg,7001
-ruleopt/estimator/sklearn_/rug.py,sha256=n_K7XPXxM2b7hxEoXBT09L0dw8Nr9esX3r2VFn1xnHU,18249
-ruleopt/estimator/sklearn_/rux.py,sha256=H4bmlGaX_3GkDyM1voYlAy_o1AkJwvMhVVlSMR9Sc78,5662
-ruleopt/explainer/__init__.py,sha256=vYoSUVEeHYFWLjrwXyBSVdu7lhU9nK1KWGyEwidMgvQ,68
-ruleopt/explainer/explainer.py,sha256=U8Ql-GB2QEfz_WRAnQUek1YiYf4Jui-6YvfKuVju5uY,9014
-ruleopt/rule_cost/__init__.py,sha256=RyyBoe-0dHNHd_LMMYnEiCotnqQAJgRPhI9PHizvN7g,137
-ruleopt/rule_cost/rule_cost.py,sha256=oRMArJEWRVOyyAY2olNV3NLZ2uBgAQoiywtgOu7Ckgs,5252
-ruleopt/solver/__init__.py,sha256=2Ggey48NybpjTnmsXk11S3UHdgplsn8PojuOYatw3eY,293
-ruleopt/solver/base.py,sha256=9ihPetgGh7YCGiQNL2Oz44i0xuAQT8H-WhDCqQZtdSY,3208
-ruleopt/solver/cplex_solver.py,sha256=9pfy5IHJnq66d6oiT9hv7hhyX8ER0hgCPiDs2nofQNI,4086
-ruleopt/solver/gurobi_solver.py,sha256=BZ_9jQrwzEbNqSCitv7Q9gL2kmoTzCfPo2kqXpQRrow,3875
-ruleopt/solver/ortools_solver.py,sha256=_NfIqiys2WMNBPCZ1ENuagESmlSn3MI_L1hF8ytAFkw,5120
-ruleopt/solver/unc_solver.py,sha256=L1-NEXbJ7DLQsr0qnFp3Mh4AGYYMeG_TYfliJuCcLS8,10425
-ruleopt/utils/__init__.py,sha256=Nu-b2q1FCwPPlzMCKOHS1GHDpDXtEqXsywvd4W21m2M,124
-ruleopt/utils/utils.py,sha256=h5QypOLcP01jNq3te8p7TZmQ1QEwpbcLQ_GBjKGf83s,1086
-ruleopt-0.1.4.dist-info/LICENSE,sha256=92AcbQTTnaW3MHsF7V5rVL_3m2vpSdZLi5uzGGid4-o,1135
-ruleopt-0.1.4.dist-info/METADATA,sha256=f11MiUTz4MrA_VhcRz9ejTo_cVIxR5T7qBhooR8Gof8,4153
-ruleopt-0.1.4.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
-ruleopt-0.1.4.dist-info/top_level.txt,sha256=5eXJDNaihq5rZdf1uSl9UMqGI050yX27S4inqYQMGSA,8
-ruleopt-0.1.4.dist-info/RECORD,,
+ruleopt/__init__.py,sha256=o8AuVrs9jVmDhaQwkVxxw5LJ90cNfXetouaQ2ELc9iw,247
+ruleopt/estimator/__init__.py,sha256=84Fg0ed4aOcTTKXOKx9HGjfrXAud-PuSurZieMmbNCw,271
+ruleopt/estimator/lightgbm_.py,sha256=t_KyfpsROrN1y-HcseU9SU17Tt51oKc_giu7xvL0rgU,12388
+ruleopt/estimator/xgboost_.py,sha256=k-lPwbpCaEMZJzzROUR_Ny4q1m0Nyl5K3M8VzXL1qGk,11253
+ruleopt/estimator/base.py,sha256=VDN9m9QTUk--I-m5Hb4_cFN97MSyedqgCPZhcsS4J4E,23998
+ruleopt/estimator/sklearn_/base_sklearn.py,sha256=x2CybLDjJra2HzpH0nWzM5aJUeok2dpqyh1eZqABtB8,6805
+ruleopt/estimator/sklearn_/rux.py,sha256=Vwy8GWBgzi769fBBQhXRffoylVwa3n3zkQQr6lcbxEo,5514
+ruleopt/estimator/sklearn_/__init__.py,sha256=3hCiryClpNFIMGTkiTotqQoPjMk8A81o1dV3sjuA2Io,119
+ruleopt/estimator/sklearn_/rug.py,sha256=-Kogh0TixeUoEkD-bL5F1JbkI8lFjhoc9ScFMncrWZY,17777
+ruleopt/utils/__init__.py,sha256=fH5j0D0cpNx1APzpF3DLHrjfoD_B3xm6bWFm--_vJGM,120
+ruleopt/utils/utils.py,sha256=i27-ZnwKw9fXlCoUOeIwdf6c1NGzoRGLjbDEb1LtzM8,1053
+ruleopt/solver/unc_solver.py,sha256=jRjz-UATMzx8ZKSLoMUbKsgIL1lGY2aGKkhLAFg_pC8,10141
+ruleopt/solver/ortools_solver.py,sha256=tuLKRjXkWfbYaG8NSy3Tf4kJqhAAqso6wv6rfW3U3lE,4975
+ruleopt/solver/__init__.py,sha256=vDPoPasoLN8mrMAK9XqxA8o2hhujz17AyOBjSBLcLEk,286
+ruleopt/solver/gurobi_solver.py,sha256=8goOKnpwfdgpbhPsbYWfjxTjuzYhQq8KQJv9RBVNuIg,3756
+ruleopt/solver/cplex_solver.py,sha256=sosqL-EErtb8n67enF4qF1G-vV_NljbT4WckLgsIBU4,3957
+ruleopt/solver/base.py,sha256=z2i0yB9rGlx_UXz9JbGpDB3cgHUySA7xwBcKfFI7LlY,3126
+ruleopt/rule_cost/rule_cost.py,sha256=z8hTXNgWCDLfsfTY43Totuh06rab9z7gjpvsRvGUdLY,5079
+ruleopt/rule_cost/__init__.py,sha256=umUIFMfNXmHTCdj819rVeO-daVXGGXQSpJAGAZhBVJo,134
+ruleopt/explainer/explainer.py,sha256=UilDlCWeHBREwZR25-cRtpwKXlF-7rcK8YvFkCcGZH4,8771
+ruleopt/explainer/__init__.py,sha256=Qj8LYBiKERpljG1l85lDYaSTZCn9ZRCu3cAadAr2fF4,65
+ruleopt/aux_classes/__init__.py,sha256=N-EmTWYG51YSOZJ4Lw7fq2hJNyzBp9eirffgV7CBRUw,91
+ruleopt/aux_classes/aux_classes.cpython-39-darwin.so,sha256=OgN_nySlobYZ25-ifVFSmc1AOW-CS2QBph6Mnuxi3oM,266544
+ruleopt/aux_classes/aux_classes.c,sha256=Y7WyLGuoU88Ko6yBYn5lzKD_OlcvFiQP8BXdy_Q4mSQ,1424395
+ruleopt-0.1.5.dist-info/RECORD,,
+ruleopt-0.1.5.dist-info/LICENSE,sha256=xYWPUH6qfAqTDo8XQnQIi0n2OKjJkXO6cS7B1qwABVc,1112
+ruleopt-0.1.5.dist-info/WHEEL,sha256=t3aNIuHimB-eyeerOmc50nLML0b4_R6yjydcdcJkGHg,108
+ruleopt-0.1.5.dist-info/top_level.txt,sha256=5eXJDNaihq5rZdf1uSl9UMqGI050yX27S4inqYQMGSA,8
+ruleopt-0.1.5.dist-info/METADATA,sha256=qEjvme5Kq2Tq6-gY2gBlL6daqVxfwlhUllvXi5toMQw,4060
```

