# Comparing `tmp/olive_ai-0.5.0-py3-none-any.whl.zip` & `tmp/olive_ai-0.5.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,240 +1,242 @@
-Zip file size: 489083 bytes, number of entries: 238
--rw-rw-r--  2.0 unx      608 b- defN 24-Mar-07 22:29 olive/__init__.py
--rw-rw-r--  2.0 unx     9889 b- defN 24-Mar-07 22:29 olive/cache.py
--rw-rw-r--  2.0 unx     1113 b- defN 24-Mar-07 22:29 olive/constants.py
--rw-rw-r--  2.0 unx      797 b- defN 24-Mar-07 22:29 olive/extra_dependencies.json
--rw-rw-r--  2.0 unx     2626 b- defN 24-Mar-07 22:29 olive/logging.py
--rw-rw-r--  2.0 unx    22594 b- defN 24-Mar-07 22:29 olive/resource_path.py
--rw-rw-r--  2.0 unx     5181 b- defN 24-Mar-07 22:29 olive/auto_optimizer/__init__.py
--rw-rw-r--  2.0 unx     5663 b- defN 24-Mar-07 22:29 olive/auto_optimizer/regulate_mixins.py
--rw-rw-r--  2.0 unx     3615 b- defN 24-Mar-07 22:29 olive/auto_optimizer/template_mapping.py
--rw-rw-r--  2.0 unx     1171 b- defN 24-Mar-07 22:29 olive/auto_optimizer/config_template/opt_level_passes.yaml
--rw-rw-r--  2.0 unx     2062 b- defN 24-Mar-07 22:29 olive/auto_optimizer/config_template/pass_capability.yaml
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/azureml/__init__.py
--rw-rw-r--  2.0 unx     6412 b- defN 24-Mar-07 22:29 olive/azureml/azureml_client.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/common/__init__.py
--rw-rw-r--  2.0 unx     3441 b- defN 24-Mar-07 22:29 olive/common/auto_config.py
--rw-rw-r--  2.0 unx    10777 b- defN 24-Mar-07 22:29 olive/common/config_utils.py
--rw-rw-r--  2.0 unx     1488 b- defN 24-Mar-07 22:29 olive/common/import_lib.py
--rw-rw-r--  2.0 unx    16029 b- defN 24-Mar-07 22:29 olive/common/ort_inference.py
--rw-rw-r--  2.0 unx      765 b- defN 24-Mar-07 22:29 olive/common/pydantic_v1.py
--rw-rw-r--  2.0 unx     1728 b- defN 24-Mar-07 22:29 olive/common/user_module_loader.py
--rw-rw-r--  2.0 unx    11765 b- defN 24-Mar-07 22:29 olive/common/utils.py
--rw-rw-r--  2.0 unx      345 b- defN 24-Mar-07 22:29 olive/data/__init__.py
--rw-rw-r--  2.0 unx    10680 b- defN 24-Mar-07 22:29 olive/data/config.py
--rw-rw-r--  2.0 unx     1443 b- defN 24-Mar-07 22:29 olive/data/constants.py
--rw-rw-r--  2.0 unx     8119 b- defN 24-Mar-07 22:29 olive/data/registry.py
--rw-rw-r--  2.0 unx     3573 b- defN 24-Mar-07 22:29 olive/data/template.py
--rw-rw-r--  2.0 unx      444 b- defN 24-Mar-07 22:29 olive/data/component/__init__.py
--rw-rw-r--  2.0 unx     1955 b- defN 24-Mar-07 22:29 olive/data/component/dataloader.py
--rw-rw-r--  2.0 unx    10679 b- defN 24-Mar-07 22:29 olive/data/component/dataset.py
--rw-rw-r--  2.0 unx     1928 b- defN 24-Mar-07 22:29 olive/data/component/load_dataset.py
--rw-rw-r--  2.0 unx     2256 b- defN 24-Mar-07 22:29 olive/data/component/post_process_data.py
--rw-rw-r--  2.0 unx    11446 b- defN 24-Mar-07 22:29 olive/data/component/pre_process_data.py
--rw-rw-r--  2.0 unx    29278 b- defN 24-Mar-07 22:29 olive/data/component/text_generation.py
--rw-rw-r--  2.0 unx      480 b- defN 24-Mar-07 22:29 olive/data/container/__init__.py
--rw-rw-r--  2.0 unx     3385 b- defN 24-Mar-07 22:29 olive/data/container/data_container.py
--rw-rw-r--  2.0 unx     1159 b- defN 24-Mar-07 22:29 olive/data/container/dummy_data_container.py
--rw-rw-r--  2.0 unx     1690 b- defN 24-Mar-07 22:29 olive/data/container/huggingface_container.py
--rw-rw-r--  2.0 unx     1721 b- defN 24-Mar-07 22:29 olive/data/container/raw_data_container.py
--rw-rw-r--  2.0 unx      411 b- defN 24-Mar-07 22:29 olive/engine/__init__.py
--rw-rw-r--  2.0 unx     1432 b- defN 24-Mar-07 22:29 olive/engine/config.py
--rw-rw-r--  2.0 unx    45918 b- defN 24-Mar-07 22:29 olive/engine/engine.py
--rw-rw-r--  2.0 unx    17410 b- defN 24-Mar-07 22:29 olive/engine/footprint.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/engine/packaging/__init__.py
--rw-rw-r--  2.0 unx      622 b- defN 24-Mar-07 22:29 olive/engine/packaging/packaging_config.py
--rw-rw-r--  2.0 unx    15871 b- defN 24-Mar-07 22:29 olive/engine/packaging/packaging_generator.py
--rw-rw-r--  2.0 unx     1588 b- defN 24-Mar-07 22:29 olive/engine/packaging/sample_code/ONNXModel/cpp/README.md
--rw-rw-r--  2.0 unx     4990 b- defN 24-Mar-07 22:29 olive/engine/packaging/sample_code/ONNXModel/cpp/code_sample.cpp
--rw-rw-r--  2.0 unx     1590 b- defN 24-Mar-07 22:29 olive/engine/packaging/sample_code/ONNXModel/cs/README.md
--rw-rw-r--  2.0 unx     4991 b- defN 24-Mar-07 22:29 olive/engine/packaging/sample_code/ONNXModel/cs/code_sample.cs
--rw-rw-r--  2.0 unx     2296 b- defN 24-Mar-07 22:29 olive/engine/packaging/sample_code/ONNXModel/python/README.md
--rw-rw-r--  2.0 unx     2545 b- defN 24-Mar-07 22:29 olive/engine/packaging/sample_code/ONNXModel/python/code_sample.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/evaluator/__init__.py
--rw-rw-r--  2.0 unx     6473 b- defN 24-Mar-07 22:29 olive/evaluator/accuracy.py
--rw-rw-r--  2.0 unx     8747 b- defN 24-Mar-07 22:29 olive/evaluator/metric.py
--rw-rw-r--  2.0 unx     4457 b- defN 24-Mar-07 22:29 olive/evaluator/metric_backend.py
--rw-rw-r--  2.0 unx     4395 b- defN 24-Mar-07 22:29 olive/evaluator/metric_config.py
--rw-rw-r--  2.0 unx    47429 b- defN 24-Mar-07 22:29 olive/evaluator/olive_evaluator.py
--rw-rw-r--  2.0 unx      592 b- defN 24-Mar-07 22:29 olive/exception/__init__.py
--rw-rw-r--  2.0 unx      621 b- defN 24-Mar-07 22:29 olive/hardware/__init__.py
--rw-rw-r--  2.0 unx     9413 b- defN 24-Mar-07 22:29 olive/hardware/accelerator.py
--rw-rw-r--  2.0 unx     1353 b- defN 24-Mar-07 22:29 olive/hardware/constants.py
--rw-rw-r--  2.0 unx      451 b- defN 24-Mar-07 22:29 olive/model/__init__.py
--rw-rw-r--  2.0 unx      482 b- defN 24-Mar-07 22:29 olive/model/config/__init__.py
--rw-rw-r--  2.0 unx    10899 b- defN 24-Mar-07 22:29 olive/model/config/hf_config.py
--rw-rw-r--  2.0 unx     4843 b- defN 24-Mar-07 22:29 olive/model/config/io_config.py
--rw-rw-r--  2.0 unx     3943 b- defN 24-Mar-07 22:29 olive/model/config/model_config.py
--rw-rw-r--  2.0 unx     1045 b- defN 24-Mar-07 22:29 olive/model/config/registry.py
--rw-rw-r--  2.0 unx     1139 b- defN 24-Mar-07 22:29 olive/model/handler/__init__.py
--rw-rw-r--  2.0 unx     3190 b- defN 24-Mar-07 22:29 olive/model/handler/base.py
--rw-rw-r--  2.0 unx     4461 b- defN 24-Mar-07 22:29 olive/model/handler/composite.py
--rw-rw-r--  2.0 unx     8706 b- defN 24-Mar-07 22:29 olive/model/handler/onnx.py
--rw-rw-r--  2.0 unx     3556 b- defN 24-Mar-07 22:29 olive/model/handler/openvino.py
--rw-rw-r--  2.0 unx    16307 b- defN 24-Mar-07 22:29 olive/model/handler/pytorch.py
--rw-rw-r--  2.0 unx     4704 b- defN 24-Mar-07 22:29 olive/model/handler/qnn.py
--rw-rw-r--  2.0 unx     2597 b- defN 24-Mar-07 22:29 olive/model/handler/snpe.py
--rw-rw-r--  2.0 unx     1474 b- defN 24-Mar-07 22:29 olive/model/handler/tensorflow.py
--rw-rw-r--  2.0 unx      936 b- defN 24-Mar-07 22:29 olive/model/handler/mixin/__init__.py
--rw-rw-r--  2.0 unx      418 b- defN 24-Mar-07 22:29 olive/model/handler/mixin/composite.py
--rw-rw-r--  2.0 unx     3021 b- defN 24-Mar-07 22:29 olive/model/handler/mixin/dummy_inputs.py
--rw-rw-r--  2.0 unx     3959 b- defN 24-Mar-07 22:29 olive/model/handler/mixin/hf_config.py
--rw-rw-r--  2.0 unx      613 b- defN 24-Mar-07 22:29 olive/model/handler/mixin/io_config.py
--rw-rw-r--  2.0 unx     1472 b- defN 24-Mar-07 22:29 olive/model/handler/mixin/json.py
--rw-rw-r--  2.0 unx     1402 b- defN 24-Mar-07 22:29 olive/model/handler/mixin/onnx_ep.py
--rw-rw-r--  2.0 unx     3946 b- defN 24-Mar-07 22:29 olive/model/handler/mixin/onnx_graph.py
--rw-rw-r--  2.0 unx     2998 b- defN 24-Mar-07 22:29 olive/model/handler/mixin/resource.py
--rw-rw-r--  2.0 unx      609 b- defN 24-Mar-07 22:29 olive/model/utils/__init__.py
--rw-rw-r--  2.0 unx     3091 b- defN 24-Mar-07 22:29 olive/model/utils/hf_mappings.py
--rw-rw-r--  2.0 unx     4517 b- defN 24-Mar-07 22:29 olive/model/utils/hf_onnx_config.py
--rw-rw-r--  2.0 unx     7888 b- defN 24-Mar-07 22:29 olive/model/utils/hf_utils.py
--rw-rw-r--  2.0 unx     2893 b- defN 24-Mar-07 22:29 olive/model/utils/onnx_utils.py
--rw-rw-r--  2.0 unx     1510 b- defN 24-Mar-07 22:29 olive/model/utils/path_utils.py
--rw-rw-r--  2.0 unx      693 b- defN 24-Mar-07 22:29 olive/passes/__init__.py
--rw-rw-r--  2.0 unx    19969 b- defN 24-Mar-07 22:29 olive/passes/olive_pass.py
--rw-rw-r--  2.0 unx     5154 b- defN 24-Mar-07 22:29 olive/passes/pass_config.py
--rw-rw-r--  2.0 unx     2285 b- defN 24-Mar-07 22:29 olive/passes/onnx/__init__.py
--rw-rw-r--  2.0 unx     8404 b- defN 24-Mar-07 22:29 olive/passes/onnx/append_pre_post_processing_ops.py
--rw-rw-r--  2.0 unx     5520 b- defN 24-Mar-07 22:29 olive/passes/onnx/bnb_quantization.py
--rw-rw-r--  2.0 unx     7485 b- defN 24-Mar-07 22:29 olive/passes/onnx/common.py
--rw-rw-r--  2.0 unx    26410 b- defN 24-Mar-07 22:29 olive/passes/onnx/conversion.py
--rw-rw-r--  2.0 unx     4856 b- defN 24-Mar-07 22:29 olive/passes/onnx/dynamic_to_fixed_shape.py
--rw-rw-r--  2.0 unx     3086 b- defN 24-Mar-07 22:29 olive/passes/onnx/float16_conversion.py
--rw-rw-r--  2.0 unx     3507 b- defN 24-Mar-07 22:29 olive/passes/onnx/genai_model_exporter.py
--rw-rw-r--  2.0 unx    26480 b- defN 24-Mar-07 22:29 olive/passes/onnx/inc_quantization.py
--rw-rw-r--  2.0 unx    12955 b- defN 24-Mar-07 22:29 olive/passes/onnx/insert_beam_search.py
--rw-rw-r--  2.0 unx    18495 b- defN 24-Mar-07 22:29 olive/passes/onnx/merge_decoders.py
--rw-rw-r--  2.0 unx     9050 b- defN 24-Mar-07 22:29 olive/passes/onnx/mixed_precision.py
--rw-rw-r--  2.0 unx    10007 b- defN 24-Mar-07 22:29 olive/passes/onnx/model_optimizer.py
--rw-rw-r--  2.0 unx    15816 b- defN 24-Mar-07 22:29 olive/passes/onnx/moe_experts_distributor.py
--rw-rw-r--  2.0 unx     6158 b- defN 24-Mar-07 22:29 olive/passes/onnx/optimum_conversion.py
--rw-rw-r--  2.0 unx     3680 b- defN 24-Mar-07 22:29 olive/passes/onnx/optimum_merging.py
--rw-rw-r--  2.0 unx    29999 b- defN 24-Mar-07 22:29 olive/passes/onnx/perf_tuning.py
--rw-rw-r--  2.0 unx     1962 b- defN 24-Mar-07 22:29 olive/passes/onnx/qnn_preprocess.py
--rw-rw-r--  2.0 unx    27601 b- defN 24-Mar-07 22:29 olive/passes/onnx/quantization.py
--rw-rw-r--  2.0 unx    18748 b- defN 24-Mar-07 22:29 olive/passes/onnx/transformer_optimization.py
--rw-rw-r--  2.0 unx    15105 b- defN 24-Mar-07 22:29 olive/passes/onnx/vitis_ai_quantization.py
--rw-rw-r--  2.0 unx     3239 b- defN 24-Mar-07 22:29 olive/passes/onnx/pipeline/__init__.py
--rw-rw-r--  2.0 unx     8457 b- defN 24-Mar-07 22:29 olive/passes/onnx/pipeline/step_utils.py
--rw-rw-r--  2.0 unx      644 b- defN 24-Mar-07 22:29 olive/passes/onnx/vitis_ai/__init__.py
--rw-rw-r--  2.0 unx     7762 b- defN 24-Mar-07 22:29 olive/passes/onnx/vitis_ai/calibrate.py
--rw-rw-r--  2.0 unx    15332 b- defN 24-Mar-07 22:29 olive/passes/onnx/vitis_ai/quant_utils.py
--rw-rw-r--  2.0 unx    13580 b- defN 24-Mar-07 22:29 olive/passes/onnx/vitis_ai/quantize.py
--rw-rw-r--  2.0 unx    36359 b- defN 24-Mar-07 22:29 olive/passes/onnx/vitis_ai/quantizer.py
--rw-rw-r--  2.0 unx    18700 b- defN 24-Mar-07 22:29 olive/passes/onnx/vitis_ai/refine.py
--rw-rw-r--  2.0 unx      448 b- defN 24-Mar-07 22:29 olive/passes/openvino/__init__.py
--rw-rw-r--  2.0 unx     4551 b- defN 24-Mar-07 22:29 olive/passes/openvino/conversion.py
--rw-rw-r--  2.0 unx    12801 b- defN 24-Mar-07 22:29 olive/passes/openvino/quantization.py
--rw-rw-r--  2.0 unx      746 b- defN 24-Mar-07 22:29 olive/passes/pytorch/__init__.py
--rw-rw-r--  2.0 unx     7058 b- defN 24-Mar-07 22:29 olive/passes/pytorch/cluster.py
--rw-rw-r--  2.0 unx    48323 b- defN 24-Mar-07 22:29 olive/passes/pytorch/lora.py
--rw-rw-r--  2.0 unx     1021 b- defN 24-Mar-07 22:29 olive/passes/pytorch/pytorch_lightning_utils.py
--rw-rw-r--  2.0 unx     8038 b- defN 24-Mar-07 22:29 olive/passes/pytorch/qat_utils.py
--rw-rw-r--  2.0 unx     6440 b- defN 24-Mar-07 22:29 olive/passes/pytorch/quantization_aware_training.py
--rw-rw-r--  2.0 unx     9049 b- defN 24-Mar-07 22:29 olive/passes/pytorch/sparsegpt.py
--rw-rw-r--  2.0 unx    10774 b- defN 24-Mar-07 22:29 olive/passes/pytorch/sparsegpt_utils.py
--rw-rw-r--  2.0 unx     7118 b- defN 24-Mar-07 22:29 olive/passes/pytorch/tensor_parallel.py
--rw-rw-r--  2.0 unx     5372 b- defN 24-Mar-07 22:29 olive/passes/pytorch/tensor_parallel_layers.py
--rw-rw-r--  2.0 unx    17239 b- defN 24-Mar-07 22:29 olive/passes/pytorch/tensor_parallel_llama2.py
--rw-rw-r--  2.0 unx     8049 b- defN 24-Mar-07 22:29 olive/passes/pytorch/torch_trt_conversion.py
--rw-rw-r--  2.0 unx     3185 b- defN 24-Mar-07 22:29 olive/passes/pytorch/trt_utils.py
--rw-rw-r--  2.0 unx      534 b- defN 24-Mar-07 22:29 olive/passes/qnn/__init__.py
--rw-rw-r--  2.0 unx     1460 b- defN 24-Mar-07 22:29 olive/passes/qnn/common.py
--rw-rw-r--  2.0 unx     3440 b- defN 24-Mar-07 22:29 olive/passes/qnn/context_binary_generator.py
--rw-rw-r--  2.0 unx     5516 b- defN 24-Mar-07 22:29 olive/passes/qnn/conversion.py
--rw-rw-r--  2.0 unx     3546 b- defN 24-Mar-07 22:29 olive/passes/qnn/model_lib_generator.py
--rw-rw-r--  2.0 unx      501 b- defN 24-Mar-07 22:29 olive/passes/snpe/__init__.py
--rw-rw-r--  2.0 unx     4844 b- defN 24-Mar-07 22:29 olive/passes/snpe/conversion.py
--rw-rw-r--  2.0 unx     5246 b- defN 24-Mar-07 22:29 olive/passes/snpe/quantization.py
--rw-rw-r--  2.0 unx     2631 b- defN 24-Mar-07 22:29 olive/passes/snpe/snpe_to_onnx.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/passes/utils/__init__.py
--rw-rw-r--  2.0 unx     1619 b- defN 24-Mar-07 22:29 olive/passes/utils/whisper_prepost.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/platform_sdk/__init__.py
--rw-rw-r--  2.0 unx      341 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/__init__.py
--rw-rw-r--  2.0 unx     3740 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/configure.py
--rw-rw-r--  2.0 unx     1348 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/constants.py
--rw-rw-r--  2.0 unx     1046 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/copy_libcdsprpc.ps1
--rw-rw-r--  2.0 unx     2387 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/create_python_env.ps1
--rw-rw-r--  2.0 unx     2431 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/create_python_env.sh
--rw-rw-r--  2.0 unx     4034 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/env.py
--rw-rw-r--  2.0 unx     3022 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/runner.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/qnn/__init__.py
--rw-rw-r--  2.0 unx     2034 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/qnn/env.py
--rw-rw-r--  2.0 unx     7269 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/qnn/qnn.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/qnn/utils/__init__.py
--rw-rw-r--  2.0 unx      396 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/snpe/__init__.py
--rw-rw-r--  2.0 unx     2117 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/snpe/env.py
--rw-rw-r--  2.0 unx     3969 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/snpe/snpe.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/snpe/tools/__init__.py
--rw-rw-r--  2.0 unx    10518 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/snpe/tools/dev.py
--rw-rw-r--  2.0 unx    19039 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/snpe/tools/inference.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/snpe/utils/__init__.py
--rw-rw-r--  2.0 unx     6978 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/snpe/utils/adb.py
--rw-rw-r--  2.0 unx      374 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/utils/__init__.py
--rw-rw-r--  2.0 unx    14746 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/utils/data_loader.py
--rw-rw-r--  2.0 unx     8878 b- defN 24-Mar-07 22:29 olive/platform_sdk/qualcomm/utils/input_list.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/scripts/__init__.py
--rw-rw-r--  2.0 unx     6435 b- defN 24-Mar-07 22:29 olive/scripts/manage_compute_instance.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/strategy/__init__.py
--rw-rw-r--  2.0 unx    11837 b- defN 24-Mar-07 22:29 olive/strategy/search_parameter.py
--rw-rw-r--  2.0 unx     5631 b- defN 24-Mar-07 22:29 olive/strategy/search_results.py
--rw-rw-r--  2.0 unx     4633 b- defN 24-Mar-07 22:29 olive/strategy/search_space.py
--rw-rw-r--  2.0 unx    12885 b- defN 24-Mar-07 22:29 olive/strategy/search_strategy.py
--rw-rw-r--  2.0 unx     2817 b- defN 24-Mar-07 22:29 olive/strategy/utils.py
--rw-rw-r--  2.0 unx      705 b- defN 24-Mar-07 22:29 olive/strategy/search_algorithm/__init__.py
--rw-rw-r--  2.0 unx     1089 b- defN 24-Mar-07 22:29 olive/strategy/search_algorithm/exhaustive.py
--rw-rw-r--  2.0 unx     4332 b- defN 24-Mar-07 22:29 olive/strategy/search_algorithm/optuna_sampler.py
--rw-rw-r--  2.0 unx     2419 b- defN 24-Mar-07 22:29 olive/strategy/search_algorithm/random_sampler.py
--rw-rw-r--  2.0 unx     2390 b- defN 24-Mar-07 22:29 olive/strategy/search_algorithm/search_algorithm.py
--rw-rw-r--  2.0 unx     1853 b- defN 24-Mar-07 22:29 olive/strategy/search_algorithm/tpe_sampler.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/systems/__init__.py
--rw-rw-r--  2.0 unx     1842 b- defN 24-Mar-07 22:29 olive/systems/common.py
--rw-rw-r--  2.0 unx     2448 b- defN 24-Mar-07 22:29 olive/systems/local.py
--rw-rw-r--  2.0 unx     1718 b- defN 24-Mar-07 22:29 olive/systems/olive_system.py
--rw-rw-r--  2.0 unx     3140 b- defN 24-Mar-07 22:29 olive/systems/system_alias.py
--rw-rw-r--  2.0 unx     5151 b- defN 24-Mar-07 22:29 olive/systems/system_config.py
--rw-rw-r--  2.0 unx      411 b- defN 24-Mar-07 22:29 olive/systems/azureml/__init__.py
--rw-rw-r--  2.0 unx     2883 b- defN 24-Mar-07 22:29 olive/systems/azureml/aml_evaluation_runner.py
--rw-rw-r--  2.0 unx     8521 b- defN 24-Mar-07 22:29 olive/systems/azureml/aml_pass_runner.py
--rw-rw-r--  2.0 unx    31742 b- defN 24-Mar-07 22:29 olive/systems/azureml/aml_system.py
--rw-rw-r--  2.0 unx      717 b- defN 24-Mar-07 22:29 olive/systems/docker/Dockerfile
--rw-rw-r--  2.0 unx      464 b- defN 24-Mar-07 22:29 olive/systems/docker/Dockerfile.cpu
--rw-rw-r--  2.0 unx     1006 b- defN 24-Mar-07 22:29 olive/systems/docker/Dockerfile.gpu
--rw-rw-r--  2.0 unx     2531 b- defN 24-Mar-07 22:29 olive/systems/docker/Dockerfile.openvino
--rw-rw-r--  2.0 unx      407 b- defN 24-Mar-07 22:29 olive/systems/docker/__init__.py
--rw-rw-r--  2.0 unx    18212 b- defN 24-Mar-07 22:29 olive/systems/docker/docker_system.py
--rw-rw-r--  2.0 unx     2187 b- defN 24-Mar-07 22:29 olive/systems/docker/eval.py
--rw-rw-r--  2.0 unx     1713 b- defN 24-Mar-07 22:29 olive/systems/docker/runner.py
--rw-rw-r--  2.0 unx     6200 b- defN 24-Mar-07 22:29 olive/systems/docker/utils.py
--rw-rw-r--  2.0 unx      357 b- defN 24-Mar-07 22:29 olive/systems/isolated_ort/__init__.py
--rw-rw-r--  2.0 unx     3077 b- defN 24-Mar-07 22:29 olive/systems/isolated_ort/inference_runner.py
--rw-rw-r--  2.0 unx    10786 b- defN 24-Mar-07 22:29 olive/systems/isolated_ort/isolated_ort_system.py
--rw-rw-r--  2.0 unx      381 b- defN 24-Mar-07 22:29 olive/systems/python_environment/__init__.py
--rw-rw-r--  2.0 unx       37 b- defN 24-Mar-07 22:29 olive/systems/python_environment/common_requirements.txt
--rw-rw-r--  2.0 unx     2279 b- defN 24-Mar-07 22:29 olive/systems/python_environment/evaluation_runner.py
--rw-rw-r--  2.0 unx     1767 b- defN 24-Mar-07 22:29 olive/systems/python_environment/pass_runner.py
--rw-rw-r--  2.0 unx     7944 b- defN 24-Mar-07 22:29 olive/systems/python_environment/python_environment_system.py
--rw-rw-r--  2.0 unx      689 b- defN 24-Mar-07 22:29 olive/systems/utils/__init__.py
--rw-rw-r--  2.0 unx     2082 b- defN 24-Mar-07 22:29 olive/systems/utils/arg_parser.py
--rw-rw-r--  2.0 unx      913 b- defN 24-Mar-07 22:29 olive/systems/utils/available_providers_runner.py
--rw-rw-r--  2.0 unx     6783 b- defN 24-Mar-07 22:29 olive/systems/utils/misc.py
--rw-rw-r--  2.0 unx      306 b- defN 24-Mar-07 22:29 olive/workflows/__init__.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/workflows/run/__init__.py
--rw-rw-r--  2.0 unx      980 b- defN 24-Mar-07 22:29 olive/workflows/run/__main__.py
--rw-rw-r--  2.0 unx    13705 b- defN 24-Mar-07 22:29 olive/workflows/run/config.py
--rw-rw-r--  2.0 unx    11781 b- defN 24-Mar-07 22:29 olive/workflows/run/run.py
--rw-rw-r--  2.0 unx      431 b- defN 24-Mar-07 22:29 olive/workflows/snpe/__init__.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/workflows/snpe/convertquantize/__init__.py
--rw-rw-r--  2.0 unx     1339 b- defN 24-Mar-07 22:29 olive/workflows/snpe/convertquantize/__main__.py
--rw-rw-r--  2.0 unx     4445 b- defN 24-Mar-07 22:29 olive/workflows/snpe/convertquantize/convertquantize.py
--rw-rw-r--  2.0 unx      247 b- defN 24-Mar-07 22:29 olive/workflows/snpe/evaluate/__init__.py
--rw-rw-r--  2.0 unx      955 b- defN 24-Mar-07 22:29 olive/workflows/snpe/evaluate/__main__.py
--rw-rw-r--  2.0 unx     2974 b- defN 24-Mar-07 22:29 olive/workflows/snpe/evaluate/evaluate.py
--rw-rw-r--  2.0 unx     1141 b- defN 24-Mar-07 22:30 olive_ai-0.5.0.dist-info/LICENSE
--rw-rw-r--  2.0 unx     3220 b- defN 24-Mar-07 22:30 olive_ai-0.5.0.dist-info/METADATA
--rw-rw-r--  2.0 unx   764423 b- defN 24-Mar-07 22:30 olive_ai-0.5.0.dist-info/NOTICE.txt
--rw-rw-r--  2.0 unx       92 b- defN 24-Mar-07 22:30 olive_ai-0.5.0.dist-info/WHEEL
--rw-rw-r--  2.0 unx      101 b- defN 24-Mar-07 22:30 olive_ai-0.5.0.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        6 b- defN 24-Mar-07 22:30 olive_ai-0.5.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    22043 b- defN 24-Mar-07 22:30 olive_ai-0.5.0.dist-info/RECORD
-238 files, 2167541 bytes uncompressed, 453877 bytes compressed:  79.1%
+Zip file size: 503221 bytes, number of entries: 240
+-rw-rw-r--  2.0 unx      608 b- defN 24-Apr-07 07:47 olive/__init__.py
+-rw-rw-r--  2.0 unx     9998 b- defN 24-Apr-07 07:47 olive/cache.py
+-rw-rw-r--  2.0 unx     1113 b- defN 24-Apr-07 07:47 olive/constants.py
+-rw-rw-r--  2.0 unx     2626 b- defN 24-Apr-07 07:47 olive/logging.py
+-rw-rw-r--  2.0 unx     7817 b- defN 24-Apr-07 07:47 olive/olive_config.json
+-rw-rw-r--  2.0 unx     1406 b- defN 24-Apr-07 07:47 olive/package_config.py
+-rw-rw-r--  2.0 unx    22999 b- defN 24-Apr-07 07:47 olive/resource_path.py
+-rw-rw-r--  2.0 unx     5181 b- defN 24-Apr-07 07:47 olive/auto_optimizer/__init__.py
+-rw-rw-r--  2.0 unx     5871 b- defN 24-Apr-07 07:47 olive/auto_optimizer/regulate_mixins.py
+-rw-rw-r--  2.0 unx     3615 b- defN 24-Apr-07 07:47 olive/auto_optimizer/template_mapping.py
+-rw-rw-r--  2.0 unx     1171 b- defN 24-Apr-07 07:47 olive/auto_optimizer/config_template/opt_level_passes.yaml
+-rw-rw-r--  2.0 unx     2062 b- defN 24-Apr-07 07:47 olive/auto_optimizer/config_template/pass_capability.yaml
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/azureml/__init__.py
+-rw-rw-r--  2.0 unx     6412 b- defN 24-Apr-07 07:47 olive/azureml/azureml_client.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/common/__init__.py
+-rw-rw-r--  2.0 unx     3430 b- defN 24-Apr-07 07:47 olive/common/auto_config.py
+-rw-rw-r--  2.0 unx    10881 b- defN 24-Apr-07 07:47 olive/common/config_utils.py
+-rw-rw-r--  2.0 unx     1488 b- defN 24-Apr-07 07:47 olive/common/import_lib.py
+-rw-rw-r--  2.0 unx    16029 b- defN 24-Apr-07 07:47 olive/common/ort_inference.py
+-rw-rw-r--  2.0 unx      765 b- defN 24-Apr-07 07:47 olive/common/pydantic_v1.py
+-rw-rw-r--  2.0 unx     1809 b- defN 24-Apr-07 07:47 olive/common/user_module_loader.py
+-rw-rw-r--  2.0 unx    11627 b- defN 24-Apr-07 07:47 olive/common/utils.py
+-rw-rw-r--  2.0 unx      345 b- defN 24-Apr-07 07:47 olive/data/__init__.py
+-rw-rw-r--  2.0 unx    10680 b- defN 24-Apr-07 07:47 olive/data/config.py
+-rw-rw-r--  2.0 unx     1443 b- defN 24-Apr-07 07:47 olive/data/constants.py
+-rw-rw-r--  2.0 unx     8119 b- defN 24-Apr-07 07:47 olive/data/registry.py
+-rw-rw-r--  2.0 unx     3573 b- defN 24-Apr-07 07:47 olive/data/template.py
+-rw-rw-r--  2.0 unx      444 b- defN 24-Apr-07 07:47 olive/data/component/__init__.py
+-rw-rw-r--  2.0 unx     1955 b- defN 24-Apr-07 07:47 olive/data/component/dataloader.py
+-rw-rw-r--  2.0 unx    10679 b- defN 24-Apr-07 07:47 olive/data/component/dataset.py
+-rw-rw-r--  2.0 unx     1928 b- defN 24-Apr-07 07:47 olive/data/component/load_dataset.py
+-rw-rw-r--  2.0 unx     2256 b- defN 24-Apr-07 07:47 olive/data/component/post_process_data.py
+-rw-rw-r--  2.0 unx    11446 b- defN 24-Apr-07 07:47 olive/data/component/pre_process_data.py
+-rw-rw-r--  2.0 unx    29278 b- defN 24-Apr-07 07:47 olive/data/component/text_generation.py
+-rw-rw-r--  2.0 unx      480 b- defN 24-Apr-07 07:47 olive/data/container/__init__.py
+-rw-rw-r--  2.0 unx     3385 b- defN 24-Apr-07 07:47 olive/data/container/data_container.py
+-rw-rw-r--  2.0 unx     1159 b- defN 24-Apr-07 07:47 olive/data/container/dummy_data_container.py
+-rw-rw-r--  2.0 unx     1690 b- defN 24-Apr-07 07:47 olive/data/container/huggingface_container.py
+-rw-rw-r--  2.0 unx     1721 b- defN 24-Apr-07 07:47 olive/data/container/raw_data_container.py
+-rw-rw-r--  2.0 unx      411 b- defN 24-Apr-07 07:47 olive/engine/__init__.py
+-rw-rw-r--  2.0 unx     1447 b- defN 24-Apr-07 07:47 olive/engine/config.py
+-rw-rw-r--  2.0 unx    46635 b- defN 24-Apr-07 07:47 olive/engine/engine.py
+-rw-rw-r--  2.0 unx    17410 b- defN 24-Apr-07 07:47 olive/engine/footprint.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/engine/packaging/__init__.py
+-rw-rw-r--  2.0 unx     1642 b- defN 24-Apr-07 07:47 olive/engine/packaging/packaging_config.py
+-rw-rw-r--  2.0 unx    19748 b- defN 24-Apr-07 07:47 olive/engine/packaging/packaging_generator.py
+-rw-rw-r--  2.0 unx     1588 b- defN 24-Apr-07 07:47 olive/engine/packaging/sample_code/ONNXModel/cpp/README.md
+-rw-rw-r--  2.0 unx     4990 b- defN 24-Apr-07 07:47 olive/engine/packaging/sample_code/ONNXModel/cpp/code_sample.cpp
+-rw-rw-r--  2.0 unx     1590 b- defN 24-Apr-07 07:47 olive/engine/packaging/sample_code/ONNXModel/cs/README.md
+-rw-rw-r--  2.0 unx     4991 b- defN 24-Apr-07 07:47 olive/engine/packaging/sample_code/ONNXModel/cs/code_sample.cs
+-rw-rw-r--  2.0 unx     2296 b- defN 24-Apr-07 07:47 olive/engine/packaging/sample_code/ONNXModel/python/README.md
+-rw-rw-r--  2.0 unx     2545 b- defN 24-Apr-07 07:47 olive/engine/packaging/sample_code/ONNXModel/python/code_sample.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/evaluator/__init__.py
+-rw-rw-r--  2.0 unx     6473 b- defN 24-Apr-07 07:47 olive/evaluator/accuracy.py
+-rw-rw-r--  2.0 unx     8723 b- defN 24-Apr-07 07:47 olive/evaluator/metric.py
+-rw-rw-r--  2.0 unx     4461 b- defN 24-Apr-07 07:47 olive/evaluator/metric_backend.py
+-rw-rw-r--  2.0 unx     4395 b- defN 24-Apr-07 07:47 olive/evaluator/metric_config.py
+-rw-rw-r--  2.0 unx    47429 b- defN 24-Apr-07 07:47 olive/evaluator/olive_evaluator.py
+-rw-rw-r--  2.0 unx      592 b- defN 24-Apr-07 07:47 olive/exception/__init__.py
+-rw-rw-r--  2.0 unx      621 b- defN 24-Apr-07 07:47 olive/hardware/__init__.py
+-rw-rw-r--  2.0 unx    15059 b- defN 24-Apr-07 07:47 olive/hardware/accelerator.py
+-rw-rw-r--  2.0 unx     1273 b- defN 24-Apr-07 07:47 olive/hardware/constants.py
+-rw-rw-r--  2.0 unx      451 b- defN 24-Apr-07 07:47 olive/model/__init__.py
+-rw-rw-r--  2.0 unx      482 b- defN 24-Apr-07 07:47 olive/model/config/__init__.py
+-rw-rw-r--  2.0 unx    10950 b- defN 24-Apr-07 07:47 olive/model/config/hf_config.py
+-rw-rw-r--  2.0 unx     4843 b- defN 24-Apr-07 07:47 olive/model/config/io_config.py
+-rw-rw-r--  2.0 unx     3943 b- defN 24-Apr-07 07:47 olive/model/config/model_config.py
+-rw-rw-r--  2.0 unx     1045 b- defN 24-Apr-07 07:47 olive/model/config/registry.py
+-rw-rw-r--  2.0 unx     1139 b- defN 24-Apr-07 07:47 olive/model/handler/__init__.py
+-rw-rw-r--  2.0 unx     3190 b- defN 24-Apr-07 07:47 olive/model/handler/base.py
+-rw-rw-r--  2.0 unx     4461 b- defN 24-Apr-07 07:47 olive/model/handler/composite.py
+-rw-rw-r--  2.0 unx     8706 b- defN 24-Apr-07 07:47 olive/model/handler/onnx.py
+-rw-rw-r--  2.0 unx     3556 b- defN 24-Apr-07 07:47 olive/model/handler/openvino.py
+-rw-rw-r--  2.0 unx    16490 b- defN 24-Apr-07 07:47 olive/model/handler/pytorch.py
+-rw-rw-r--  2.0 unx     4704 b- defN 24-Apr-07 07:47 olive/model/handler/qnn.py
+-rw-rw-r--  2.0 unx     2597 b- defN 24-Apr-07 07:47 olive/model/handler/snpe.py
+-rw-rw-r--  2.0 unx     1474 b- defN 24-Apr-07 07:47 olive/model/handler/tensorflow.py
+-rw-rw-r--  2.0 unx      936 b- defN 24-Apr-07 07:47 olive/model/handler/mixin/__init__.py
+-rw-rw-r--  2.0 unx      418 b- defN 24-Apr-07 07:47 olive/model/handler/mixin/composite.py
+-rw-rw-r--  2.0 unx     3021 b- defN 24-Apr-07 07:47 olive/model/handler/mixin/dummy_inputs.py
+-rw-rw-r--  2.0 unx     3967 b- defN 24-Apr-07 07:47 olive/model/handler/mixin/hf_config.py
+-rw-rw-r--  2.0 unx      613 b- defN 24-Apr-07 07:47 olive/model/handler/mixin/io_config.py
+-rw-rw-r--  2.0 unx     1472 b- defN 24-Apr-07 07:47 olive/model/handler/mixin/json.py
+-rw-rw-r--  2.0 unx     1402 b- defN 24-Apr-07 07:47 olive/model/handler/mixin/onnx_ep.py
+-rw-rw-r--  2.0 unx     3946 b- defN 24-Apr-07 07:47 olive/model/handler/mixin/onnx_graph.py
+-rw-rw-r--  2.0 unx     2998 b- defN 24-Apr-07 07:47 olive/model/handler/mixin/resource.py
+-rw-rw-r--  2.0 unx      609 b- defN 24-Apr-07 07:47 olive/model/utils/__init__.py
+-rw-rw-r--  2.0 unx     3091 b- defN 24-Apr-07 07:47 olive/model/utils/hf_mappings.py
+-rw-rw-r--  2.0 unx     4517 b- defN 24-Apr-07 07:47 olive/model/utils/hf_onnx_config.py
+-rw-rw-r--  2.0 unx     8183 b- defN 24-Apr-07 07:47 olive/model/utils/hf_utils.py
+-rw-rw-r--  2.0 unx     2893 b- defN 24-Apr-07 07:47 olive/model/utils/onnx_utils.py
+-rw-rw-r--  2.0 unx     1510 b- defN 24-Apr-07 07:47 olive/model/utils/path_utils.py
+-rw-rw-r--  2.0 unx      499 b- defN 24-Apr-07 07:47 olive/passes/__init__.py
+-rw-rw-r--  2.0 unx    19895 b- defN 24-Apr-07 07:47 olive/passes/olive_pass.py
+-rw-rw-r--  2.0 unx     6139 b- defN 24-Apr-07 07:47 olive/passes/pass_config.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/passes/onnx/__init__.py
+-rw-rw-r--  2.0 unx     8408 b- defN 24-Apr-07 07:47 olive/passes/onnx/append_pre_post_processing_ops.py
+-rw-rw-r--  2.0 unx     5524 b- defN 24-Apr-07 07:47 olive/passes/onnx/bnb_quantization.py
+-rw-rw-r--  2.0 unx     7485 b- defN 24-Apr-07 07:47 olive/passes/onnx/common.py
+-rw-rw-r--  2.0 unx    27072 b- defN 24-Apr-07 07:47 olive/passes/onnx/conversion.py
+-rw-rw-r--  2.0 unx     4862 b- defN 24-Apr-07 07:47 olive/passes/onnx/dynamic_to_fixed_shape.py
+-rw-rw-r--  2.0 unx     3090 b- defN 24-Apr-07 07:47 olive/passes/onnx/float16_conversion.py
+-rw-rw-r--  2.0 unx     3491 b- defN 24-Apr-07 07:47 olive/passes/onnx/genai_model_exporter.py
+-rw-rw-r--  2.0 unx    27095 b- defN 24-Apr-07 07:47 olive/passes/onnx/inc_quantization.py
+-rw-rw-r--  2.0 unx    15972 b- defN 24-Apr-07 07:47 olive/passes/onnx/insert_beam_search.py
+-rw-rw-r--  2.0 unx    18495 b- defN 24-Apr-07 07:47 olive/passes/onnx/merge_decoders.py
+-rw-rw-r--  2.0 unx     9054 b- defN 24-Apr-07 07:47 olive/passes/onnx/mixed_precision.py
+-rw-rw-r--  2.0 unx    10011 b- defN 24-Apr-07 07:47 olive/passes/onnx/model_optimizer.py
+-rw-rw-r--  2.0 unx    15752 b- defN 24-Apr-07 07:47 olive/passes/onnx/moe_experts_distributor.py
+-rw-rw-r--  2.0 unx     6162 b- defN 24-Apr-07 07:47 olive/passes/onnx/optimum_conversion.py
+-rw-rw-r--  2.0 unx     3709 b- defN 24-Apr-07 07:47 olive/passes/onnx/optimum_merging.py
+-rw-rw-r--  2.0 unx    29003 b- defN 24-Apr-07 07:47 olive/passes/onnx/perf_tuning.py
+-rw-rw-r--  2.0 unx     5375 b- defN 24-Apr-07 07:47 olive/passes/onnx/qnn_preprocess.py
+-rw-rw-r--  2.0 unx    37646 b- defN 24-Apr-07 07:47 olive/passes/onnx/quantization.py
+-rw-rw-r--  2.0 unx    19253 b- defN 24-Apr-07 07:47 olive/passes/onnx/transformer_optimization.py
+-rw-rw-r--  2.0 unx    15134 b- defN 24-Apr-07 07:47 olive/passes/onnx/vitis_ai_quantization.py
+-rw-rw-r--  2.0 unx     3239 b- defN 24-Apr-07 07:47 olive/passes/onnx/pipeline/__init__.py
+-rw-rw-r--  2.0 unx     8457 b- defN 24-Apr-07 07:47 olive/passes/onnx/pipeline/step_utils.py
+-rw-rw-r--  2.0 unx      508 b- defN 24-Apr-07 07:47 olive/passes/onnx/vitis_ai/__init__.py
+-rw-rw-r--  2.0 unx     7762 b- defN 24-Apr-07 07:47 olive/passes/onnx/vitis_ai/calibrate.py
+-rw-rw-r--  2.0 unx    15332 b- defN 24-Apr-07 07:47 olive/passes/onnx/vitis_ai/quant_utils.py
+-rw-rw-r--  2.0 unx    13580 b- defN 24-Apr-07 07:47 olive/passes/onnx/vitis_ai/quantize.py
+-rw-rw-r--  2.0 unx    36359 b- defN 24-Apr-07 07:47 olive/passes/onnx/vitis_ai/quantizer.py
+-rw-rw-r--  2.0 unx    18700 b- defN 24-Apr-07 07:47 olive/passes/onnx/vitis_ai/refine.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/passes/openvino/__init__.py
+-rw-rw-r--  2.0 unx     4555 b- defN 24-Apr-07 07:47 olive/passes/openvino/conversion.py
+-rw-rw-r--  2.0 unx    12610 b- defN 24-Apr-07 07:47 olive/passes/openvino/quantization.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/passes/pytorch/__init__.py
+-rw-rw-r--  2.0 unx     7058 b- defN 24-Apr-07 07:47 olive/passes/pytorch/cluster.py
+-rw-rw-r--  2.0 unx     9842 b- defN 24-Apr-07 07:47 olive/passes/pytorch/gptq.py
+-rw-rw-r--  2.0 unx    11528 b- defN 24-Apr-07 07:47 olive/passes/pytorch/gptq_utils.py
+-rw-rw-r--  2.0 unx    48812 b- defN 24-Apr-07 07:47 olive/passes/pytorch/lora.py
+-rw-rw-r--  2.0 unx     1021 b- defN 24-Apr-07 07:47 olive/passes/pytorch/pytorch_lightning_utils.py
+-rw-rw-r--  2.0 unx     8038 b- defN 24-Apr-07 07:47 olive/passes/pytorch/qat_utils.py
+-rw-rw-r--  2.0 unx     6444 b- defN 24-Apr-07 07:47 olive/passes/pytorch/quantization_aware_training.py
+-rw-rw-r--  2.0 unx     9021 b- defN 24-Apr-07 07:47 olive/passes/pytorch/sparsegpt.py
+-rw-rw-r--  2.0 unx    10774 b- defN 24-Apr-07 07:47 olive/passes/pytorch/sparsegpt_utils.py
+-rw-rw-r--  2.0 unx     7032 b- defN 24-Apr-07 07:47 olive/passes/pytorch/tensor_parallel.py
+-rw-rw-r--  2.0 unx     5372 b- defN 24-Apr-07 07:47 olive/passes/pytorch/tensor_parallel_layers.py
+-rw-rw-r--  2.0 unx    17239 b- defN 24-Apr-07 07:47 olive/passes/pytorch/tensor_parallel_llama2.py
+-rw-rw-r--  2.0 unx     8053 b- defN 24-Apr-07 07:47 olive/passes/pytorch/torch_trt_conversion.py
+-rw-rw-r--  2.0 unx     3185 b- defN 24-Apr-07 07:47 olive/passes/pytorch/trt_utils.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/passes/qnn/__init__.py
+-rw-rw-r--  2.0 unx     3192 b- defN 24-Apr-07 07:47 olive/passes/qnn/context_binary_generator.py
+-rw-rw-r--  2.0 unx     5268 b- defN 24-Apr-07 07:47 olive/passes/qnn/conversion.py
+-rw-rw-r--  2.0 unx     3298 b- defN 24-Apr-07 07:47 olive/passes/qnn/model_lib_generator.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/passes/snpe/__init__.py
+-rw-rw-r--  2.0 unx     4802 b- defN 24-Apr-07 07:47 olive/passes/snpe/conversion.py
+-rw-rw-r--  2.0 unx     5254 b- defN 24-Apr-07 07:47 olive/passes/snpe/quantization.py
+-rw-rw-r--  2.0 unx     2637 b- defN 24-Apr-07 07:47 olive/passes/snpe/snpe_to_onnx.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/passes/utils/__init__.py
+-rw-rw-r--  2.0 unx     1619 b- defN 24-Apr-07 07:47 olive/passes/utils/whisper_prepost.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/platform_sdk/__init__.py
+-rw-rw-r--  2.0 unx      341 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/__init__.py
+-rw-rw-r--  2.0 unx     3740 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/configure.py
+-rw-rw-r--  2.0 unx     1348 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/constants.py
+-rw-rw-r--  2.0 unx     1046 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/copy_libcdsprpc.ps1
+-rw-rw-r--  2.0 unx     2387 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/create_python_env.ps1
+-rw-rw-r--  2.0 unx     2431 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/create_python_env.sh
+-rw-rw-r--  2.0 unx     4034 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/env.py
+-rw-rw-r--  2.0 unx     3652 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/runner.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/qnn/__init__.py
+-rw-rw-r--  2.0 unx     2034 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/qnn/env.py
+-rw-rw-r--  2.0 unx     7269 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/qnn/qnn.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/qnn/utils/__init__.py
+-rw-rw-r--  2.0 unx      396 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/snpe/__init__.py
+-rw-rw-r--  2.0 unx     2117 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/snpe/env.py
+-rw-rw-r--  2.0 unx     3969 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/snpe/snpe.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/snpe/tools/__init__.py
+-rw-rw-r--  2.0 unx    10951 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/snpe/tools/dev.py
+-rw-rw-r--  2.0 unx    19039 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/snpe/tools/inference.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/snpe/utils/__init__.py
+-rw-rw-r--  2.0 unx     6978 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/snpe/utils/adb.py
+-rw-rw-r--  2.0 unx      374 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/utils/__init__.py
+-rw-rw-r--  2.0 unx    14746 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/utils/data_loader.py
+-rw-rw-r--  2.0 unx     8878 b- defN 24-Apr-07 07:47 olive/platform_sdk/qualcomm/utils/input_list.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/scripts/__init__.py
+-rw-rw-r--  2.0 unx     6435 b- defN 24-Apr-07 07:47 olive/scripts/manage_compute_instance.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/strategy/__init__.py
+-rw-rw-r--  2.0 unx    11837 b- defN 24-Apr-07 07:47 olive/strategy/search_parameter.py
+-rw-rw-r--  2.0 unx     5631 b- defN 24-Apr-07 07:47 olive/strategy/search_results.py
+-rw-rw-r--  2.0 unx     4702 b- defN 24-Apr-07 07:47 olive/strategy/search_space.py
+-rw-rw-r--  2.0 unx    12873 b- defN 24-Apr-07 07:47 olive/strategy/search_strategy.py
+-rw-rw-r--  2.0 unx     2817 b- defN 24-Apr-07 07:47 olive/strategy/utils.py
+-rw-rw-r--  2.0 unx      705 b- defN 24-Apr-07 07:47 olive/strategy/search_algorithm/__init__.py
+-rw-rw-r--  2.0 unx     1091 b- defN 24-Apr-07 07:47 olive/strategy/search_algorithm/exhaustive.py
+-rw-rw-r--  2.0 unx     4334 b- defN 24-Apr-07 07:47 olive/strategy/search_algorithm/optuna_sampler.py
+-rw-rw-r--  2.0 unx     2421 b- defN 24-Apr-07 07:47 olive/strategy/search_algorithm/random_sampler.py
+-rw-rw-r--  2.0 unx     2306 b- defN 24-Apr-07 07:47 olive/strategy/search_algorithm/search_algorithm.py
+-rw-rw-r--  2.0 unx     1841 b- defN 24-Apr-07 07:47 olive/strategy/search_algorithm/tpe_sampler.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/systems/__init__.py
+-rw-rw-r--  2.0 unx     2227 b- defN 24-Apr-07 07:47 olive/systems/common.py
+-rw-rw-r--  2.0 unx     2300 b- defN 24-Apr-07 07:47 olive/systems/local.py
+-rw-rw-r--  2.0 unx     2158 b- defN 24-Apr-07 07:47 olive/systems/olive_system.py
+-rw-rw-r--  2.0 unx     3140 b- defN 24-Apr-07 07:47 olive/systems/system_alias.py
+-rw-rw-r--  2.0 unx     7048 b- defN 24-Apr-07 07:47 olive/systems/system_config.py
+-rw-rw-r--  2.0 unx      411 b- defN 24-Apr-07 07:47 olive/systems/azureml/__init__.py
+-rw-rw-r--  2.0 unx     2883 b- defN 24-Apr-07 07:47 olive/systems/azureml/aml_evaluation_runner.py
+-rw-rw-r--  2.0 unx     8763 b- defN 24-Apr-07 07:47 olive/systems/azureml/aml_pass_runner.py
+-rw-rw-r--  2.0 unx    31831 b- defN 24-Apr-07 07:47 olive/systems/azureml/aml_system.py
+-rw-rw-r--  2.0 unx      717 b- defN 24-Apr-07 07:47 olive/systems/docker/Dockerfile
+-rw-rw-r--  2.0 unx      464 b- defN 24-Apr-07 07:47 olive/systems/docker/Dockerfile.cpu
+-rw-rw-r--  2.0 unx     1006 b- defN 24-Apr-07 07:47 olive/systems/docker/Dockerfile.gpu
+-rw-rw-r--  2.0 unx     2531 b- defN 24-Apr-07 07:47 olive/systems/docker/Dockerfile.openvino
+-rw-rw-r--  2.0 unx      407 b- defN 24-Apr-07 07:47 olive/systems/docker/__init__.py
+-rw-rw-r--  2.0 unx    18244 b- defN 24-Apr-07 07:47 olive/systems/docker/docker_system.py
+-rw-rw-r--  2.0 unx     2187 b- defN 24-Apr-07 07:47 olive/systems/docker/eval.py
+-rw-rw-r--  2.0 unx     1956 b- defN 24-Apr-07 07:47 olive/systems/docker/runner.py
+-rw-rw-r--  2.0 unx     6200 b- defN 24-Apr-07 07:47 olive/systems/docker/utils.py
+-rw-rw-r--  2.0 unx      357 b- defN 24-Apr-07 07:47 olive/systems/isolated_ort/__init__.py
+-rw-rw-r--  2.0 unx     3077 b- defN 24-Apr-07 07:47 olive/systems/isolated_ort/inference_runner.py
+-rw-rw-r--  2.0 unx    10877 b- defN 24-Apr-07 07:47 olive/systems/isolated_ort/isolated_ort_system.py
+-rw-rw-r--  2.0 unx      381 b- defN 24-Apr-07 07:47 olive/systems/python_environment/__init__.py
+-rw-rw-r--  2.0 unx       37 b- defN 24-Apr-07 07:47 olive/systems/python_environment/common_requirements.txt
+-rw-rw-r--  2.0 unx     2279 b- defN 24-Apr-07 07:47 olive/systems/python_environment/evaluation_runner.py
+-rw-rw-r--  2.0 unx     2037 b- defN 24-Apr-07 07:47 olive/systems/python_environment/pass_runner.py
+-rw-rw-r--  2.0 unx     7892 b- defN 24-Apr-07 07:47 olive/systems/python_environment/python_environment_system.py
+-rw-rw-r--  2.0 unx      705 b- defN 24-Apr-07 07:47 olive/systems/utils/__init__.py
+-rw-rw-r--  2.0 unx     2082 b- defN 24-Apr-07 07:47 olive/systems/utils/arg_parser.py
+-rw-rw-r--  2.0 unx      913 b- defN 24-Apr-07 07:47 olive/systems/utils/available_providers_runner.py
+-rw-rw-r--  2.0 unx     7419 b- defN 24-Apr-07 07:47 olive/systems/utils/misc.py
+-rw-rw-r--  2.0 unx      306 b- defN 24-Apr-07 07:47 olive/workflows/__init__.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/workflows/run/__init__.py
+-rw-rw-r--  2.0 unx     1418 b- defN 24-Apr-07 07:47 olive/workflows/run/__main__.py
+-rw-rw-r--  2.0 unx    13710 b- defN 24-Apr-07 07:47 olive/workflows/run/config.py
+-rw-rw-r--  2.0 unx    13451 b- defN 24-Apr-07 07:47 olive/workflows/run/run.py
+-rw-rw-r--  2.0 unx      431 b- defN 24-Apr-07 07:47 olive/workflows/snpe/__init__.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/workflows/snpe/convertquantize/__init__.py
+-rw-rw-r--  2.0 unx     1339 b- defN 24-Apr-07 07:47 olive/workflows/snpe/convertquantize/__main__.py
+-rw-rw-r--  2.0 unx     4445 b- defN 24-Apr-07 07:47 olive/workflows/snpe/convertquantize/convertquantize.py
+-rw-rw-r--  2.0 unx      247 b- defN 24-Apr-07 07:47 olive/workflows/snpe/evaluate/__init__.py
+-rw-rw-r--  2.0 unx      955 b- defN 24-Apr-07 07:47 olive/workflows/snpe/evaluate/__main__.py
+-rw-rw-r--  2.0 unx     2974 b- defN 24-Apr-07 07:47 olive/workflows/snpe/evaluate/evaluate.py
+-rw-rw-r--  2.0 unx     1141 b- defN 24-Apr-07 07:47 olive_ai-0.5.1.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     3438 b- defN 24-Apr-07 07:47 olive_ai-0.5.1.dist-info/METADATA
+-rw-rw-r--  2.0 unx   764423 b- defN 24-Apr-07 07:47 olive_ai-0.5.1.dist-info/NOTICE.txt
+-rw-rw-r--  2.0 unx       92 b- defN 24-Apr-07 07:47 olive_ai-0.5.1.dist-info/WHEEL
+-rw-rw-r--  2.0 unx      101 b- defN 24-Apr-07 07:47 olive_ai-0.5.1.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        6 b- defN 24-Apr-07 07:47 olive_ai-0.5.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    22214 b- defN 24-Apr-07 07:47 olive_ai-0.5.1.dist-info/RECORD
+240 files, 2230015 bytes uncompressed, 467757 bytes compressed:  79.0%
```

## zipnote {}

```diff
@@ -3,18 +3,21 @@
 
 Filename: olive/cache.py
 Comment: 
 
 Filename: olive/constants.py
 Comment: 
 
-Filename: olive/extra_dependencies.json
+Filename: olive/logging.py
 Comment: 
 
-Filename: olive/logging.py
+Filename: olive/olive_config.json
+Comment: 
+
+Filename: olive/package_config.py
 Comment: 
 
 Filename: olive/resource_path.py
 Comment: 
 
 Filename: olive/auto_optimizer/__init__.py
 Comment: 
@@ -378,14 +381,20 @@
 
 Filename: olive/passes/pytorch/__init__.py
 Comment: 
 
 Filename: olive/passes/pytorch/cluster.py
 Comment: 
 
+Filename: olive/passes/pytorch/gptq.py
+Comment: 
+
+Filename: olive/passes/pytorch/gptq_utils.py
+Comment: 
+
 Filename: olive/passes/pytorch/lora.py
 Comment: 
 
 Filename: olive/passes/pytorch/pytorch_lightning_utils.py
 Comment: 
 
 Filename: olive/passes/pytorch/qat_utils.py
@@ -414,17 +423,14 @@
 
 Filename: olive/passes/pytorch/trt_utils.py
 Comment: 
 
 Filename: olive/passes/qnn/__init__.py
 Comment: 
 
-Filename: olive/passes/qnn/common.py
-Comment: 
-
 Filename: olive/passes/qnn/context_binary_generator.py
 Comment: 
 
 Filename: olive/passes/qnn/conversion.py
 Comment: 
 
 Filename: olive/passes/qnn/model_lib_generator.py
@@ -687,29 +693,29 @@
 
 Filename: olive/workflows/snpe/evaluate/__main__.py
 Comment: 
 
 Filename: olive/workflows/snpe/evaluate/evaluate.py
 Comment: 
 
-Filename: olive_ai-0.5.0.dist-info/LICENSE
+Filename: olive_ai-0.5.1.dist-info/LICENSE
 Comment: 
 
-Filename: olive_ai-0.5.0.dist-info/METADATA
+Filename: olive_ai-0.5.1.dist-info/METADATA
 Comment: 
 
-Filename: olive_ai-0.5.0.dist-info/NOTICE.txt
+Filename: olive_ai-0.5.1.dist-info/NOTICE.txt
 Comment: 
 
-Filename: olive_ai-0.5.0.dist-info/WHEEL
+Filename: olive_ai-0.5.1.dist-info/WHEEL
 Comment: 
 
-Filename: olive_ai-0.5.0.dist-info/entry_points.txt
+Filename: olive_ai-0.5.1.dist-info/entry_points.txt
 Comment: 
 
-Filename: olive_ai-0.5.0.dist-info/top_level.txt
+Filename: olive_ai-0.5.1.dist-info/top_level.txt
 Comment: 
 
-Filename: olive_ai-0.5.0.dist-info/RECORD
+Filename: olive_ai-0.5.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## olive/__init__.py

```diff
@@ -10,8 +10,8 @@
 
 _sc = logging.StreamHandler(stream=sys.stdout)
 _formatter = logging.Formatter("[%(asctime)s] [%(levelname)s] [%(filename)s:%(lineno)d:%(funcName)s] %(message)s")
 _sc.setFormatter(_formatter)
 _logger.addHandler(_sc)
 _logger.propagate = False
 
-__version__ = "0.5.0"
+__version__ = "0.5.1"
```

## olive/cache.py

```diff
@@ -3,15 +3,15 @@
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import json
 import logging
 import os
 import shutil
 from pathlib import Path
-from typing import Optional, Union
+from typing import Dict, Optional, Union
 
 from olive.common.config_utils import serialize_to_json
 from olive.common.utils import hash_dict
 from olive.resource_path import ResourcePath, create_resource_path
 
 logger = logging.getLogger(__name__)
 
@@ -172,15 +172,16 @@
             if isinstance(data_dir, ResourcePath) and data_dir.is_azureml_resource():
                 raise ValueError("could not append AzureML data to data_root")
 
             # we cannot use Path to join the path. If the data_root is something like: azureml://, then Path will
             # change the data_root to azureml:/, which is not a valid path
             data_full_path = os.path.join(data_root, data_dir_str).replace("\\", "/")
         else:
-            data_full_path = data_dir_str
+            # will keep this as is so that we don't lose information inside ResourcePath
+            data_full_path = data_dir
 
     return create_resource_path(data_full_path)
 
 
 def get_local_path_from_root(
     data_root: Union[str, Path], data_dir: Union[str, Path, ResourcePath], cache_dir: Union[str, Path] = ".olive-cache"
 ):
@@ -193,15 +194,15 @@
 
 def save_model(
     model_number: str,
     output_dir: Union[str, Path] = None,
     output_name: Union[str, Path] = None,
     overwrite: bool = False,
     cache_dir: Union[str, Path] = ".olive-cache",
-):
+) -> Optional[Dict]:
     """Save a model from the cache to a given path."""
     # This function should probably be outside of the cache module
     # just to be safe, import lazily to avoid any future circular imports
     from olive.model import ModelConfig
 
     model_number = model_number.split("_")[0]
     output_dir = Path(output_dir) if output_dir else Path.cwd()
```

## olive/resource_path.py

```diff
@@ -97,15 +97,15 @@
 
     @validator("config", pre=True)
     def validate_config(cls, v, values):
         if "type" not in values:
             raise ValueError("Invalid type.")
 
         config_class = ResourcePath.registry[values["type"]].get_config_class()
-        return validate_config(v, ConfigBase, config_class)
+        return validate_config(v, config_class)
 
     def create_resource_path(self) -> ResourcePath:
         return ResourcePath.registry[self.type](self.config)
 
 
 def create_resource_path(
     resource_path: Optional[Union[str, Path, Dict[str, Any], ResourcePathConfig, ResourcePath]]
@@ -183,22 +183,22 @@
         raise ValueError(f"Path {v} does not exist.")
     return Path(v).resolve()
 
 
 class LocalResourcePath(ResourcePath):
     """Base class for a local resource path."""
 
-    @staticmethod
-    def _default_config() -> Dict[str, Any]:
+    @classmethod
+    def _default_config(cls) -> Dict[str, Any]:
         return {
             "path": ConfigParam(type_=Union[Path, str], required=True, description="Path to the resource."),
         }
 
-    @staticmethod
-    def _validators() -> Dict[str, Callable]:
+    @classmethod
+    def _validators(cls) -> Dict[str, Callable]:
         return {"validate_path": validator("path", allow_reuse=True)(_validate_path)}
 
     def get_path(self) -> str:
         return str(self.config.path)
 
     def save_to_dir(self, dir_path: Union[Path, str], name: str = None, overwrite: bool = False) -> str:
         # directory to save the resource to
@@ -232,17 +232,17 @@
 
 
 class LocalFile(LocalResourcePath):
     """Local file resource path."""
 
     name = ResourceType.LocalFile
 
-    @staticmethod
-    def _validators() -> Dict[str, Callable[..., Any]]:
-        validators = LocalResourcePath._validators()
+    @classmethod
+    def _validators(cls) -> Dict[str, Callable[..., Any]]:
+        validators = super()._validators()
         validators.update({"validate_file_path": validator("path", allow_reuse=True)(_validate_file_path)})
         return validators
 
 
 def _validate_folder_path(v):
     path = Path(v)
     if not path.is_dir():
@@ -251,28 +251,28 @@
 
 
 class LocalFolder(LocalResourcePath):
     """Local folder resource path."""
 
     name = ResourceType.LocalFolder
 
-    @staticmethod
-    def _validators() -> Dict[str, Callable[..., Any]]:
-        validators = LocalResourcePath._validators()
+    @classmethod
+    def _validators(cls) -> Dict[str, Callable[..., Any]]:
+        validators = super()._validators()
         validators.update({"validate_folder_path": validator("path", allow_reuse=True)(_validate_folder_path)})
         return validators
 
 
 class StringName(ResourcePath):
     """String name resource path."""
 
     name = ResourceType.StringName
 
-    @staticmethod
-    def _default_config() -> Dict[str, Any]:
+    @classmethod
+    def _default_config(cls) -> Dict[str, Any]:
         return {
             "name": ConfigParam(type_=str, required=True, description="Name of the resource."),
         }
 
     def get_path(self) -> str:
         return self.config.name
 
@@ -340,16 +340,16 @@
 
 
 class AzureMLModel(AzureMLResource):
     """AzureML Model resource path."""
 
     name = ResourceType.AzureMLModel
 
-    @staticmethod
-    def _default_config() -> Dict[str, Any]:
+    @classmethod
+    def _default_config(cls) -> Dict[str, Any]:
         return {
             "azureml_client": ConfigParam(
                 type_=AzureMLClientConfig, required=True, description="AzureML client config."
             ),
             "name": ConfigParam(type_=str, required=True, description="Name of the model."),
             "version": ConfigParam(type_=Union[int, str], required=True, description="Version of the model."),
         }
@@ -366,16 +366,16 @@
 
 
 class AzureMLRegistryModel(AzureMLResource):
     """AzureML Model resource path."""
 
     name = ResourceType.AzureMLRegistryModel
 
-    @staticmethod
-    def _default_config() -> Dict[str, Any]:
+    @classmethod
+    def _default_config(cls) -> Dict[str, Any]:
         return {
             "azureml_client": ConfigParam(
                 type_=AzureMLClientConfig, required=False, description="AzureML client config."
             ),
             "registry_name": ConfigParam(type_=str, required=True, description="Name of the registry."),
             "name": ConfigParam(type_=str, required=True, description="Name of the model."),
             "version": ConfigParam(type_=Union[int, str], required=True, description="Version of the model."),
@@ -410,26 +410,26 @@
 
 
 class AzureMLDatastore(ResourcePath):
     """AzureML DataStore resource path."""
 
     name = ResourceType.AzureMLDatastore
 
-    @staticmethod
-    def _validators() -> Dict[str, Callable[..., Any]]:
-        validators = ResourcePath._validators()
+    @classmethod
+    def _validators(cls) -> Dict[str, Callable[..., Any]]:
+        validators = super()._validators()
         validators.update(
             {
                 "validate_datastore_url": validator("datastore_url", allow_reuse=True)(_datastore_url_validator),
             }
         )
         return validators
 
-    @staticmethod
-    def _default_config() -> Dict[str, Any]:
+    @classmethod
+    def _default_config(cls) -> Dict[str, Any]:
         return {
             "azureml_client": ConfigParam(type_=AzureMLClientConfig, description="AzureML client config."),
             "datastore_name": ConfigParam(type_=str, description="Name of the datastore."),
             "relative_path": ConfigParam(type_=str, description="Relative path to the resource."),
             "datastore_url": ConfigParam(type_=str, description="URL of the datastore."),
         }
 
@@ -447,32 +447,34 @@
         try:
             from azureml.fsspec import AzureMachineLearningFileSystem
         except ImportError:
             raise ImportError(
                 "azureml-fsspec is not installed. Please install azureml-fsspec to use AzureMLDatastore resource path."
             ) from None
         if fsspec is None:
-            fsspec = AzureMachineLearningFileSystem(self.get_path())
+            # provide mlclient so that it is used for authentication
+            fsspec = AzureMachineLearningFileSystem(
+                self.get_path(), ml_client=self.get_aml_client_config().create_client()
+            )
         return fsspec.info(self.get_relative_path()).get("type") == "file"
 
     def get_relative_path(self) -> str:
         if self.config.datastore_url:
             return re.split("/datastores/.*/paths/", self.config.datastore_url)[-1]
         return self.config.relative_path
 
     def get_aml_client_config(self) -> AzureMLClientConfig:
         if self.config.datastore_url:
-            subscription_id = re.split("/subscriptions/", self.config.datastore_url)[-1].split("/")[0]
-            resource_group = re.split("/resourcegroups/", self.config.datastore_url)[-1].split("/")[0]
-            workspace_name = re.split("/workspaces/", self.config.datastore_url)[-1].split("/")[0]
-            return AzureMLClientConfig(
-                subscription_id=subscription_id,
-                resource_group=resource_group,
-                workspace_name=workspace_name,
-            )
+            # datastore_url is always created by validator
+            # so we should start with azureml_client if it is already there
+            client_config = self.config.azureml_client.dict() if self.config.azureml_client else {}
+            client_config["subscription_id"] = re.split("/subscriptions/", self.config.datastore_url)[-1].split("/")[0]
+            client_config["resource_group"] = re.split("/resourcegroups/", self.config.datastore_url)[-1].split("/")[0]
+            client_config["workspace_name"] = re.split("/workspaces/", self.config.datastore_url)[-1].split("/")[0]
+            return AzureMLClientConfig.parse_obj(client_config)
         return self.config.azureml_client
 
     def save_to_dir(self, dir_path: Union[Path, str], name: str = None, overwrite: bool = False) -> str:
         # there is no direct way to download a file from a datastore
         # so we will use a workaround to download the file by creating a aml model
         # that references the file and downloading the model
         try:
@@ -481,15 +483,16 @@
             raise ImportError(
                 "azureml-fsspec is not installed. Please install azureml-fsspec to use AzureMLDatastore resource path."
             ) from None
 
         azureml_client_config = self.get_aml_client_config()
 
         # azureml file system
-        fs = AzureMachineLearningFileSystem(self.get_path())
+        # provide mlclient so that it is used for authentication
+        fs = AzureMachineLearningFileSystem(self.get_path(), ml_client=azureml_client_config.create_client())
         relative_path = Path(self.get_relative_path())
         is_file = self.is_file(fs)
         # path to save the resource to
         if name:
             new_path_name = Path(name).with_suffix("" if not is_file else relative_path.suffix).name
         else:
             new_path_name = relative_path.name
@@ -523,16 +526,16 @@
 
 
 class AzureMLJobOutput(ResourcePath):
     """AzureML job output resource path."""
 
     name = ResourceType.AzureMLJobOutput
 
-    @staticmethod
-    def _default_config() -> Dict[str, Any]:
+    @classmethod
+    def _default_config(cls) -> Dict[str, Any]:
         return {
             "azureml_client": ConfigParam(
                 type_=AzureMLClientConfig, required=True, description="AzureML client config."
             ),
             "job_name": ConfigParam(type_=str, required=True, description="Name of the job."),
             "output_name": ConfigParam(type_=str, required=True, description="Name of the output."),
             "relative_path": ConfigParam(type_=str, required=True, description="Relative path to the resource."),
```

## olive/auto_optimizer/regulate_mixins.py

```diff
@@ -32,15 +32,18 @@
             for pf in pfs:
                 if tuple(pf) not in unique_pass_flows:
                     pass_flows.append(pf)
                 unique_pass_flows.add(tuple(pf))
                 for p in pf:
                     if p not in pass_config:
                         pass_config.update({p: {"type": p, "config": {}}})
-
+        # disable pass search when search strategy is None/False
+        if not self.evaluator_config:
+            for pass_name in pass_config:
+                pass_config[pass_name]["disable_search"] = True
         return pass_config, pass_flows
 
     def _regulate_fp16(self, pass_config, pass_flows):
         pass_config = pass_config or {}
         is_gpu = self.accelerator_spec.accelerator_type == Device.GPU and self.accelerator_spec.execution_provider in [
             "CUDAExecutionProvider",
             "DmlExecutionProvider",
```

## olive/common/auto_config.py

```diff
@@ -16,16 +16,16 @@
     The class maintains a registry of all concrete subclasses.
     To refresh the registry for a child base class, e.g., SearchAlgorithm, just set registry = {}
 
     Dynamically created config class
     All classes are instantiated by passing a config dictionary or config class (BaseModel) instance.
     Sub-class developer just needs to implement the static method _default_config
     E.g.,
-        @staticmethod
-        def _default_config():
+        @classmethod
+        def _default_config(cls):
             return {
                 "str_param": ConfigParam(type_=str, required=True),
                 "func_param" ConfigParam(type_=Union[str, Callable], category=ParamCategory.OBJECT)
             }
     The class dynamically creates its config class through the class method `get_config_class`.
     This config class has validates for types and also automatically validates object/func params
     to ensure `script_dir` is present if the param value is string.
@@ -35,16 +35,16 @@
     E.g.,
         from olive.common.pydantic_v1 import validator
 
         def validate_func_param(v, values):
             ...
             return v
 
-        @staticmethod
-        def _validators():
+        @classmethod
+        def _validators(cls):
             return {"validate_func_param": validator("func_param", allow_reuse=True)(validate_func_param)}
     """
 
     registry: ClassVar[Dict[str, Type]] = {}
     name: str = None
     _config_base: Type[ConfigBase] = ConfigBase
 
@@ -55,24 +55,24 @@
         if inspect.isabstract(cls):
             return
         name = cls.name if cls.name is not None else cls.__name__.lower()
         cls.registry[name] = cls
 
     def __init__(self, config: Union[ConfigBase, Dict[str, Any]]) -> None:
         self.config_class = self.get_config_class()
-        self.config = validate_config(config, self._config_base, self.config_class)
+        self.config = validate_config(config, self.config_class)
 
-    @staticmethod
+    @classmethod
     @abstractmethod
-    def _default_config() -> Dict[str, ConfigParam]:
+    def _default_config(cls) -> Dict[str, ConfigParam]:
         """Get the default configuration for the class."""
         raise NotImplementedError
 
-    @staticmethod
-    def _validators() -> Dict[str, Callable]:
+    @classmethod
+    def _validators(cls) -> Dict[str, Callable]:
         """Get ydantic validators for config params."""
         return {}
 
     @classmethod
     def default_config(cls):
         """Get the default configuration."""
         assert not inspect.isabstract(cls), "Cannot get default config for abstract class"
```

## olive/common/config_utils.py

```diff
@@ -282,34 +282,35 @@
 
 
 T = TypeVar("T", bound=ConfigBase)
 
 
 def validate_config(
     config: Union[Dict[str, Any], ConfigBase, None],
-    base_class: Type[T],
-    instance_class: Optional[Type[T]] = None,
+    instance_class: Type[T],
     warn_unused_keys: bool = True,
 ) -> T:
     """Validate a config dictionary or object against a base class and instance class.
 
     instance class is a subclass of base class.
     """
     config = config or {}
 
-    if instance_class is None:
-        instance_class = base_class
-
     if isinstance(config, dict):
         user_keys = set(config.keys())
         config = instance_class(**config)
         config_keys = set(config.dict().keys())
         unused_keys = user_keys - config_keys
         if unused_keys and warn_unused_keys:
             logger.warning("Keys %s are not part of %s. Ignoring them.", unused_keys, instance_class.__name__)
-    elif isinstance(config, base_class) and config.__class__.__name__ == instance_class.__name__:
+    # for dynamically created class by Pydantic create_model, the classes are different even if the class names are same
+    elif (
+        isinstance(config, ConfigBase)
+        and config.__class__.__module__ == instance_class.__module__
+        and config.__class__.__name__ == instance_class.__name__
+    ):
         pass
     else:
         raise ValueError(
             f"Invalid config class. Expected {instance_class.__name__} but got {config.__class__.__name__}"
         )
     return config
```

## olive/common/user_module_loader.py

```diff
@@ -1,11 +1,12 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
+import sys
 from pathlib import Path
 from types import FunctionType, MethodType
 from typing import Any, Callable, Optional, Union
 
 from olive.common.import_lib import import_user_module
 
 
@@ -16,14 +17,15 @@
     """
 
     def __init__(self, user_script: Optional[Union[Path, str]], script_dir: Optional[Union[Path, str]] = None):
         self.user_script = user_script
         self.script_dir = script_dir
         if self.user_script:
             self.user_module = import_user_module(user_script, script_dir)
+            sys.modules[self.user_module.__name__] = self.user_module
         else:
             self.user_module = None
 
     def call_object(self, obj: Union[str, Callable, Any], *args, **kwargs):
         """Call obj with given arguments if it is a function, otherwise just return the object."""
         obj = self.load_object(obj)
         # We check for FunctionType, MethodType here instead of Callable since objects with __call__ methods
```

## olive/common/utils.py

```diff
@@ -22,18 +22,15 @@
 def run_subprocess(cmd, env=None, cwd=None, check=False):
     logger.debug("Running command: %s", cmd)
 
     assert isinstance(cmd, (str, list)), f"cmd must be a string or a list, got {type(cmd)}."
     windows = platform.system() == "Windows"
     if isinstance(cmd, str):
         cmd = shlex.split(cmd, posix=not windows)
-    if windows:
-        path = env.get("PATH") if env else None
-        cmd_exe = shutil.which(cmd[0], path=path)
-        cmd[0] = cmd_exe
+
     try:
         out = subprocess.run(cmd, env=env, cwd=cwd, capture_output=True, check=check)
     except subprocess.CalledProcessError as e:
         err_msg = [
             f"Failed to run {cmd} with returncode {e.returncode}!",
             f"Stderr: {e.stderr.decode('utf-8')}",
             f"Stdout: {e.stdout.decode('utf-8')}",
```

## olive/engine/config.py

```diff
@@ -1,34 +1,34 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from pathlib import Path
-from typing import List, Optional, Union
+from typing import Optional, Union
 
 from olive.azureml.azureml_client import AzureMLClientConfig
 from olive.common.config_utils import ConfigBase
+from olive.common.pydantic_v1 import Extra
 from olive.engine.packaging.packaging_config import PackagingConfig
 from olive.evaluator.olive_evaluator import OliveEvaluatorConfig
 from olive.strategy.search_strategy import SearchStrategyConfig
 from olive.systems.system_config import SystemConfig
 
 # pass search-point was pruned due to failed run
 FAILED_CONFIG = "failed-config"
 # pass search-point was pruned due to invalid config
 INVALID_CONFIG = "invalid-config"
 # list of all pruned configs
 PRUNED_CONFIGS = (FAILED_CONFIG, INVALID_CONFIG)
 
 
-class EngineConfig(ConfigBase):
+class EngineConfig(ConfigBase, extra=Extra.forbid):
     search_strategy: Union[SearchStrategyConfig, bool] = None
     host: SystemConfig = None
     target: SystemConfig = None
-    execution_providers: List[str] = None
     evaluator: OliveEvaluatorConfig = None
     azureml_client_config: Optional[AzureMLClientConfig] = None
     packaging_config: PackagingConfig = None
     cache_dir: Union[Path, str] = ".olive-cache"
     clean_cache: bool = False
     clean_evaluation_cache: bool = False
     plot_pareto_frontier: bool = False
```

## olive/engine/engine.py

```diff
@@ -15,26 +15,25 @@
 from olive.common.config_utils import ConfigBase, validate_config
 from olive.common.utils import hash_dict
 from olive.engine.config import FAILED_CONFIG, INVALID_CONFIG, PRUNED_CONFIGS, EngineConfig
 from olive.engine.footprint import Footprint, FootprintNodeMetric
 from olive.engine.packaging.packaging_generator import generate_output_artifacts
 from olive.evaluator.metric import Metric, MetricResult, joint_metric_key
 from olive.exception import EXCEPTIONS_TO_RAISE, OlivePassError
+from olive.hardware import AcceleratorSpec
 from olive.model import ModelConfig
 from olive.strategy.search_strategy import SearchStrategy
 from olive.systems.common import SystemType
-from olive.systems.local import LocalSystem
 from olive.systems.olive_system import OliveSystem
 from olive.systems.system_config import LocalTargetUserConfig, SystemConfig
-from olive.systems.utils import create_new_system_with_cache
+from olive.systems.utils import create_managed_system_with_cache
 
 if TYPE_CHECKING:
     from olive.engine.packaging.packaging_config import PackagingConfig
     from olive.evaluator.olive_evaluator import OliveEvaluatorConfig
-    from olive.hardware import AcceleratorSpec
     from olive.passes.olive_pass import Pass
 
 logger = logging.getLogger(__name__)
 
 
 class Engine:
     """The engine executes the registered Olive Steps.
@@ -192,15 +191,15 @@
                 name = p.__class__.__name__
                 if idx > 0:
                     name = f"{name}_{idx}"
                 idx += 1
                 if name not in self.passes:
                     break
 
-        if self.no_search and len(p.search_space()) > 0:
+        if self.no_search and len(p.search_space) > 0:
             raise ValueError(f"Search strategy is None but pass {name} has search space")
         if output_name and not self.no_search:
             # In no-search mode, if output_name is provided, the output model of the pass will be saved to
             # engine's output_dir with the prefix of output_name.
             logger.debug("output_name %s for pass %s will be ignored if search strategy is None", output_name, name)
 
         self.passes[name] = {
@@ -223,15 +222,15 @@
             self.pass_flows = pass_flows
 
     def run(
         self,
         input_model_config: ModelConfig,
         accelerator_specs: List["AcceleratorSpec"],
         data_root: str = None,
-        packaging_config: Optional["PackagingConfig"] = None,
+        packaging_config: Optional[Union["PackagingConfig", List["PackagingConfig"]]] = None,
         output_dir: str = None,
         output_name: str = None,
         evaluate_input_model: bool = True,
     ):
         """Run all the registered Olive passes on the input model and produce one or more candidate models.
 
         Args:
@@ -266,15 +265,15 @@
         output_dir: Path = Path(output_dir) if output_dir else Path.cwd()
         output_dir.mkdir(parents=True, exist_ok=True)
 
         outputs = {}
 
         for accelerator_spec in accelerator_specs:
             logger.info("Running Olive on accelerator: %s", accelerator_spec)
-            with self.create_managed_environment(accelerator_spec):
+            with self._create_system(accelerator_spec):
                 run_result = self.run_accelerator(
                     input_model_config,
                     data_root,
                     output_dir,
                     output_name,
                     evaluate_input_model,
                     accelerator_spec,
@@ -294,14 +293,15 @@
             # TODO(trajep): should we support packaging pytorch model?
             logger.info("Package top ranked %d models as artifacts", sum(len(f.nodes) for f in outputs.values()))
             generate_output_artifacts(
                 packaging_config,
                 self.footprints,
                 outputs,
                 output_dir,
+                self.azureml_client_config,
             )
         else:
             logger.info("No packaging config provided, skip packaging artifacts")
 
         return outputs
 
     def run_accelerator(
@@ -368,22 +368,30 @@
 
         output_fp_path = output_dir / f"{prefix_output_name}_footprints.json"
         logger.info("Save footprint to %s.", output_fp_path)
         self.footprints[accelerator_spec].to_file(output_fp_path)
         logger.debug("run_accelerator done")
         return output_footprint
 
+    def get_host_device(self):
+        if self.host_config.config.accelerators:
+            # for host device, we will always use the first accelerator device
+            return self.host_config.config.accelerators[0].device
+        else:
+            return None
+
     def setup_passes(self, accelerator_spec: "AcceleratorSpec"):
+        host_device = self.get_host_device()
         # clean the passes
         self.passes.clear()
         for name, config in self.pass_config.items():
             pass_cls: Type["Pass"] = config["type"]
             pass_cfg = config["config"]
             pass_cfg = pass_cls.generate_search_space(accelerator_spec, pass_cfg, config["disable_search"])
-            p = pass_cls(accelerator_spec, pass_cfg, config["disable_search"])
+            p = pass_cls(accelerator_spec, pass_cfg, config["disable_search"], host_device)
             self.register_pass(
                 p,
                 name=name,
                 host=config["host"],
                 evaluator_config=config["evaluator"],
                 output_name=config["output_name"],
             )
@@ -391,15 +399,15 @@
         # list of passes starting from the first pass with non-empty search space
         # These passes will be added to the search space
         self.pass_flows_search_spaces = []
         for pass_flow in self.pass_flows:
             pass_search_spaces = []
             for pass_name in pass_flow:
                 p: "Pass" = self.passes[pass_name]["pass"]
-                pass_search_spaces.append((pass_name, p.search_space()))
+                pass_search_spaces.append((pass_name, p.search_space))
             self.pass_flows_search_spaces.append(pass_search_spaces)
 
     def reset_passes(self):
         """Cleanup the passes."""
         self.passes.clear()
         self.pass_config.clear()
         self.pass_flows = []
@@ -411,15 +419,15 @@
         data_root: str,
         accelerator_spec: "AcceleratorSpec",
         output_dir: str = None,
         output_name: str = None,
     ):
         """Run all the registered Olive pass flows in no-search mode."""
         for pass_item in self.passes.values():
-            if len(pass_item["pass"].search_space()) > 0:
+            if len(pass_item["pass"].search_space) > 0:
                 pass_name = pass_item["name"]
                 raise ValueError(f"Pass {pass_name} has search space but search strategy is None")
 
         output_models = {}
         for pass_flow in self.pass_flows:
             # search point is empty since there is no search
             passes_to_run = [(pass_id, {}) for pass_id in pass_flow]
@@ -456,27 +464,30 @@
                 output_model_json = cache_utils.save_model(
                     model_number=pass_output_model_id,
                     output_dir=output_dir_with_pf,
                     output_name=f"{pass_output_name}_model",
                     overwrite=True,
                     cache_dir=self._config.cache_dir,
                 )
+                # it is not supported to save compositepytorchmodel/compositemodel again
+                # so the output_model_json could be None
                 output_models[pass_output_model_id] = output_model_json
 
             # save the evaluation results to output_dir
             if signal is not None:
                 results_path = output_dir_with_pf / f"{final_output_name}_metrics.json"
                 with results_path.open("w") as f:
                     json.dump(signal.to_json(), f, indent=4)
 
         output_model_ids = list(output_models.keys())
         fp_outputs = self.footprints[accelerator_spec].create_footprints_by_model_ids(output_model_ids)
         # update the output model config
         for model_id, model_config in output_models.items():
-            fp_outputs.nodes[model_id].model_config = model_config
+            if model_config:
+                fp_outputs.nodes[model_id].model_config = model_config
 
         return fp_outputs
 
     def run_search(
         self,
         input_model_config: ModelConfig,
         input_model_id: str,
@@ -917,24 +928,21 @@
         # run pass
         host = self.host_for_pass(pass_id)
         if host.system_type != SystemType.AzureML:
             input_model_config = self._prepare_non_local_model(input_model_config)
 
         run_start_time = datetime.now().timestamp()
         try:
-            from olive.passes.onnx.perf_tuning import OrtPerfTuning
-
-            # For perf-tuning, we need the model evaluation happens in target even if it is a pass.
-            # TODO(myguo): consider more better way to handle System doesn't support run pass like DockerSystem
-            if (
-                isinstance(p, OrtPerfTuning)
-                and isinstance(host, LocalSystem)
-                and not isinstance(self.target, LocalSystem)
-            ):
-                p.set_target(self.target)
+            if p.run_on_target:
+                if self.target.system_type == SystemType.IsolatedORT:
+                    logger.warning(
+                        "Cannot run pass %s on IsolatedORT target, will use the host to run the pass.", pass_id
+                    )
+                else:
+                    host = self.target
 
             output_model_config = host.run_pass(p, input_model_config, data_root, output_model_path, pass_search_point)
         except OlivePassError:
             logger.exception("Pass run_pass failed")
             output_model_config = FAILED_CONFIG
         except EXCEPTIONS_TO_RAISE:
             # Don't catch these errors since most of time, it is caused by the user errors and need not retry.
@@ -1055,34 +1063,41 @@
         return signal
 
     @staticmethod
     def _get_prefix_output_name(output_name: str, accelerator_spec: "AcceleratorSpec"):
         return f"{output_name}_{accelerator_spec}" if output_name else str(accelerator_spec)
 
     @contextmanager
-    def create_managed_environment(self, accelerator_spec):
+    def _create_system(self, accelerator_spec):
         def create_system(config: "SystemConfig", accelerator_spec):
             assert config, "System config is not provided"
             if config.olive_managed_env:
                 logger.debug(
                     "Creating olive_managed_env %s with EP %s", config.type, accelerator_spec.execution_provider
                 )
-                return create_new_system_with_cache(config, accelerator_spec)
+                return create_managed_system_with_cache(config, accelerator_spec)
             else:
                 logger.debug("create native OliveSystem %s", config.type)
                 return config.create_system()
 
         if not self.target:
             self.target = create_system(self.target_config, accelerator_spec)
         if not self.host:
-            self.host = create_system(self.host_config, accelerator_spec)
+            host_accelerators = self.host_config.config.accelerators
+            if host_accelerators and host_accelerators[0].execution_providers:
+                host_accelerator_spec = AcceleratorSpec(
+                    host_accelerators[0].device, host_accelerators[0].execution_providers[0]
+                )
+            else:
+                host_accelerator_spec = None
+            self.host = create_system(self.host_config, host_accelerator_spec)
 
         yield
 
         if self.target_config.olive_managed_env:
             self.target.remove()
             self.target = None
         if self.host_config.olive_managed_env:
             self.host.remove()
             self.host = None
 
-        create_new_system_with_cache.cache_clear()
+        create_managed_system_with_cache.cache_clear()
```

## olive/engine/packaging/packaging_config.py

```diff
@@ -1,21 +1,56 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from enum import Enum
+from typing import Optional, Union
 
-from olive.common.config_utils import ConfigBase
+from olive.common.config_utils import ConfigBase, validate_config
+from olive.common.pydantic_v1 import validator
 
 
 class PackagingType(str, Enum):
     """Output Artifacts type."""
 
     Zipfile = "Zipfile"
+    AzureMLModels = "AzureMLModels"
+    AzureMLData = "AzureMLData"
+
+
+class CommonPackagingConfig(ConfigBase):
+    export_in_mlflow_format: bool = False
+
+
+class ZipfilePackagingConfig(CommonPackagingConfig):
+    pass
+
+
+class AzureMLDataPackagingConfig(CommonPackagingConfig):
+    version: Union[int, str] = "1"
+    description: Optional[str] = None
+
+
+class AzureMLModelsPackagingConfig(CommonPackagingConfig):
+    version: Union[int, str] = "1"
+    description: Optional[str] = None
+
+
+_type_to_config = {
+    PackagingType.Zipfile: ZipfilePackagingConfig,
+    PackagingType.AzureMLModels: AzureMLModelsPackagingConfig,
+    PackagingType.AzureMLData: AzureMLDataPackagingConfig,
+}
 
 
 class PackagingConfig(ConfigBase):
     """Olive output artifacts generation config."""
 
     type: PackagingType = PackagingType.Zipfile
     name: str = "OutputModels"
-    export_in_mlflow_format: bool = False
+    config: CommonPackagingConfig = None
+
+    @validator("config", pre=True, always=True)
+    def _validate_config(cls, v, values):
+        packaging_type = values.get("type")
+        config_class = _type_to_config.get(packaging_type)
+        return validate_config(v, config_class)
```

## olive/engine/packaging/packaging_generator.py

```diff
@@ -1,226 +1,330 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
-import itertools
 import json
 import logging
 import platform
 import shutil
+import sys
 import tempfile
 import urllib.request
 from collections import OrderedDict
 from pathlib import Path
 from string import Template
-from typing import TYPE_CHECKING, Dict, List
+from typing import TYPE_CHECKING, Dict, List, Union
 
 import pkg_resources
 
-from olive.common.utils import copy_dir, run_subprocess
-from olive.engine.packaging.packaging_config import PackagingConfig, PackagingType
+from olive.common.utils import copy_dir, retry_func, run_subprocess
+from olive.engine.packaging.packaging_config import (
+    AzureMLDataPackagingConfig,
+    AzureMLModelsPackagingConfig,
+    PackagingConfig,
+    PackagingType,
+)
 from olive.model import ONNXModelHandler
 from olive.resource_path import ResourceType, create_resource_path
 from olive.systems.utils import get_package_name_from_ep
 
 if TYPE_CHECKING:
-    from olive.engine.footprint import Footprint
+    from olive.azureml.azureml_client import AzureMLClientConfig
+    from olive.engine.footprint import Footprint, FootprintNode
     from olive.hardware import AcceleratorSpec
 
 logger = logging.getLogger(__name__)
 
 # ruff: noqa: N806
 
 
 def generate_output_artifacts(
-    packaging_config: PackagingConfig,
+    packaging_configs: Union[PackagingConfig, List[PackagingConfig]],
     footprints: Dict["AcceleratorSpec", "Footprint"],
     pf_footprints: Dict["AcceleratorSpec", "Footprint"],
     output_dir: Path,
+    azureml_client_config: "AzureMLClientConfig" = None,
 ):
     if sum(len(f.nodes) if f.nodes else 0 for f in pf_footprints.values()) == 0:
         logger.warning("No model is selected. Skip packaging output artifacts.")
         return
-    if packaging_config.type == PackagingType.Zipfile:
-        _generate_zipfile_output(packaging_config, footprints, pf_footprints, output_dir)
+    packaging_config_list = packaging_configs if isinstance(packaging_configs, list) else [packaging_configs]
+    for packaging_config in packaging_config_list:
+        _package_candidate_models(packaging_config, output_dir, footprints, pf_footprints, azureml_client_config)
 
 
-def _generate_zipfile_output(
+def _package_candidate_models(
     packaging_config: PackagingConfig,
+    output_dir: Path,
     footprints: Dict["AcceleratorSpec", "Footprint"],
     pf_footprints: Dict["AcceleratorSpec", "Footprint"],
-    output_dir: Path,
-) -> None:
-    logger.info("Packaging Zipfile output artifacts")
-    cur_path = Path(__file__).parent
+    azureml_client_config: "AzureMLClientConfig" = None,
+):
+    packaging_type = packaging_config.type
+    output_name = packaging_config.name
+    config = packaging_config.config
+    export_in_mlflow_format = config.export_in_mlflow_format
+
+    logger.info("Packaging output models to %s", packaging_type)
+
     with tempfile.TemporaryDirectory() as temp_dir:
+
         tempdir = Path(temp_dir)
-        _package_sample_code(cur_path, tempdir)
-        _package_models_rank(tempdir, pf_footprints)
+
+        if packaging_type == PackagingType.Zipfile:
+            cur_path = Path(__file__).parent
+            _package_sample_code(cur_path, tempdir)
+            _package_onnxruntime_packages(tempdir, next(iter(pf_footprints.values())))
+
         for accelerator_spec, pf_footprint in pf_footprints.items():
-            if pf_footprint.nodes and footprints[accelerator_spec].nodes:
-                _package_candidate_models(
-                    tempdir,
-                    footprints[accelerator_spec],
-                    pf_footprint,
-                    accelerator_spec,
-                    packaging_config.export_in_mlflow_format,
-                )
-        _package_onnxruntime_packages(tempdir, next(iter(pf_footprints.values())))
-        shutil.make_archive(packaging_config.name, "zip", tempdir)
-        package_file = f"{packaging_config.name}.zip"
-        shutil.move(package_file, output_dir / package_file)
+            footprint = footprints[accelerator_spec]
+            if pf_footprint.nodes and footprint.nodes:
+                model_rank = 1
+                input_node = footprint.get_input_node()
+                for model_id, node in pf_footprint.nodes.items():
+                    model_name = f"{output_name}_{accelerator_spec}_{model_rank}"
+                    if packaging_type == PackagingType.Zipfile:
+                        model_dir = (
+                            tempdir / "CandidateModels" / str(accelerator_spec) / f"BestCandidateModel_{model_rank}"
+                        )
+                    else:
+                        model_dir = tempdir / model_name
 
+                    model_dir.mkdir(parents=True, exist_ok=True)
 
-def _package_models_rank(tempdir, footprints: Dict["AcceleratorSpec", "Footprint"]):
-    metrics_dict = next(iter(footprints.values())).objective_dict
-    sorted_nodes = sorted(
-        itertools.chain.from_iterable(f.nodes.values() for f in footprints.values()),
-        key=lambda x: tuple(
-            x.metrics.value[metric].value if x.metrics.cmp_direction[metric] == 1 else -x.metrics.value[metric].value
-            for metric in metrics_dict
-        ),
-        reverse=True,
+                    # Copy inference config
+                    inference_config_path = model_dir / "inference_config.json"
+                    inference_config = pf_footprint.get_model_inference_config(model_id) or {}
+
+                    _copy_inference_config(inference_config_path, inference_config)
+                    _copy_configurations(model_dir, footprint, model_id)
+                    _copy_metrics(model_dir, input_node, node)
+                    model_path = _save_model(
+                        pf_footprint, model_id, model_dir, inference_config, export_in_mlflow_format
+                    )
+
+                    model_info_list = []
+                    relative_path = str(model_path.relative_to(tempdir))
+                    model_info = _get_model_info(node, model_rank, relative_path, packaging_type)
+                    model_info_list.append(model_info)
+                    _copy_model_info(model_dir, model_info)
+
+                    if packaging_type == PackagingType.AzureMLModels:
+                        _upload_to_azureml_models(azureml_client_config, model_dir, model_name, config)
+                    elif packaging_type == PackagingType.AzureMLData:
+                        _upload_to_azureml_data(azureml_client_config, model_dir, model_name, config)
+
+                model_rank += 1
+
+        if packaging_type == PackagingType.Zipfile:
+            _copy_models_rank(tempdir, model_info_list)
+            _package_zipfile_model(output_dir, output_name, tempdir)
+
+
+def _upload_to_azureml_models(
+    azureml_client_config: "AzureMLClientConfig",
+    model_path: Path,
+    model_name: str,
+    config: AzureMLModelsPackagingConfig,
+):
+    """Upload model to AzureML workspace Models."""
+    from azure.ai.ml.constants import AssetTypes
+    from azure.ai.ml.entities import Model
+    from azure.core.exceptions import ServiceResponseError
+
+    ml_client = azureml_client_config.create_client()
+    model = Model(
+        path=model_path,
+        type=AssetTypes.MLFLOW_MODEL if config.export_in_mlflow_format else AssetTypes.CUSTOM_MODEL,
+        name=model_name,
+        version=str(config.version),
+        description=config.description,
     )
-    rank = 1
-    model_info_list = []
-    for node in sorted_nodes:
-        model_info = {
-            "rank": rank,
-            "model_config": node.model_config,
-            "metrics": node.metrics.value.to_json() if node.metrics else None,
-        }
-        model_info_list.append(model_info)
-        rank += 1
+    retry_func(
+        ml_client.models.create_or_update,
+        [model],
+        max_tries=azureml_client_config.max_operation_retries,
+        delay=azureml_client_config.operation_retry_interval,
+        exceptions=ServiceResponseError,
+    )
+
+
+def _upload_to_azureml_data(
+    azureml_client_config: "AzureMLClientConfig", model_path: Path, model_name: str, config: AzureMLDataPackagingConfig
+):
+    """Upload model as Data to AzureML workspace Data."""
+    from azure.ai.ml.constants import AssetTypes
+    from azure.ai.ml.entities import Data
+    from azure.core.exceptions import ServiceResponseError
+
+    ml_client = azureml_client_config.create_client()
+    data = Data(
+        path=str(model_path),
+        type=AssetTypes.URI_FILE if model_path.is_file() else AssetTypes.URI_FOLDER,
+        description=config.description,
+        name=model_name,
+        version=str(config.version),
+    )
+    retry_func(
+        ml_client.data.create_or_update,
+        [data],
+        max_tries=azureml_client_config.max_operation_retries,
+        delay=azureml_client_config.operation_retry_interval,
+        exceptions=ServiceResponseError,
+    )
+
+
+def _get_model_info(node: "FootprintNode", model_rank: int, relative_path: str, packaging_type: PackagingType):
+    model_config = node.model_config
+    if packaging_type == PackagingType.Zipfile:
+        model_config["config"]["model_path"] = relative_path
+    return {
+        "rank": model_rank,
+        "model_config": model_config,
+        "metrics": node.metrics.value.to_json() if node.metrics else None,
+    }
+
+
+def _copy_models_rank(tempdir: Path, model_info_list: List[Dict]):
     with (tempdir / "models_rank.json").open("w") as f:
         f.write(json.dumps(model_info_list))
 
 
-def _package_sample_code(cur_path, tempdir):
+def _package_sample_code(cur_path: Path, tempdir: Path):
     copy_dir(cur_path / "sample_code", tempdir / "SampleCode")
 
 
-def _package_candidate_models(
-    tempdir,
-    footprint: "Footprint",
+def _package_zipfile_model(output_dir: Path, output_name: str, model_dir: Path):
+    shutil.make_archive(output_name, "zip", model_dir)
+    package_file = f"{output_name}.zip"
+    shutil.move(package_file, output_dir / package_file)
+
+
+def _copy_model_info(model_dir: Path, model_info: Dict):
+    model_info_path = model_dir / "model_info.json"
+    with model_info_path.open("w") as f:
+        json.dump(model_info, f, indent=4)
+
+
+def _copy_inference_config(path: Path, inference_config: Dict):
+    with path.open("w") as f:
+        json.dump(inference_config, f, indent=4)
+
+
+def _copy_configurations(model_dir: Path, footprint: "Footprint", model_id: str):
+    configuration_path = model_dir / "configurations.json"
+    with configuration_path.open("w") as f:
+        json.dump(OrderedDict(reversed(footprint.trace_back_run_history(model_id).items())), f, indent=4)
+
+
+# TODO(xiaoyu): Add target info to metrics file
+def _copy_metrics(model_dir: Path, input_node: "FootprintNode", node: "FootprintNode"):
+    metric_path = model_dir / "metrics.json"
+    if node.metrics:
+        with metric_path.open("w") as f:
+            metrics = {
+                "input_model_metrics": input_node.metrics.value.to_json() if input_node.metrics else None,
+                "candidate_model_metrics": node.metrics.value.to_json(),
+            }
+            json.dump(metrics, f, indent=4)
+
+
+def _save_model(
     pf_footprint: "Footprint",
-    accelerator_spec: "AcceleratorSpec",
-    export_in_mlflow_format=False,
-) -> None:
-    candidate_models_dir = tempdir / "CandidateModels"
-    model_rank = 1
-    input_node = footprint.get_input_node()
-    for model_id, node in pf_footprint.nodes.items():
-        model_dir = candidate_models_dir / str(accelerator_spec) / f"BestCandidateModel_{model_rank}"
-        model_dir.mkdir(parents=True, exist_ok=True)
-        model_rank += 1
-
-        # Copy inference config
-        inference_config_path = model_dir / "inference_config.json"
-        inference_config = pf_footprint.get_model_inference_config(model_id) or {}
-
-        # Add use_ort_extensions to inference config if needed
-        use_ort_extensions = pf_footprint.get_use_ort_extensions(model_id)
-        if use_ort_extensions:
-            inference_config["use_ort_extensions"] = True
-
-        with inference_config_path.open("w") as f:
-            json.dump(inference_config, f)
-
-        # Copy model file
-        model_path = pf_footprint.get_model_path(model_id)
-        model_resource_path = create_resource_path(model_path) if model_path else None
-        model_type = pf_footprint.get_model_type(model_id)
-        if model_type.lower() == "onnxmodel":
-            with tempfile.TemporaryDirectory(dir=model_dir, prefix="olive_tmp") as model_tempdir:
-                # save to model_tempdir first since model_path may be a folder
-                temp_resource_path = create_resource_path(model_resource_path.save_to_dir(model_tempdir, "model", True))
-                # save to model_dir
-                if temp_resource_path.type == ResourceType.LocalFile:
-                    # if model_path is a file, rename it to model_dir / model.onnx
-                    Path(temp_resource_path.get_path()).rename(model_dir / "model.onnx")
-                elif temp_resource_path.type == ResourceType.LocalFolder:
-                    # if model_path is a folder, save all files in the folder to model_dir / file_name
-                    # file_name for .onnx file is model.onnx, otherwise keep the original file name
-                    model_config = pf_footprint.get_model_config(model_id)
-                    onnx_file_name = model_config.get("onnx_file_name")
-                    onnx_model = ONNXModelHandler(temp_resource_path, onnx_file_name)
-                    model_name = Path(onnx_model.model_path).name
-                    for file in Path(temp_resource_path.get_path()).iterdir():
-                        if file.name == model_name:
-                            file_name = "model.onnx"
-                        else:
-                            file_name = file.name
-                        Path(file).rename(model_dir / file_name)
-                if export_in_mlflow_format:
-                    try:
-                        import mlflow
-                    except ImportError:
-                        raise ImportError("Exporting model in MLflow format requires mlflow>=2.4.0") from None
-                    from packaging.version import Version
-
-                    if Version(mlflow.__version__) < Version("2.4.0"):
-                        logger.warning(
-                            "Exporting model in MLflow format requires mlflow>=2.4.0. "
-                            "Skip exporting model in MLflow format."
-                        )
+    model_id: str,
+    saved_model_path: Path,
+    inference_config: Dict,
+    export_in_mlflow_format: bool,
+):
+    model_path = pf_footprint.get_model_path(model_id)
+    model_resource_path = create_resource_path(model_path) if model_path else None
+    model_type = pf_footprint.get_model_type(model_id)
+
+    if model_type.lower() == "onnxmodel":
+        with tempfile.TemporaryDirectory(dir=saved_model_path, prefix="olive_tmp") as model_tempdir:
+            # save to model_tempdir first since model_path may be a folder
+            temp_resource_path = create_resource_path(model_resource_path.save_to_dir(model_tempdir, "model", True))
+            # save to model_dir
+            if temp_resource_path.type == ResourceType.LocalFile:
+                # if model_path is a file, rename it to model_dir / model.onnx
+                Path(temp_resource_path.get_path()).rename(saved_model_path / "model.onnx")
+            elif temp_resource_path.type == ResourceType.LocalFolder:
+                # if model_path is a folder, save all files in the folder to model_dir / file_name
+                # file_name for .onnx file is model.onnx, otherwise keep the original file name
+                model_config = pf_footprint.get_model_config(model_id)
+                onnx_file_name = model_config.get("onnx_file_name")
+                onnx_model = ONNXModelHandler(temp_resource_path, onnx_file_name)
+                model_name = Path(onnx_model.model_path).name
+                for file in Path(temp_resource_path.get_path()).iterdir():
+                    if file.name == model_name:
+                        file_name = "model.onnx"
                     else:
-                        _generate_onnx_mlflow_model(model_dir, inference_config)
+                        file_name = file.name
+                    Path(file).rename(saved_model_path / file_name)
+            if export_in_mlflow_format:
+                _generate_onnx_mlflow_model(saved_model_path, inference_config)
+                return saved_model_path / "mlflow_model"
+            return (
+                saved_model_path
+                if model_resource_path.type == ResourceType.LocalFolder
+                else saved_model_path / "model.onnx"
+            )
 
-        elif model_type.lower() == "openvinomodel":
-            model_resource_path.save_to_dir(model_dir, "model", True)
-        else:
-            raise ValueError(f"Unsupported model type: {model_type} for packaging")
+    elif model_type.lower() == "openvinomodel":
+        model_resource_path.save_to_dir(saved_model_path, "model", True)
+        return saved_model_path
+    else:
+        raise ValueError(
+            f"Unsupported model type: {model_type} for packaging,"
+            " you can set `packaging_config` as None to mitigate this issue."
+        )
 
-        # Copy Passes configurations
-        configuration_path = model_dir / "configurations.json"
-        with configuration_path.open("w") as f:
-            json.dump(OrderedDict(reversed(footprint.trace_back_run_history(model_id).items())), f)
-
-        # Copy metrics
-        # TODO(xiaoyu): Add target info to metrics file
-        if node.metrics:
-            metric_path = model_dir / "metrics.json"
-            with metric_path.open("w") as f:
-                metrics = {
-                    "input_model_metrics": input_node.metrics.value.to_json() if input_node.metrics else None,
-                    "candidate_model_metrics": node.metrics.value.to_json(),
-                }
-                json.dump(metrics, f, indent=4)
 
+def _generate_onnx_mlflow_model(model_dir: Path, inference_config: Dict):
+    try:
+        import mlflow
+    except ImportError:
+        raise ImportError("Exporting model in MLflow format requires mlflow>=2.4.0") from None
+    from packaging.version import Version
+
+    if Version(mlflow.__version__) < Version("2.4.0"):
+        logger.warning(
+            "Exporting model in MLflow format requires mlflow>=2.4.0. Skip exporting model in MLflow format."
+        )
+        return None
 
-def _generate_onnx_mlflow_model(model_dir, inference_config):
-    import mlflow
     import onnx
 
     logger.info("Exporting model in MLflow format")
     execution_mode_mappping = {0: "SEQUENTIAL", 1: "PARALLEL"}
 
     session_dict = {}
     if inference_config.get("session_options"):
         session_dict = {k: v for k, v in inference_config.get("session_options").items() if v is not None}
         if "execution_mode" in session_dict:
             session_dict["execution_mode"] = execution_mode_mappping[session_dict["execution_mode"]]
 
     onnx_model_path = model_dir / "model.onnx"
     model_proto = onnx.load(onnx_model_path)
     onnx_model_path.unlink()
+    mlflow_model_path = model_dir / "mlflow_model"
 
     # MLFlow will save models with default config save_as_external_data=True
     # https://github.com/mlflow/mlflow/blob/1d6eaaa65dca18688d9d1efa3b8b96e25801b4e9/mlflow/onnx.py#L175
     # There will be an aphanumeric file generated in the same folder as the model file
     mlflow.onnx.save_model(
         model_proto,
-        model_dir / "mlflow_model",
+        mlflow_model_path,
         onnx_execution_providers=inference_config.get("execution_provider"),
         onnx_session_options=session_dict,
     )
+    return mlflow_model_path
 
 
-def _package_onnxruntime_packages(tempdir, pf_footprint: "Footprint"):
+def _package_onnxruntime_packages(tempdir: Path, pf_footprint: "Footprint"):
     # pylint: disable=not-an-iterable
     installed_packages = pkg_resources.working_set
     onnxruntime_pkg = [i for i in installed_packages if i.key.startswith("onnxruntime")]
     ort_nightly_pkg = [i for i in installed_packages if i.key.startswith("ort-nightly")]
     is_nightly = bool(ort_nightly_pkg)
     is_stable = bool(onnxruntime_pkg)
 
@@ -245,20 +349,20 @@
             ep_list = inference_settings["execution_provider"]
             package_name_list.extend([get_package_name_from_ep(ep[0]) for ep in ep_list])
             package_name_list = set(package_name_list)
 
     try:
         # Download Python onnxruntime package
         NIGHTLY_PYTHON_COMMAND = Template(
-            "python -m pip download -i "
+            f"{sys.executable} -m pip download -i "
             "https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/ "
             "$package_name==$ort_version --no-deps -d $python_download_path"
         )
         STABLE_PYTHON_COMMAND = Template(
-            "python -m pip download $package_name==$ort_version --no-deps -d $python_download_path"
+            f"{sys.executable} -m pip download $package_name==$ort_version --no-deps -d $python_download_path"
         )
         python_download_path = tempdir / "ONNXRuntimePackages" / "python"
         python_download_path.mkdir(parents=True, exist_ok=True)
         python_download_path = str(python_download_path)
         _download_ort_extensions_package(use_ort_extensions, python_download_path)
 
         if is_nightly:
@@ -305,26 +409,28 @@
             return
         version = onnxruntime_extensions.__version__
         # Hardcode the nightly version number for now until we have a better way to identify nightly version
         if version.startswith("0.8.0."):
             system = platform.system()
             if system == "Windows":
                 download_command = (
-                    "python -m pip download -i "
+                    f"{sys.executable} -m pip download -i "
                     "https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/ "
                     f"onnxruntime-extensions=={version} --no-deps -d {download_path}"
                 )
                 run_subprocess(download_command)
             elif system == "Linux":
                 logger.warning(
                     "ONNXRuntime-Extensions nightly package is not available for Linux. "
                     "Skip packaging ONNXRuntime-Extensions package. Please manually install ONNXRuntime-Extensions."
                 )
         else:
-            download_command = f"python -m pip download onnxruntime-extensions=={version} --no-deps -d {download_path}"
+            download_command = (
+                f"{sys.executable} -m pip download onnxruntime-extensions=={version} --no-deps -d {download_path}"
+            )
             run_subprocess(download_command)
 
 
 def _download_c_packages(package_name_list: List[str], ort_version: str, ort_download_path: str):
     PACKAGE_DOWNLOAD_LINK_MAPPING = {
         "onnxruntime": Template("https://www.nuget.org/api/v2/package/Microsoft.ML.OnnxRuntime/$ort_version"),
         "onnxruntime-gpu": Template("https://www.nuget.org/api/v2/package/Microsoft.ML.OnnxRuntime.Gpu/$ort_version"),
@@ -341,15 +447,15 @@
             urllib.request.urlretrieve(download_link.substitute(ort_version=ort_version), download_path)
         else:
             logger.warning(
                 "Package %s is not available for packaging. Please manually install the package.", package_name
             )
 
 
-def _skip_download_c_package(package_path):
+def _skip_download_c_package(package_path: Path):
     warning_msg = (
         "Found ort-nightly package installed. Please manually download "
         "ort-nightly package from https://aiinfra.visualstudio.com/PublicPackages/_artifacts/feed/ORT-Nightly"
     )
     logger.warning(warning_msg)
     readme_path = package_path / "README.md"
     with readme_path.open("w") as f:
```

## olive/evaluator/metric.py

```diff
@@ -174,25 +174,25 @@
                 metric_config_cls = HuggingfaceMetrics.get_config_class()
         elif values["type"] == MetricType.LATENCY:
             v["higher_is_better"] = v.get("higher_is_better", False)
             metric_config_cls = LatencyMetricConfig
         elif values["type"] == MetricType.THROUGHPUT:
             v["higher_is_better"] = v.get("higher_is_better", True)
             metric_config_cls = ThroughputMetricConfig
-        v["metric_config"] = validate_config(v.get("metric_config", {}), ConfigBase, metric_config_cls)
+        v["metric_config"] = validate_config(v.get("metric_config", {}), metric_config_cls)
 
         return v
 
     @validator("user_config", pre=True, always=True)
     def validate_user_config(cls, v, values):
         if "type" not in values:
             raise ValueError("Invalid type")
 
         user_config_class = get_user_config_class(values["type"])
-        return validate_config(v, ConfigBase, user_config_class)
+        return validate_config(v, user_config_class)
 
 
 class SubMetricResult(ConfigBase):
     value: Union[float, int]
     priority: int
     higher_is_better: bool
```

## olive/evaluator/metric_backend.py

```diff
@@ -13,16 +13,16 @@
 
 class MetricBackend(AutoConfigClass):
     registry: ClassVar[Dict[str, Type["MetricBackend"]]] = {}
 
     def __init__(self, config: Union[ConfigBase, Dict[str, Any]] = None) -> None:
         super().__init__(config)
 
-    @staticmethod
-    def _default_config() -> Dict[str, ConfigParam]:
+    @classmethod
+    def _default_config(cls) -> Dict[str, ConfigParam]:
         return {}
 
     @abstractmethod
     def measure_sub_metric(
         self, model_output: Union[Tuple, NamedTuple], targets: Any, sub_metric: SubMetric
     ) -> SubMetricResult:
         # model_output: (preds, logits)
@@ -56,16 +56,16 @@
         super().__init__(config)
         try:
             import evaluate
         except ImportError:
             raise ImportError("Please install the huggingface/evaluate package to use huggingface metrics.") from None
         self.evaluate_module = evaluate
 
-    @staticmethod
-    def _default_config() -> Dict[str, ConfigParam]:
+    @classmethod
+    def _default_config(cls) -> Dict[str, ConfigParam]:
         return {
             "load_params": ConfigParam(
                 type_=Dict[str, Any], default_value=None, description="The parameters to load the metric."
             ),
             "compute_params": ConfigParam(
                 type_=Dict[str, Any], default_value=None, description="The parameters to compute the metric."
             ),
```

## olive/hardware/accelerator.py

```diff
@@ -57,27 +57,27 @@
     accelerator_type=Device.GPU, execution_provider="TensorrtExecutionProvider"
 )
 
 
 class AcceleratorLookup:
     @staticmethod
     def get_managed_supported_execution_providers(device: Device):
-        return DEVICE_TO_EXECUTION_PROVIDERS.get(device)
+        return [*DEVICE_TO_EXECUTION_PROVIDERS.get(device), "CPUExecutionProvider"]
 
     @staticmethod
     def get_execution_providers_for_device(device: Device):
         import onnxruntime
 
         return AcceleratorLookup.get_execution_providers_for_device_by_available_providers(
             device, onnxruntime.get_available_providers()
         )
 
     @staticmethod
     def get_execution_providers_for_device_by_available_providers(device: Device, available_providers):
-        eps_per_device = DEVICE_TO_EXECUTION_PROVIDERS.get(device)
+        eps_per_device = [*DEVICE_TO_EXECUTION_PROVIDERS.get(device), "CPUExecutionProvider"]
         return AcceleratorLookup.get_execution_providers(eps_per_device, available_providers)
 
     @staticmethod
     def get_execution_providers(execution_providers, available_providers):
         eps = AcceleratorLookup.filter_execution_providers(execution_providers, available_providers)
         return eps or available_providers
 
@@ -88,129 +88,238 @@
 
         assert isinstance(execution_providers, list)
         assert isinstance(available_providers, list)
 
         return [ep for ep in available_providers if ep in execution_providers]
 
     @staticmethod
-    def infer_accelerators_from_execution_provider(execution_provider: List[str]):
+    def infer_devices_from_execution_providers(execution_providers: List[str]):
         """Infer the device from the execution provider name.
 
         If all the execution provider is uniquely mapped to a device, return the device list.
         Otherwise, return None.
+        Please note that the CPUExecutionProvider is skipped for device infer. And only other ORT EPs are considered.
         For example:
             execution_provider = ["CPUExecutionProvider", "CUDAExecutionProvider"]
-            return None (CPUExecutionProvider is mapped to CPU and GPU, Olive cannot infer the device)
+            return ["gpu"]
             execution_provider = ["CUDAExecutionProvider", "TensorrtExecutionProvider"]
             return ["gpu"]
         """
-        if not execution_provider:
+        if not execution_providers:
             return None
 
-        is_unique_inferring = True
-        accelerators = []
-        for idx, ep in enumerate(execution_provider):
-            accelerators.append([])
-            for accelerator, eps in DEVICE_TO_EXECUTION_PROVIDERS.items():
+        ep_to_devices = {}
+        for ep in execution_providers:
+            if ep == "CPUExecutionProvider":
+                # cannot infer device for CPUExecutionProvider since all ORT EP supports CPU
+                continue
+
+            inferered_devices = []
+            for device, eps in DEVICE_TO_EXECUTION_PROVIDERS.items():
                 if ep in eps:
-                    accelerators[idx].append(accelerator)
-                    if len(accelerators[idx]) > 1:
-                        logger.warning(
-                            "Execution provider %s is mapped to multiple accelerators %s. "
-                            "Olive cannot infer the device which may cause unexpected behavior. "
-                            "Please specify the accelerator in the accelerator configs",
-                            ep,
-                            accelerators[idx],
-                        )
-                        is_unique_inferring = False
+                    inferered_devices.append(device)
+            if inferered_devices:
+                ep_to_devices[ep] = inferered_devices
+            else:
+                ep_to_devices[ep] = None
+
+        mapped_devices = []
+        for ep, inferred_device in ep_to_devices.items():
+            if inferred_device is None:
+                logger.warning(
+                    "Execution provider %s is not able to be mapped to any device. "
+                    "Olive cannot infer the device which may cause unexpected behavior. "
+                    "Please specify the accelerator in the accelerator configs",
+                    ep,
+                )
+                return None
+            elif len(inferred_device) > 1:
+                logger.warning(
+                    "Execution provider %s is mapped to multiple devices %s. "
+                    "Olive cannot infer the device which may cause unexpected behavior. "
+                    "Please specify the accelerator in the accelerator configs",
+                    ep,
+                    inferred_device,
+                )
+                return None
+            else:
+                if inferred_device[0] not in mapped_devices:
+                    mapped_devices.append(inferred_device[0])
+        return mapped_devices if mapped_devices else None
+
+    @staticmethod
+    def infer_single_device_from_execution_providers(execution_providers: List[str]) -> str:
+        if not execution_providers:
+            return None
+
+        if execution_providers == ["CPUExecutionProvider"]:
+            inferred_devices = ["cpu"]
+        else:
+            inferred_devices = AcceleratorLookup.infer_devices_from_execution_providers(execution_providers)
+            assert inferred_devices, (
+                f"Cannot infer the devices from the execution providers {execution_providers}."
+                " Please specify the device in the accelerator configs."
+            )
+            assert len(inferred_devices) == 1, (
+                f"Cannot infer the devices from the execution providers {execution_providers}. "
+                f"Multiple devices are inferred: {inferred_devices}."
+                " Please specify the device in the accelerator configs."
+            )
+
+        return inferred_devices[0]
 
-        if is_unique_inferring:
-            return list({accelerator[0] for accelerator in accelerators})
-        return None
 
+def normalize_accelerators(system_config: "SystemConfig", skip_supported_eps_check: bool = True) -> "SystemConfig":
+    """Normalize the accelerators in the system config.
 
-def create_accelerators(
-    system_config: "SystemConfig", execution_providers: List[str] = None, skip_supported_eps_check: bool = True
-) -> List[AcceleratorSpec]:
+    * the accelerators is not specified, infer the device/ep based on the installed ORT in case of local/python system.
+    * only device is specified, infer the execution providers based on the installed ORT in case of local/python system.
+    * only EP is specified, infer the device based on the installed ORT in case of local/python system.
+    * For AzureML and Docker system, the accelerators and execution providers must be specified.
+    """
     from olive.systems.common import SystemType
 
     if system_config.olive_managed_env:
-        if not execution_providers:
-            raise ValueError("Managed environment requires execution providers to be specified.")
+        if not system_config.config.accelerators:
+            raise ValueError("Managed environment requires accelerators to be specified.")
+
+        for accelerator in system_config.config.accelerators:
+            if not accelerator.execution_providers:
+                raise ValueError(
+                    f"Managed environment requires execution providers to be specified for {accelerator.device}"
+                )
     else:
-        system_supported_eps = None
         if system_config.type in (SystemType.Local, SystemType.PythonEnvironment, SystemType.IsolatedORT):
             target = system_config.create_system()
+            # TODO(myguo): Handle the ORT not installed scenario. In this case, the call will raise ImportError.
+            # and the system_supported_eps will be None.
             system_supported_eps = target.get_supported_execution_providers()
-
-    if not execution_providers:
-        if system_config.type == SystemType.AzureML:
-            # verify the AzureML system have specified the execution providers
-            # Please note we could not use isinstance(target, AzureMLSystem) since it would import AzureML packages.
-            raise ValueError("AzureMLSystem requires execution providers to be specified.")
-        elif system_config.type == SystemType.Docker:
-            # for docker system we default use CPUExecutionProvider
-            raise ValueError("DockerSystem requires execution providers to be specified.")
-        elif system_config.type in (SystemType.Local, SystemType.PythonEnvironment, SystemType.IsolatedORT):
-            execution_providers = system_supported_eps
-    logger.debug("Initial execution providers: %s", execution_providers)
-
-    accelerators: List[str] = system_config.config.accelerators
-    if accelerators is None:
-        inferred_accelerators = AcceleratorLookup.infer_accelerators_from_execution_provider(execution_providers)
-        if not inferred_accelerators:
-            logger.warning("Cannot infer the accelerators from the target system. Use CPU as default.")
-            accelerators = ["CPU"]
+            # Remove the AzureMLExecutionProvider
+            if "AzureExecutionProvider" in system_supported_eps:
+                system_supported_eps.remove("AzureExecutionProvider")
+
+            assert system_supported_eps, "No supported execution providers found for the target system."
+
+            if not system_config.config.accelerators:
+                # User does not specify the accelerators.
+                inferred_device = AcceleratorLookup.infer_single_device_from_execution_providers(system_supported_eps)
+                # here the pydantic validate_assignment will initialize the accelerator instances
+                system_config.config.accelerators = [
+                    {"device": inferred_device, "execution_providers": system_supported_eps}
+                ]
+                logger.info(
+                    "There is no any accelerator specified. Inferred accelerators: %s",
+                    system_config.config.accelerators,
+                )
+            else:
+                for accelerator in system_config.config.accelerators:
+                    if not accelerator.device:
+                        # User does not specify the device but providing the execution providers
+                        assert accelerator.execution_providers, "The execution providers are not specified."
+                        inferred_device = AcceleratorLookup.infer_single_device_from_execution_providers(
+                            accelerator.execution_providers
+                        )
+                        logger.info("the accelerator device is not specified. Inferred device: %s.", inferred_device)
+                        accelerator.device = inferred_device
+                    elif not accelerator.execution_providers:
+                        # User specify the device but missing the execution providers
+                        execution_providers = (
+                            AcceleratorLookup.get_execution_providers_for_device_by_available_providers(
+                                accelerator.device.lower(), system_supported_eps
+                            )
+                        )
+                        accelerator.execution_providers = execution_providers
+                        filtered_eps = [ep for ep in system_supported_eps if ep not in execution_providers]
+                        if filtered_eps:
+                            logger.warning(
+                                "The following execution providers are filtered: %s. "
+                                "Please raise issue in Olive site since it might be a bug. ",
+                                ",".join(filtered_eps),
+                            )
+
+                        logger.info(
+                            "The accelerator execution providers is not specified for %s. Use the inferred ones. %s",
+                            accelerator.device,
+                            accelerator.execution_providers,
+                        )
+                    else:
+                        logger.debug("The accelerator device and execution providers are specified, skipping deduce.")
         else:
-            logger.debug(
-                "User inferred accelerators %s from given execution providers %s.", accelerators, execution_providers
-            )
-            accelerators = inferred_accelerators
-    logger.debug("Initial accelerators: %s", accelerators)
+            # for AzureML and Docker System
+            if not system_config.config.accelerators:
+                raise ValueError("AzureML and Docker system requires accelerators to be specified.")
+            for accelerator in system_config.config.accelerators:
+                if not accelerator.device or not accelerator.execution_providers:
+                    raise ValueError(
+                        "AzureML and Docker system requires device and execution providers to be specified explicitly."
+                    )
+
+    ep_not_supported = []
+    for accelerator in system_config.config.accelerators:
+        device = Device(accelerator.device.lower())
 
-    seen = set()
-    ep_to_process = [ep for ep in execution_providers if ep not in seen and not seen.add(ep)]
-
-    # Flatten the accelerators to list of AcceleratorSpec
-    accelerator_specs: List[AcceleratorSpec] = []
-    is_cpu_available = "cpu" in [accelerator.lower() for accelerator in accelerators]
-    for accelerator in accelerators:
-        device = Device(accelerator.lower())
         if system_config.olive_managed_env:
             available_eps = AcceleratorLookup.get_managed_supported_execution_providers(device)
         elif (
             system_config.type in (SystemType.Local, SystemType.PythonEnvironment, SystemType.IsolatedORT)
             and not skip_supported_eps_check
         ):
             # don't need to check the supported execution providers if there is no evaluation
             # target is only used for evaluation
             available_eps = system_supported_eps
         else:
-            available_eps = execution_providers
+            available_eps = accelerator.execution_providers
 
         supported_eps = AcceleratorLookup.get_execution_providers_for_device_by_available_providers(
             device, available_eps
         )
         logger.debug("Supported execution providers for device %s: %s", device, supported_eps)
-        for ep in ep_to_process.copy():
+
+        eps = []
+        for ep in accelerator.execution_providers:
+            if ep not in supported_eps:
+                ep_not_supported.append(ep)
+            else:
+                eps.append(ep)
+
+        # remove the unsupported execution providers
+        accelerator.execution_providers = eps
+
+    if ep_not_supported:
+        logger.warning(
+            "The following execution providers are not supported: %s. "
+            "Please consider installing an onnxruntime build that contains the relevant execution providers. ",
+            ",".join(ep_not_supported),
+        )
+    return system_config
+
+
+def create_accelerators(system_config: "SystemConfig", skip_supported_eps_check: bool = True) -> List[AcceleratorSpec]:
+    system_config = normalize_accelerators(system_config, skip_supported_eps_check)
+
+    device_to_eps = {}
+    for accelerator in system_config.config.accelerators:
+        device_to_eps[accelerator.device] = accelerator.execution_providers
+    logger.debug("Initial accelerators and execution providers: %s", device_to_eps)
+
+    # Flatten the accelerators to list of AcceleratorSpec
+    accelerator_specs: List[AcceleratorSpec] = []
+    is_cpu_available = "cpu" in [accelerator.lower() for accelerator in device_to_eps]
+    for accelerator in system_config.config.accelerators:
+        device = Device(accelerator.device.lower())
+        for ep in accelerator.execution_providers:
             if ep == "CPUExecutionProvider" and device != "cpu" and is_cpu_available:
-                logger.info("Ignore the CPUExecutionProvider for non-cpu device since cpu accelerator is also present.")
-            elif ep in supported_eps:
+                logger.warning(
+                    "Ignore the CPUExecutionProvider for non-cpu device since cpu accelerator is also present."
+                )
+            else:
                 accelerator_specs.append(AcceleratorSpec(device, ep))
-                ep_to_process.remove(ep)
 
     assert accelerator_specs, (
         "No valid accelerator specified for target system. "
         "Please specify the accelerators in the target system or provide valid execution providers. "
-        f"Given execution providers: {execution_providers}. "
-        f"Current accelerators: {accelerators}."
+        f"Given execution providers: {device_to_eps.values()}. "
+        f"Current accelerators: {device_to_eps.keys()}."
         f"Supported execution providers: {DEVICE_TO_EXECUTION_PROVIDERS}."
     )
     logger.info("Running workflow on accelerator specs: %s", ",".join([str(spec) for spec in accelerator_specs]))
-    if ep_to_process:
-        logger.warning(
-            "The following execution provider is not supported: %s. "
-            "Please consider installing an onnxruntime build that contains the relevant execution providers. ",
-            ",".join(ep_to_process),
-        )
-
     return accelerator_specs
```

## olive/hardware/constants.py

```diff
@@ -16,19 +16,18 @@
     "TensorrtExecutionProvider": ("onnxruntime-gpu", "ort-nightly-gpu"),
     "ROCMExecutionProvider": ("onnxruntime-gpu", "ort-nightly-gpu"),
     "OpenVINOExecutionProvider": ("onnxruntime-openvino", None),
     "DmlExecutionProvider": ("onnxruntime-directml", "ort-nightly-directml"),
 }
 
 DEVICE_TO_EXECUTION_PROVIDERS = {
-    "cpu": ["CPUExecutionProvider", "OpenVINOExecutionProvider"],
+    "cpu": ["OpenVINOExecutionProvider"],
     "gpu": [
         "DmlExecutionProvider",
         "CUDAExecutionProvider",
         "ROCMExecutionProvider",
         "MIGraphXExecutionProvider",
         "TensorrtExecutionProvider",
-        "CPUExecutionProvider",
         "OpenVINOExecutionProvider",
     ],
-    "npu": ["QNNExecutionProvider", "CPUExecutionProvider"],
+    "npu": ["QNNExecutionProvider"],
 }
```

## olive/model/config/hf_config.py

```diff
@@ -199,15 +199,18 @@
             ) from e
 
         # return unused kwargs doesn't work in catching unused args in config_dict
         # they just call config_cls(**config_dict), extras get absorbed in **kwargs
         config = config_cls.from_dict(config_dict, return_unused_kwargs=False)
         # we will do a manual check to see if there are unused kwargs
         # this works since config_cls is created as a dataclass
-        extras = set(config_dict.keys()) - set(config.__dict__.keys())
+        extras = set()
+        for k in config_dict:
+            if not hasattr(config, k):
+                extras.add(k)
         if extras:
             logger.warning("Unused kwargs in quantization_config: %s. Ignoring them", extras)
         return config
 
 
 class HfConfig(ConfigBase):
     """The config for HuggingFace models.
```

## olive/model/handler/pytorch.py

```diff
@@ -17,15 +17,15 @@
 from olive.common.utils import copy_dir
 from olive.constants import Framework, ModelFileFormat
 from olive.hardware.accelerator import Device
 from olive.model.config import HfComponent, HfConfig, IoConfig
 from olive.model.config.registry import model_handler_registry
 from olive.model.handler.base import OliveModelHandler
 from olive.model.handler.mixin import DummyInputsMixin, HfConfigMixin
-from olive.model.utils.hf_utils import huggingface_model_loader
+from olive.model.utils.hf_utils import load_hf_model_from_model_class
 from olive.resource_path import OLIVE_RESOURCE_ANNOTATIONS, ResourceType, create_resource_path
 
 logger = logging.getLogger(__name__)
 
 
 @model_handler_registry("PyTorchModel")
 class PyTorchModelHandler(OliveModelHandler, HfConfigMixin, DummyInputsMixin):  # pylint: disable=too-many-ancestors
@@ -122,17 +122,19 @@
     def adapter_path(self) -> str:
         return self.get_resource("adapter_path")
 
     def load_model(self, rank: int = None) -> torch.nn.Module:
         if self.model is not None:
             return self.model
 
+        # Load user module at the beginning since we may need user defined models to load model
+        user_module_loader = UserModuleLoader(self.model_script, self.script_dir)
+
         # Load special path or format model -> load model from hf config -> load normal path model
         if self.model_loader is not None:
-            user_module_loader = UserModuleLoader(self.model_script, self.script_dir)
             model = user_module_loader.call_object(self.model_loader, self.model_path)
         elif self.model_file_format == ModelFileFormat.PYTORCH_TORCH_SCRIPT:
             model = torch.jit.load(self.model_path)
         elif self.model_file_format == ModelFileFormat.PYTORCH_MLFLOW_MODEL:
             model = self._load_mlflow_model()
         elif self.hf_config and (self.hf_config.model_class or self.hf_config.task):
             model = self.load_hf_model(self.model_path)
@@ -213,17 +215,16 @@
                     hf_pretrained_class = flavors["hftransformersv2"].get("hf_pretrained_class", "AutoModel")
                 elif "hftransformers" in flavors:
                     hf_pretrained_class = flavors["hftransformers"].get("hf_pretrained_class", "AutoModel")
                 else:
                     raise ValueError(
                         "Unsupported MLFlow model flavor. Currently only support hftransformersv2/hftransformers."
                     )
-
-            model_loader = huggingface_model_loader(hf_pretrained_class)
-            loaded_model = model_loader(tmp_dir)
+            loading_args = self.hf_config.get_loading_args_from_pretrained() if self.hf_config else {}
+            loaded_model = load_hf_model_from_model_class(hf_pretrained_class, tmp_dir, **loading_args)
             loaded_model.eval()
             return loaded_model
 
     def to_json(self, check_object: bool = False):
         config = super().to_json(check_object)
         # only keep model_attributes that are not in hf_config
         if self.model_attributes and self.hf_config:
```

## olive/model/handler/mixin/dummy_inputs.py

```diff
@@ -9,15 +9,15 @@
 
 logger = logging.getLogger(__name__)
 
 
 class DummyInputsMixin:
     """Provide the dummy inputs functionality for the model handler.
 
-    the dummy data is used to evalaute the latency if user doesn't provide the data for evaluation.
+    the dummy data is used to evaluate the latency if user doesn't provide the data for evaluation.
     """
 
     def get_dummy_inputs(self):
         """Return a dummy input for the model."""
         if self.dummy_inputs is not None:
             return self.dummy_inputs
```

## olive/model/handler/mixin/hf_config.py

```diff
@@ -1,13 +1,13 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import logging
-from typing import TYPE_CHECKING, List, Optional, Tuple
+from typing import TYPE_CHECKING, Generator, Optional, Tuple
 
 from olive.constants import ModelFileFormat
 from olive.model.utils.hf_utils import (
     get_hf_model_config,
     get_hf_model_dummy_input,
     get_hf_model_io_config,
     load_hf_model_from_model_class,
@@ -52,15 +52,15 @@
                 self.hf_config.task,
                 self.hf_config.feature,
                 **self.hf_config.get_loading_args_from_pretrained(),
             )
         else:
             return None
 
-    def get_hf_components(self, rank: Optional[int] = None) -> List[Tuple[str, "PyTorchModelHandler"]]:
+    def get_hf_components(self, rank: Optional[int] = None) -> Generator[Tuple[str, "PyTorchModelHandler"], None, None]:
         if self.hf_config and self.hf_config.components:
             for component in self.hf_config.components:
                 yield component.name, self.get_component_model(component, rank)
 
     def load_hf_model(self, model_path: str = None):
         """Load model from model_path or model_name."""
         model_name_or_path = model_path or self.hf_config.model_name
@@ -74,15 +74,15 @@
             raise ValueError("Either task or model_class must be specified")
 
         return model
 
     def get_hf_dummy_inputs(self):
         """Get dummy inputs for the model."""
         return get_hf_model_dummy_input(
-            self.model_path or self.hf_config.model_name,
+            self._get_model_path_or_name(),
             self.hf_config.task,
             self.hf_config.feature,
             **self.hf_config.get_loading_args_from_pretrained(),
         )
 
     def is_model_loaded_from_hf_config(self) -> bool:
         """Return True if the model is loaded from hf_config, False otherwise."""
```

## olive/model/utils/hf_utils.py

```diff
@@ -67,15 +67,15 @@
 
 
 def load_hf_model_from_model_class(model_class: str, name: str, **kwargs):
     """Load huggingface model from model_loader and name."""
     return huggingface_model_loader(model_class)(name, **kwargs)
 
 
-# patched version of transforrmers.onnx.features.supported_features_mapping
+# patched version of transformers.onnx.features.supported_features_mapping
 # to support additional models in olive
 def patched_supported_features_mapping(*supported_features: str, onnx_config_cls: str = None) -> Dict[str, Callable]:
     """Generate the mapping between supported the features and their corresponding OnnxConfig for a given model.
 
     Args:
         *supported_features: The names of the supported features.
         onnx_config_cls: The OnnxConfig full name corresponding to the model.
@@ -135,14 +135,20 @@
     return FeaturesManager.get_config(model_type, feature)(config)
 
 
 def get_hf_model_io_config(model_name: str, task: str, feature: Optional[str] = None, **kwargs):
     model_config = get_onnx_config(model_name, task, feature, **kwargs)
     inputs = model_config.inputs
     outputs = model_config.outputs
+    if not inputs or not outputs:
+        # just log a warning and return None, since this is not a critical error
+        # and following pass may not use the io_config, like OptimumConversion
+        logger.warning("No inputs or outputs found from model %s", model_config)
+        return None
+
     io_config = {}
     io_config["input_names"] = list(inputs.keys())
     io_config["output_names"] = list(outputs.keys())
     io_config["dynamic_axes"] = dict(chain(inputs.items(), outputs.items()))
     return io_config
```

## olive/passes/__init__.py

```diff
@@ -1,19 +1,15 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from olive.passes.olive_pass import FullPassConfig, Pass
-from olive.passes.onnx import *  # noqa: F403
-from olive.passes.openvino import *  # noqa: F403
-from olive.passes.pass_config import PassParamDefault
-from olive.passes.pytorch import *  # noqa: F403
-from olive.passes.qnn import *  # noqa: F403
-from olive.passes.snpe import *  # noqa: F403
+from olive.passes.pass_config import PassModuleConfig, PassParamDefault
 
 REGISTRY = Pass.registry
 
 __all__ = [
     "Pass",
     "PassParamDefault",
+    "PassModuleConfig",
     "FullPassConfig",
 ]
```

## olive/passes/olive_pass.py

```diff
@@ -5,15 +5,14 @@
 import inspect
 import logging
 from abc import ABC, abstractmethod
 from pathlib import Path
 from typing import Any, Callable, ClassVar, Dict, Optional, Tuple, Type, Union, get_args
 
 from olive.common.config_utils import ConfigBase, ParamCategory, validate_config
-from olive.common.pydantic_v1 import validator
 from olive.common.user_module_loader import UserModuleLoader
 from olive.data.config import DataConfig
 from olive.hardware import DEFAULT_CPU_ACCELERATOR, AcceleratorSpec
 from olive.model import CompositeModelHandler, DistributedOnnxModelHandler, OliveModelHandler
 from olive.passes.pass_config import (
     PassConfigBase,
     PassConfigParam,
@@ -46,48 +45,60 @@
     registry: ClassVar[Dict[str, Type["Pass"]]] = {}
     # True if pass configuration requires user script for non-local host support
     _requires_user_script: bool = False
     # True if the pass processes a composite model at once. Otherwise, the components of the
     # composite model will be processed individually.
     _accepts_composite_model: bool = False
 
+    # Flag indicate whether the pass need to be run in target instead of host
+    run_on_target: bool = False
+
     @classmethod
     def __init_subclass__(cls, **kwargs) -> None:
         """Register the Pass."""
         super().__init_subclass__(**kwargs)
         if not inspect.isabstract(cls):
             cls.registry[cls.__name__.lower()] = cls
 
     def __init__(
-        self, accelerator_spec: AcceleratorSpec, config: Dict[str, Any], disable_search: Optional[bool] = False
+        self,
+        accelerator_spec: AcceleratorSpec,
+        config: Dict[str, Any],
+        disable_search: Optional[bool] = False,
+        host_device=None,
     ):
         """Initialize the pass.
 
-        :param config_class: the PassConfig class with the default value or default search values.
-        :type config_class: Type[PassConfigBase]
+        :param accelerator_spec: the accelerator spec for the pass.
+        :type accelerator_spec: AcceleratorSpec
         :param config: the configuration representing search space.
         :type config: Dict[str, Any]
+        :param disable_search: whether to disable search.
+        :type disable_search: Optional[bool]
+        :param host_device: the host device for the pass.
+        :type host_device: Optional[str]
         """
         assert accelerator_spec is not None, "Please specify the accelerator spec for the pass."
         assert config is not None, "Please specify the configuration for the pass."
 
         config_class, default_config = self.get_config_class(accelerator_spec, disable_search)
 
         self.accelerator_spec = accelerator_spec
+        self.host_device = host_device
 
         self._config_class = config_class
         self.config = config
         if self._requires_user_script:
             self._user_module_loader = UserModuleLoader(self.config["user_script"], self.config["script_dir"])
 
         self._fixed_params = {}
-        self._search_space = {}
+        self.search_space = {}
         for k, v in self.config.items():
             if isinstance(v, SearchParameter):
-                self._search_space[k] = v
+                self.search_space[k] = v
             else:
                 self._fixed_params[k] = v
 
         # Params that are paths [(param_name, required)]
         self.path_params = []
         for param, param_config in default_config.items():
             if param_config.category in (ParamCategory.PATH, ParamCategory.DATA):
@@ -113,15 +124,15 @@
     ) -> Tuple[Type[PassConfigBase], Dict[str, Any]]:
         """Generate search space for the pass."""
         assert accelerator_spec is not None, "Please specify the accelerator spec for the pass"
 
         # Get the config class with default value or default search value
         config_class, default_config = cls.get_config_class(accelerator_spec, disable_search)
         # Generate the search space by using both default value and default search value and user provided config
-        config = validate_config(config, PassConfigBase, config_class)
+        config = validate_config(config, config_class)
 
         config = cls._resolve_config(config, default_config)
         return cls._init_fixed_and_search_params(config, default_config)
 
     @classmethod
     def get_config_class(cls, accelerator_spec: AcceleratorSpec, disable_search: Optional[bool] = False):
         """Get the configuration class for the pass."""
@@ -144,22 +155,96 @@
             if param.endswith("data_config"):
                 param_type = param_config.type_
                 assert param_type == DataConfig or DataConfig in get_args(
                     param_type
                 ), f"{param} ending with data_config must be of type DataConfig."
         return config
 
-    @staticmethod
-    def _validators() -> Dict[str, Callable]:
+    def config_at_search_point(self, point: Dict[str, Any]) -> Dict[str, Any]:
+        """Get the configuration for the pass at a specific point in the search space."""
+        assert set(point.keys()) == set(self.search_space.keys()), "Search point is not in the search space."
+        config = self._fixed_params.copy()
+        for key, value in point.items():
+            config[key] = value
+        return self._config_class(**config).dict()
+
+    def validate_search_point(
+        self, search_point: Dict[str, Any], accelerator_spec: AcceleratorSpec, with_fixed_value: bool = False
+    ) -> bool:
+        """Validate the search point for the pass."""
+        return True
+
+    def run(
+        self, model: OliveModelHandler, data_root: str, output_model_path: str, point: Optional[Dict[str, Any]] = None
+    ) -> OliveModelHandler:
+        """Run the pass on the model at a specific point in the search space."""
+        point = point or {}
+        config = self.config_at_search_point(point)
+
+        if not self._initialized:
+            self._initialize()
+            self._initialized = True
+
+        # Optimization pass still works on individual graphs.
+        if isinstance(model, DistributedOnnxModelHandler):
+            for rank in range(model.num_ranks):
+                input_ranked_model = model.load_model(rank)
+                ranked_output_path = Path(output_model_path).with_suffix("") / model.ranked_model_name(rank)
+                self._run_for_config(input_ranked_model, data_root, config, str(ranked_output_path))
+
+            output_model = DistributedOnnxModelHandler(
+                model_path=str(Path(output_model_path).with_suffix("")),
+                model_name_pattern=model.model_name_pattern,
+                num_ranks=model.num_ranks,
+                inference_settings=model.inference_settings,
+            )
+        elif isinstance(model, CompositeModelHandler) and not self._accepts_composite_model:
+            # CompositePyTorchModel is also handled here.
+            components = []
+            component_names = []
+            for component_name, component_model in model.get_model_components():
+                component_output_path = Path(output_model_path).with_suffix("") / component_name
+                output_model_component = self._run_for_config(
+                    component_model, data_root, config, str(component_output_path)
+                )
+                output_model_component.model_attributes = (
+                    output_model_component.model_attributes or component_model.model_attributes
+                )
+                components.append(output_model_component)
+                component_names.append(component_name)
+            output_model = CompositeModelHandler(components, component_names)
+        else:
+            output_model = self._run_for_config(model, data_root, config, output_model_path)
+        # assumption: the model attributes from passes, if any, are more important than
+        # the input model attributes, we should not update/extend anymore outside of the pass run
+        output_model.model_attributes = output_model.model_attributes or model.model_attributes
+        return output_model
+
+    def serialize_config(self, config: Dict[str, Any], check_object: bool = False) -> str:
+        """Serialize the configuration."""
+        return self._config_class(**config).to_json(check_object)
+
+    def to_json(self, check_object: bool = False) -> Dict[str, Any]:
+        """Convert the pass to json."""
+        return {
+            "type": self.__class__.__name__,
+            "disable_search": True,
+            "accelerator": self.accelerator_spec.to_json(),
+            "host_device": self.host_device,
+            "config": self.serialize_config(self.config, check_object),
+        }
+
+    @classmethod
+    def _validators(cls) -> Dict[str, Callable]:
         """Pydantic validators for config params."""
         return {}
 
-    @staticmethod
+    @classmethod
     @abstractmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         """Get the default configuration for the pass. Doesn't include user_script and script_dir.
 
         Example:
             return {
                 # required parameter
                 "param1": PassConfigParam(type_=int, required=True, description="param1 description"),
                 # optional parameter with default value
@@ -301,129 +386,47 @@
             user_module_loader = UserModuleLoader(config["user_script"], config["script_dir"])
             config = cls._validate_user_script(config, user_module_loader, default_config)
         return config
 
     def _initialize(self):
         """Initialize the pass. Pass specific initialization should be done here."""
 
-    def search_space(self) -> Dict[str, SearchParameter]:
-        """Get the search space for the pass."""
-        return self._search_space
-
-    def config_at_search_point(self, point: Dict[str, Any]) -> Dict[str, Any]:
-        """Get the configuration for the pass at a specific point in the search space."""
-        assert set(point.keys()) == set(self._search_space.keys()), "Search point is not in the search space."
-        config = self._fixed_params.copy()
-        for key, value in point.items():
-            config[key] = value
-        return self._config_class(**config).dict()
-
-    def validate_search_point(
-        self, search_point: Dict[str, Any], accelerator_spec: AcceleratorSpec, with_fixed_value: bool = False
-    ) -> bool:
-        """Validate the search point for the pass."""
-        return True
-
-    def filter_ignored_params(self, config: Dict[str, Any]) -> Dict[str, Any]:
-        """Filter out ignored parameters."""
-        return {key: value for key, value in config.items() if value != SpecialParamValue.IGNORED}
-
     @abstractmethod
     def _run_for_config(
         self, model: OliveModelHandler, data_root: str, config: Dict[str, Any], output_model_path: str
     ) -> OliveModelHandler:
         """Run the pass on the model with the given configuration."""
         raise NotImplementedError
 
-    def run(
-        self, model: OliveModelHandler, data_root: str, output_model_path: str, point: Optional[Dict[str, Any]] = None
-    ) -> OliveModelHandler:
-        """Run the pass on the model at a specific point in the search space."""
-        point = point or {}
-        config = self.config_at_search_point(point)
-
-        if not self._initialized:
-            self._initialize()
-            self._initialized = True
-
-        # Optimization pass still works on individual graphs.
-        if isinstance(model, DistributedOnnxModelHandler):
-            for rank in range(model.num_ranks):
-                input_ranked_model = model.load_model(rank)
-                ranked_output_path = Path(output_model_path).with_suffix("") / model.ranked_model_name(rank)
-                self._run_for_config(input_ranked_model, data_root, config, str(ranked_output_path))
-
-            output_model = DistributedOnnxModelHandler(
-                model_path=str(Path(output_model_path).with_suffix("")),
-                model_name_pattern=model.model_name_pattern,
-                num_ranks=model.num_ranks,
-                inference_settings=model.inference_settings,
-            )
-        elif isinstance(model, CompositeModelHandler) and not self._accepts_composite_model:
-            # CompositePyTorchModel is also handled here.
-            components = []
-            component_names = []
-            for component_name, component_model in model.get_model_components():
-                component_output_path = Path(output_model_path).with_suffix("") / component_name
-                output_model_component = self._run_for_config(
-                    component_model, data_root, config, str(component_output_path)
-                )
-                output_model_component.model_attributes = (
-                    output_model_component.model_attributes or component_model.model_attributes
-                )
-                components.append(output_model_component)
-                component_names.append(component_name)
-            output_model = CompositeModelHandler(components, component_names)
-        else:
-            output_model = self._run_for_config(model, data_root, config, output_model_path)
-        # assumption: the model attributes from passes, if any, are more important than
-        # the input model attributes, we should not update/extend anymore outside of the pass run
-        output_model.model_attributes = output_model.model_attributes or model.model_attributes
-        return output_model
-
-    def serialize_config(self, config: Dict[str, Any], check_object: bool = False) -> str:
-        """Serialize the configuration."""
-        return self._config_class(**config).to_json(check_object)
-
-    def to_json(self, check_object: bool = False) -> Dict[str, Any]:
-        """Convert the pass to json."""
-        return {
-            "type": self.__class__.__name__,
-            "disable_search": True,
-            "accelerator": self.accelerator_spec.to_json(),
-            "config": self.serialize_config(self.config, check_object),
-        }
-
 
 # TODO(jambayk): rename. We are using FullPassConfig since PassConfigBase already refers to inner config
 class FullPassConfig(ConfigBase):
     type: str
     disable_search: bool = False
     accelerator: Dict[str, str] = None
+    host_device: Optional[str] = None
     config: Dict[str, Any] = None
 
-    @validator("type")
-    def validate_type(cls, v):
-        if v.lower() not in Pass.registry:
-            raise ValueError(f"Unknown pass type {v}")
-        return v
-
     def create_pass(self):
         if not isinstance(self.accelerator, dict):
             raise ValueError(f"accelerator must be a dict, got {self.accelerator}")
 
         pass_cls = Pass.registry[self.type.lower()]
         accelerator_spec = AcceleratorSpec(**self.accelerator)  # pylint: disable=not-a-mapping
-        return pass_cls(accelerator_spec, self.config, self.disable_search)
+        return pass_cls(accelerator_spec, self.config, self.disable_search, self.host_device)
 
 
-# TODO(myguo): deprecate or remove this method by explicitly specify the accelerator_spec in the arguments
+# TODO(myguo): deprecate or remove this function by explicitly specify the accelerator_spec in the arguments
 # instead of using the default argument.
 def create_pass_from_dict(
-    pass_cls: Type[Pass], config: Dict[str, Any] = None, disable_search=False, accelerator_spec: AcceleratorSpec = None
+    pass_cls: Type[Pass],
+    config: Dict[str, Any] = None,
+    disable_search=False,
+    accelerator_spec: AcceleratorSpec = None,
+    host_device=None,
 ) -> Pass:
     """Create a pass from a dictionary."""
     if accelerator_spec is None:
         accelerator_spec = DEFAULT_CPU_ACCELERATOR
 
     config = pass_cls.generate_search_space(accelerator_spec, config, disable_search)
-    return pass_cls(accelerator_spec, config, disable_search)
+    return pass_cls(accelerator_spec, config, disable_search, host_device)
```

## olive/passes/pass_config.py

```diff
@@ -1,18 +1,18 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from enum import Enum
 from pathlib import Path
-from typing import Callable, Dict, Optional, Type, Union
+from typing import Callable, ClassVar, Dict, List, Optional, Type, Union
 
 from olive.common.config_utils import ConfigBase, ConfigParam, ParamCategory, validate_object, validate_resource_path
 from olive.common.pydantic_v1 import create_model, validator
-from olive.strategy.search_parameter import SearchParameter, json_to_search_parameter
+from olive.strategy.search_parameter import SearchParameter, SpecialParamValue, json_to_search_parameter
 
 
 class PassParamDefault(str, Enum):
     """Default values for passes."""
 
     DEFAULT_VALUE = "DEFAULT_VALUE"
     SEARCHABLE_VALUES = "SEARCHABLE_VALUES"
@@ -113,14 +113,33 @@
             validators[f"validate_{param}"] = validator(param, allow_reuse=True)(validate_resource_path)
 
         type_ = param_config.type_
         if param_config.required:
             config[param] = (type_, ...)
             continue
 
-        type_ = Optional[Union[type_, SearchParameter, PassParamDefault]]
+        # Value can be one of
+        # 1. Instance of type_ if search is disabled or searchable_values is None
+        # 2. Search Parameter if search is enabled and searchable_values is not None
+        # 3. PassParamDefault if value is set to "DEFAULT_VALUE" or "SEARCHABLE_VALUES"
+        # 4. SpecialParamValue.IGNORED if the param is ignored for a specific search point. This is used to ignore
+        #    parameters that are only used conditional on other parameters. Such as static quantization parameters
+        #    that are only used if the quantization mode is static.
+        type_ = Optional[Union[type_, SearchParameter, PassParamDefault, SpecialParamValue]]
         if not disable_search and param_config.searchable_values is not None:
             config[param] = (type_, param_config.searchable_values)
         else:
             config[param] = (type_, param_config.default_value)
 
     return create_model(f"{pass_type}Config", **config, __base__=PassConfigBase, __validators__=validators)
+
+
+class PassModuleConfig(ConfigBase):
+    module_path: str
+    module_dependencies: ClassVar[List[str]] = []
+    extra_dependencies: ClassVar[List[str]] = []
+
+    @validator("module_path", pre=True)
+    def validate_module_path(cls, v, values):
+        if not v:
+            raise ValueError("module_path cannot be empty or None")
+        return v
```

## olive/passes/onnx/__init__.py

```diff
@@ -1,54 +1,4 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
-from olive.passes.onnx.append_pre_post_processing_ops import AppendPrePostProcessingOps
-from olive.passes.onnx.bnb_quantization import OnnxBnb4Quantization
-from olive.passes.onnx.conversion import OnnxConversion, OnnxOpVersionConversion
-from olive.passes.onnx.dynamic_to_fixed_shape import DynamicToFixedShape
-from olive.passes.onnx.float16_conversion import OnnxFloatToFloat16
-from olive.passes.onnx.genai_model_exporter import GenAIModelExporter
-from olive.passes.onnx.inc_quantization import IncDynamicQuantization, IncQuantization, IncStaticQuantization
-from olive.passes.onnx.insert_beam_search import InsertBeamSearch
-from olive.passes.onnx.mixed_precision import OrtMixedPrecision
-from olive.passes.onnx.model_optimizer import OnnxModelOptimizer
-from olive.passes.onnx.moe_experts_distributor import MoEExpertsDistributor
-from olive.passes.onnx.optimum_conversion import OptimumConversion
-from olive.passes.onnx.optimum_merging import OptimumMerging
-from olive.passes.onnx.perf_tuning import OrtPerfTuning
-from olive.passes.onnx.qnn_preprocess import QNNPreprocess
-from olive.passes.onnx.quantization import (
-    OnnxDynamicQuantization,
-    OnnxMatMul4Quantizer,
-    OnnxQuantization,
-    OnnxStaticQuantization,
-)
-from olive.passes.onnx.transformer_optimization import OrtTransformersOptimization
-from olive.passes.onnx.vitis_ai_quantization import VitisAIQuantization
-
-__all__ = [
-    "AppendPrePostProcessingOps",
-    "DynamicToFixedShape",
-    "GenAIModelExporter",
-    "IncDynamicQuantization",
-    "IncQuantization",
-    "IncStaticQuantization",
-    "InsertBeamSearch",
-    "MoEExpertsDistributor",
-    "OnnxBnb4Quantization",
-    "OnnxConversion",
-    "OnnxDynamicQuantization",
-    "OnnxFloatToFloat16",
-    "OnnxMatMul4Quantizer",
-    "OnnxModelOptimizer",
-    "OnnxOpVersionConversion",
-    "OnnxQuantization",
-    "OnnxStaticQuantization",
-    "OptimumConversion",
-    "OptimumMerging",
-    "OrtMixedPrecision",
-    "OrtPerfTuning",
-    "OrtTransformersOptimization",
-    "QNNPreprocess",
-    "VitisAIQuantization",
-]
```

## olive/passes/onnx/append_pre_post_processing_ops.py

```diff
@@ -40,16 +40,16 @@
 
         return v
 
 
 class AppendPrePostProcessingOps(Pass):
     """Add Pre/Post nodes to the input model."""
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, Dict[str, Any]]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, Dict[str, Any]]:
         config = {
             "pre": PassConfigParam(
                 type_=List[Dict[str, Any]],
                 default_value=None,
                 description="List of pre-processing commands to add.",
             ),
             "post": PassConfigParam(
```

## olive/passes/onnx/bnb_quantization.py

```diff
@@ -19,16 +19,16 @@
 
 logger = logging.getLogger(__name__)
 
 
 class OnnxBnb4Quantization(Pass):
     """Quantize MatMul nodes in ONNX model using 4bit FP4/NF4 quantization."""
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "quant_type": PassConfigParam(
                 type_=str,
                 default_value=None,
                 description="The quantization type. Only 'fp4' and 'nf4' are supported.",
             ),
             "quantized_modules": PassConfigParam(
```

## olive/passes/onnx/conversion.py

```diff
@@ -59,24 +59,24 @@
 
 
 class OnnxConversion(Pass):
     """Convert a PyTorch model to ONNX model using torch.onnx.export on CPU."""
 
     _requires_user_script = True
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "target_opset": PassConfigParam(
                 type_=int, default_value=13, description="The version of the default (ai.onnx) opset to target."
             ),
             "use_dynamo_exporter": PassConfigParam(
                 type_=bool, default_value=False, description="Whether to use dynamo_export API to export ONNX model."
             ),
-            "use_device": PassConfigParam(
+            "device": PassConfigParam(
                 type_=str,
                 description=(
                     "The device to use for conversion, e.g., 'cuda' or 'cpu'. If not specified, will use 'cpu' for"
                     " PyTorch model and 'cuda' for DistributedPyTorchModel."
                 ),
             ),
             "torch_dtype": PassConfigParam(
@@ -93,31 +93,39 @@
                 description="Number of parallel jobs. Defaulted to number of CPUs. Set it to 0 to disable.",
             ),
             "merge_components": PassConfigParam(
                 type_=bool,
                 default_value=False,
                 description="Whether to merge the converted components.",
             ),
+            "merge_adapter_weights": PassConfigParam(
+                type_=bool,
+                default_value=False,
+                description="Whether to merge adapter weights before conversion. "
+                "After merging, the model structure is consistent with base model. "
+                "That is useful if you cannot run conversion for some fine-tuned "
+                "models with adapter weights",
+            ),
         }
         config.update(get_external_data_config())
         return config
 
     def _run_for_config(
         self, model: PyTorchModelHandler, data_root: str, config: Dict[str, Any], output_model_path: str
     ) -> Union[CompositeModelHandler, DistributedOnnxModelHandler, ONNXModelHandler]:
         # get the device to use for conversion
         # default to "cpu" for PyTorchModelHandler and "cuda" for DistributedPyTorchModel
-        device = config["use_device"] or "cpu"
+        device = config["device"] or "cpu"
         # get the dtype to use for conversion
         torch_dtype = resolve_torch_dtype(config["torch_dtype"]) if config["torch_dtype"] else None
         if torch_dtype == torch.float16 and device == "cpu":
             raise ValueError("Conversion to float16 is not supported for CPU.")
 
         if isinstance(model, DistributedPyTorchModelHandler):
-            if not config["use_device"]:
+            if not config["device"]:
                 device = "cuda"
             return self._convert_distributed_model_on_device(
                 model, data_root, config, output_model_path, device, torch_dtype
             )
 
         if not model.hf_config or not model.hf_config.components:
             return self._convert_model_on_device(model, data_root, config, output_model_path, device, torch_dtype)
@@ -330,15 +338,15 @@
             )
             new_from_pretrained_args["torch_dtype"] = torch_dtype
             new_model_attributes["torch_dtype"] = str(torch_dtype).replace("torch.", "")
         elif model_dtype == torch.float16 and device == "cpu":
             logger.warning(
                 "Loading model on CPU, but the model loading args specify dtype float16 which is not supported  for"
                 " conversion on CPU. The dtype is changed to float32. If float16 model is desired, please specify"
-                " use_device as 'cuda' or use OrtTransformerOptimization/OnnxFloatToFloat16 pass after conversion to"
+                " device as 'cuda' or use OrtTransformerOptimization/OnnxFloatToFloat16 pass after conversion to"
                 " convert the model to float16."
             )
             new_from_pretrained_args["torch_dtype"] = torch.float32
             new_model_attributes["torch_dtype"] = "float32"
 
         if (
             from_pretrained_args.quantization_method == "bitsandbytes"
@@ -388,14 +396,17 @@
         output_model_path: str,
         device: str,
         torch_dtype: Optional[torch.dtype] = None,
     ) -> ONNXModelHandler:
         """Convert a PyTorchModelHandler to an ONNXModelHandler."""
         # load the model
         pytorch_model, model_attributes = self._load_pytorch_model(model, device, torch_dtype)
+        if config["merge_adapter_weights"] and is_peft_model(pytorch_model):
+            logger.debug("Merging adapter weights into base model. This is specific to PeftModel.")
+            pytorch_model = pytorch_model.merge_and_unload()
         pytorch_model.eval()
 
         # get dummy inputs
         dummy_inputs = model.get_dummy_inputs()
         io_config = model.get_io_config()
 
         converted_onnx_model = OnnxConversion._export_pytorch_model(
@@ -535,16 +546,16 @@
             model_name_pattern=DistributedOnnxModelHandler.DEFAULT_RANKED_MODEL_NAME_FORMAT,
             num_ranks=world_size,
             model_attributes=model.model_attributes,
         )
 
 
 class OnnxOpVersionConversion(Pass):
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         latest_opset_version = onnx.defs.onnx_opset_version()
 
         config = {
             "target_opset": PassConfigParam(
                 type_=int,
                 default_value=latest_opset_version,
                 description="The version of the default (ai.onnx) opset to target. Default: latest opset version.",
```

## olive/passes/onnx/dynamic_to_fixed_shape.py

```diff
@@ -16,16 +16,16 @@
 
 logger = logging.getLogger(__name__)
 
 
 class DynamicToFixedShape(Pass):
     """Convert dynamic shape to fixed shape for ONNX model."""
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "dim_param": PassConfigParam(
                 type_=List[str],
                 default_value=None,
                 required=False,
                 description=("Symbolic parameter name. Provide dim_value if specified."),
             ),
@@ -50,16 +50,16 @@
                     "All values must be > 0. e.g. [1,3,256,256]"
                 ),
             ),
         }
         config.update(get_external_data_config())
         return config
 
-    @staticmethod
-    def _validators() -> Dict[str, Callable[..., Any]]:
+    @classmethod
+    def _validators(cls) -> Dict[str, Callable[..., Any]]:
         return {
             "validate_configs": root_validator(allow_reuse=True)(_jointly_validate_configs),
         }
 
     def _run_for_config(
         self,
         model: ONNXModelHandler,
```

## olive/passes/onnx/float16_conversion.py

```diff
@@ -18,16 +18,16 @@
 class OnnxFloatToFloat16(Pass):
     """Converts a model to float16.
 
     It is based on onnxconverter-common.convert_float_to_float16.
     See https://onnxruntime.ai/docs/performance/model-optimizations/float16.html#float16-conversion
     """
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "min_positive_val": PassConfigParam(
                 type_=float, default_value=1e-7, description="Constant values will be clipped against this value"
             ),
             "max_finite_val": PassConfigParam(
                 type_=float, default_value=1e4, description="Constant values will be clipped against this value"
             ),
```

## olive/passes/onnx/genai_model_exporter.py

```diff
@@ -6,16 +6,14 @@
 # --------------------------------------------------------------------------
 import logging
 import os
 from enum import Enum
 from pathlib import Path
 from typing import Any, Dict
 
-from onnx import TensorProto
-
 from olive.hardware.accelerator import AcceleratorLookup, AcceleratorSpec, Device
 from olive.model import ONNXModelHandler, PyTorchModelHandler
 from olive.model.utils import resolve_onnx_path
 from olive.passes import Pass
 from olive.passes.olive_pass import PassConfigParam
 
 logger = logging.getLogger(__name__)
@@ -28,16 +26,16 @@
     """
 
     class Precision(str, Enum):
         FP32 = "fp32"
         FP16 = "fp16"
         INT4 = "int4"
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "precision": PassConfigParam(
                 type_=GenAIModelExporter.Precision,
                 required=True,
                 description="Precision of model.",
             )
         }
@@ -63,33 +61,28 @@
             else Device.GPU
         )
 
         logger.info(
             "Valid precision + execution provider combinations are: FP32 CPU, FP32 CUDA, FP16 CUDA, INT4 CPU, INT4 CUDA"
         )
 
-        # Set input/output precision of ONNX model
-        io_dtype = (
-            TensorProto.FLOAT
-            if precision in {"int8", "fp32"} or (precision == "int4" and device == Device.CPU)
-            else TensorProto.FLOAT16
-        )
-
         # Select cache location based on priority
         # HF_CACHE (HF >= v5) -> TRANSFORMERS_CACHE (HF < v5) -> local dir
         cache_dir = os.environ.get("HF_HOME", None)
         if not cache_dir:
             cache_dir = os.environ.get("TRANSFORMERS_CACHE", None)
         if not cache_dir:
             cache_dir = output_model_filepath.parent / "genai_cache_dir"
 
+        # currently we only support regular hf models so we can pass the name_or_path directly to model_name
+        # could also check if it is a locally saved model and pass the path to input_path but it is not necessary
         create_model(
-            model_name_or_path=str(model.hf_config.model_name),
+            model_name=str(model.model_path or model.hf_config.model_name),
+            input_path="",  # empty string for now
             output_dir=str(output_model_filepath.parent),
             precision=str(precision),
             execution_provider=str(device),
             cache_dir=str(cache_dir),
-            io_dtype=io_dtype,
             filename=str(output_model_filepath.name),
         )
 
         return ONNXModelHandler(output_model_filepath.parent, onnx_file_name=output_model_filepath.name)
```

## olive/passes/onnx/inc_quantization.py

```diff
@@ -25,14 +25,15 @@
 from olive.passes.pass_config import ParamCategory, PassConfigParam
 from olive.resource_path import OLIVE_RESOURCE_ANNOTATIONS
 from olive.strategy.search_parameter import Boolean, Categorical, Conditional
 
 logger = logging.getLogger(__name__)
 
 _inc_quantization_config = {
+    # TODO(myguo): consider remove the device since now we have accelerator_spec.device
     "device": PassConfigParam(
         type_=str,
         default_value="cpu",
         description="""
             Intel Neural Compressor quantization device. Support 'cpu' and 'gpu'.
         """,
     ),
@@ -275,22 +276,23 @@
 }
 
 
 class IncQuantization(Pass):
     """Quantize ONNX model with Intel Neural Compressor."""
 
     _requires_user_script = True
+    run_on_target = True
 
     @staticmethod
     def is_accelerator_agnostic(accelerator_spec: AcceleratorSpec) -> bool:
         """Override this method to return False by using the accelerator spec information."""
         return False
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "approach": PassConfigParam(
                 type_=str,
                 default_value="static",
                 searchable_values=Categorical(["dynamic", "static", "weight_only"]),
                 description="""
                 Intel Neural Compressor Quantization mode. 'dynamic' for dynamic quantization,
@@ -546,26 +548,32 @@
             **run_config,
             accuracy_criterion=accuracy_criterion,
             tuning_criterion=tuning_criterion,
         )
 
         inc_calib_dataloader = None
         if require_dataloader:
-            if self._user_module_loader:
+            # Never directly use `if self._user_module_loader` to check dataloader is provided or not
+            # self._user_module_loader is always not None since it is initialized in __init__
+            if config["dataloader_func"]:
                 data_dir = get_local_path_from_root(data_root, config["data_dir"])
                 inc_calib_dataloader = self._user_module_loader.call_object(
                     config["dataloader_func"],
                     data_dir,
                     config["batch_size"],
                     model_path=model.model_path,
                     **(config["dataloader_func_kwargs"] or {}),
                 )
             elif config["data_config"]:
                 data_config = validate_config(config["data_config"], DataConfig)
-                inc_calib_dataloader = data_config.to_data_container().create_calibration_dataloader(data_root)
+                # inc quantization's calibration dataloader requires:
+                # 1. input: (input, label)
+                # 2. the dataloader should have the attributes of "__iter__" and "batch_size"
+                #  which is data_config's create_dataloader but not create_calibration_dataloader
+                inc_calib_dataloader = data_config.to_data_container().create_dataloader(data_root)
 
         if run_config.get("diagnosis", False):
             assert inc_calib_dataloader is not None, "diagnosis mode requires dataloader"
 
         q_model = quantization.fit(
             model.model_path, ptq_config, calib_dataloader=inc_calib_dataloader, eval_func=eval_func
         )
@@ -588,16 +596,16 @@
 
 
 class IncDynamicQuantization(IncQuantization):
     """Intel Neural Compressor Dynamic Quantization Pass."""
 
     _requires_user_script = False
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, Any]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, Any]:
         config = {
             "approach": PassConfigParam(type_=str, default_value="dynamic", description="dynamic quantization mode")
         }
         # common quantization config
         config.update(deepcopy(_inc_quantization_config))
 
         # tuning criterion config
@@ -608,16 +616,16 @@
         config.update(get_external_data_config())
         return config
 
 
 class IncStaticQuantization(IncQuantization):
     """Intel Neural Compressor Static Quantization Pass."""
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, Any]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, Any]:
         config = {
             "approach": PassConfigParam(type_=str, default_value="static", description="static quantization mode"),
             "diagnosis": PassConfigParam(
                 type_=bool,
                 default_value=False,
                 description="""Whether to enable diagnosis mode. If enabled,
                 Intel Neural Compressor will print the quantization summary.""",
```

## olive/passes/onnx/insert_beam_search.py

```diff
@@ -17,20 +17,23 @@
 
 logger = logging.getLogger(__name__)
 
 # ruff: noqa: N806
 
 
 class InsertBeamSearch(Pass):
-    """Insert Beam Search Op."""
+    """Insert Beam Search Op. Only used for whisper models.
+
+    Uses WhisperBeamSearch contrib op if ORT version >= 1.17.1, else uses BeamSearch contrib op.
+    """
 
     _accepts_composite_model = True
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         use_gpu = accelerator_spec.accelerator_type == Device.GPU
         config = {
             "no_repeat_ngram_size": PassConfigParam(
                 type_=int,
                 default_value=0,
                 description=" If set to int > 0, all ngrams of that size can only occur once.",
             ),
@@ -61,14 +64,21 @@
                 type_=bool,
                 default_value=False,
                 description=(
                     "Use logits_processor as an extra graph input to the beam search op. Only supported in ORT >="
                     " 1.16.0"
                 ),
             ),
+            "use_temperature": PassConfigParam(
+                type_=bool,
+                default_value=False,
+                description=(
+                    "Use temperature as an extra graph input to the beam search op. Only supported in ORT >= 1.17.1"
+                ),
+            ),
             "fp16": PassConfigParam(
                 type_=bool,
                 default_value=False,
                 description="Is the model in fp16 precision.",
             ),
             "use_gpu": PassConfigParam(
                 type_=bool,
@@ -83,14 +93,16 @@
         self, model_A: ModelProto, model_A_name: str, model_B: ModelProto, model_B_name: str, model_config, options
     ):
         from onnxruntime import __version__ as OrtVersion
         from onnxruntime.transformers.convert_generation import get_shared_initializers
 
         # version check
         version_1_16 = version.parse(OrtVersion) >= version.parse("1.16.0")
+        version_1_17_1 = version.parse(OrtVersion) >= version.parse("1.17.1")
+        # NOTE: will ignore cross qk related options for now
 
         # Chain two models (model_A and model_B) by inserting beam search op in between.
         model_A.graph.name = f"{model_A_name} subgraph"
         model_B.graph.name = f"{model_B_name} subgraph"
 
         beam_inputs = [
             "input_features_fp16" if options["fp16"] else "input_features",
@@ -103,16 +115,26 @@
             "vocab_mask" if (version_1_16 and options["use_vocab_mask"]) else "",
             "prefix_vocab_mask" if (version_1_16 and options["use_prefix_vocab_mask"]) else "",
             "" if version_1_16 else "attention_mask",
         ]
         if version_1_16:
             beam_inputs.extend(["decoder_input_ids" if options["use_forced_decoder_ids"] else ""])
             beam_inputs.extend(["logits_processor" if options["use_logits_processor"] else ""])
+        if version_1_17_1:
+            beam_inputs.extend(["", ""])
+            beam_inputs.extend(
+                [("temperature_fp16" if options["fp16"] else "temperature") if options["use_temperature"] else ""]
+            )
+        # remove empty string from the end of beam_inputs
+        # otherwise, the model gives error when the last input is empty
+        while beam_inputs[-1] == "":
+            beam_inputs.pop()
 
-        input_features_cast_node, len_pen_cast_node, rep_pen_cast_node = None, None, None
+        # Cast input features to fp16 if required
+        graph_nodes = []
         if options["fp16"]:
             input_features_cast_node = helper.make_node(
                 "Cast",
                 inputs=["input_features"],
                 outputs=["input_features_fp16"],
                 name="CastInputFeaturesToFp16",
                 to=TensorProto.FLOAT16,
@@ -127,31 +149,72 @@
             rep_pen_cast_node = helper.make_node(
                 "Cast",
                 inputs=["repetition_penalty"],
                 outputs=["repetition_penalty_fp16"],
                 name="CastRepetitionPenaltyToFp16",
                 to=TensorProto.FLOAT16,
             )
+            graph_nodes.extend([input_features_cast_node, len_pen_cast_node, rep_pen_cast_node])
+
+            if version_1_17_1 and options["use_temperature"]:
+                temperature_cast_node = helper.make_node(
+                    "Cast",
+                    inputs=["temperature"],
+                    outputs=["temperature_fp16"],
+                    name="CastTemperatureToFp16",
+                    to=TensorProto.FLOAT16,
+                )
+                graph_nodes.append(temperature_cast_node)
 
         beam_outputs = ["sequences"]
 
-        node = helper.make_node("BeamSearch", inputs=beam_inputs, outputs=beam_outputs, name="BeamSearch_node")
-        node.domain = "com.microsoft"
-        node.attribute.extend(
-            [
-                helper.make_attribute("eos_token_id", model_config["eos_token_id"]),
-                helper.make_attribute("pad_token_id", model_config["pad_token_id"]),
-                helper.make_attribute("decoder_start_token_id", model_config["decoder_start_token_id"]),
-                helper.make_attribute("no_repeat_ngram_size", options["no_repeat_ngram_size"]),
-                helper.make_attribute("early_stopping", True),
-                helper.make_attribute("model_type", 2),
-            ]
+        # beam search op attributes
+        beam_search_attrs = [
+            helper.make_attribute("eos_token_id", model_config["eos_token_id"]),
+            helper.make_attribute("pad_token_id", model_config["pad_token_id"]),
+            helper.make_attribute("decoder_start_token_id", model_config["decoder_start_token_id"]),
+            helper.make_attribute("no_repeat_ngram_size", options["no_repeat_ngram_size"]),
+            helper.make_attribute("early_stopping", True),
+            helper.make_attribute("model_type", 2),
+        ]
+        if version_1_17_1:
+            from transformers import AutoTokenizer
+
+            # get tokenizer
+            # can get the base name of the model from the config
+            tokenizer = AutoTokenizer.from_pretrained(model_config["_name_or_path"])
+
+            beam_search_attrs.extend(
+                [
+                    helper.make_attribute("translate_token_id", tokenizer.convert_tokens_to_ids(["<|translate|>"])[0]),
+                    helper.make_attribute(
+                        "transcribe_token_id", tokenizer.convert_tokens_to_ids(["<|transcribe|>"])[0]
+                    ),
+                    helper.make_attribute(
+                        "start_of_lm_token_id", tokenizer.convert_tokens_to_ids(["<|startoflm|>"])[0]
+                    ),
+                    helper.make_attribute(
+                        "no_timestamps_token_id", tokenizer.convert_tokens_to_ids(["<|notimestamps|>"])[0]
+                    ),
+                    helper.make_attribute(
+                        "beginning_timestamp_token_id", tokenizer.convert_tokens_to_ids(["<|0.00|>"])[0]
+                    ),
+                ]
+            )
+
+        node = helper.make_node(
+            "WhisperBeamSearch" if version_1_17_1 else "BeamSearch",
+            inputs=beam_inputs,
+            outputs=beam_outputs,
+            name="BeamSearch_node",
+            domain="com.microsoft",
         )
+        node.attribute.extend(beam_search_attrs)
 
-        # beam graph inputs
+        # Graph inputs
         input_features = helper.make_tensor_value_info(
             "input_features", TensorProto.FLOAT, ["batch_size", "feature_size", "sequence_length"]
         )
         max_length = helper.make_tensor_value_info("max_length", TensorProto.INT32, [1])
         min_length = helper.make_tensor_value_info("min_length", TensorProto.INT32, [1])
         num_beams = helper.make_tensor_value_info("num_beams", TensorProto.INT32, [1])
         num_return_sequences = helper.make_tensor_value_info("num_return_sequences", TensorProto.INT32, [1])
@@ -191,20 +254,25 @@
                 )
                 graph_inputs.append(decoder_input_ids)
 
             if options["use_logits_processor"]:
                 logits_processor = helper.make_tensor_value_info("logits_processor", TensorProto.INT32, [1])
                 graph_inputs.append(logits_processor)
 
-        # graph outputs
+            if version_1_17_1 and options["use_temperature"]:
+                temperature = helper.make_tensor_value_info("temperature", TensorProto.FLOAT, [1])
+                graph_inputs.append(temperature)
+
+        # Graph outputs
         sequences = helper.make_tensor_value_info(
             "sequences", TensorProto.INT32, ["batch_size", "num_return_sequences", "max_length"]
         )
         graph_outputs = [sequences]
 
+        # Replace MultiHeadAttention with DecoderMaskedMultiHeadAttention for CUDA EP inference
         if options["use_gpu"] and version_1_16:
             from onnxruntime.transformers.convert_generation import (
                 update_decoder_subgraph_share_buffer_and_use_decoder_masked_mha as update_decoder_with_ort,
             )
 
             if update_decoder_with_ort(model_B.graph):
                 logger.info("Updated whisper decoder subgraph to use DecoderMaskedMultiHeadAttention successfully!")
@@ -216,22 +284,23 @@
         initializers = get_shared_initializers(model_A, model_B)
         node.attribute.extend(
             [
                 helper.make_attribute("decoder", model_B.graph),
                 helper.make_attribute("encoder", model_A.graph),
             ]
         )
+
         opset_import = [
             helper.make_opsetid(domain="com.microsoft", version=1),
             helper.make_opsetid(domain="", version=17),
         ]
-        graph_nodes = (
-            [input_features_cast_node, len_pen_cast_node, rep_pen_cast_node, node] if options["fp16"] else [node]
-        )
 
+        graph_nodes.append(node)
+
+        # Make graph with BeamSearch/WhisperBeamSearch op
         beam_graph = helper.make_graph(graph_nodes, "beam-search-test", graph_inputs, graph_outputs, initializers)
         assert model_A.ir_version == model_B.ir_version
         logger.debug("Using IR version %s for chained model", model_A.ir_version)
 
         # Set IR version of chained model to IR version of subgraphs in order to generate a working E2E model
         return helper.make_model_gen_version(
             beam_graph,
```

## olive/passes/onnx/mixed_precision.py

```diff
@@ -17,16 +17,16 @@
 
 logger = logging.getLogger(__name__)
 
 
 class OrtMixedPrecision(Pass):
     """Convert model to mixed precision."""
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "op_block_list": PassConfigParam(
                 type_=List[str],
                 default_value=["SimplifiedLayerNormalization", "SkipSimplifiedLayerNormalization", "Relu", "Add"],
                 description="List of op types to leave as float32",
             ),
             "atol": PassConfigParam(
```

## olive/passes/onnx/model_optimizer.py

```diff
@@ -215,16 +215,16 @@
         o_model.prune_graph()
         self.model = o_model.model  # pylint: disable=attribute-defined-outside-init
 
 
 class OnnxModelOptimizer(Pass):
     """Optimize ONNX model by fusing nodes."""
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return get_external_data_config()
 
     def _run_for_config(
         self, model: ONNXModelHandler, data_root: str, config: Dict[str, Any], output_model_path: str
     ) -> ONNXModelHandler:
         output_model_path = resolve_onnx_path(output_model_path, Path(model.model_path).name)
```

## olive/passes/onnx/moe_experts_distributor.py

```diff
@@ -359,16 +359,16 @@
 
         return output_filepaths
 
 
 class MoEExpertsDistributor(Pass):
     """Split the input model (and insert necessary communication operations) to prepare for distributed inferencing."""
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "world_size": PassConfigParam(
                 type_=int,
                 default=2,
                 required=True,
                 description="Number of GPU nodes to distribute the model for. Must be greater than 1.",
             ),
@@ -385,21 +385,17 @@
     @staticmethod
     def _validate_world_size(v):
         if int(v) < 2:
             raise ValueError("world_size should be >= 2")
 
         return v
 
-    @staticmethod
-    def _validators() -> Dict[str, Callable]:
-        return {
-            "validate_distributor_config": validator("world_size", allow_reuse=True)(
-                MoEExpertsDistributor._validate_world_size
-            )
-        }
+    @classmethod
+    def _validators(cls) -> Dict[str, Callable]:
+        return {"validate_distributor_config": validator("world_size", allow_reuse=True)(cls._validate_world_size)}
 
     def _run_for_config(
         self, model: ONNXModelHandler, data_root: str, config: Dict[str, Any], output_model_path: str
     ) -> DistributedOnnxModelHandler:
         # huggingface/tokenizers: The current process just got forked, after parallelism has already been used.
         # Disabling parallelism to avoid deadlocks...
         # To disable this warning, you can either:
```

## olive/passes/onnx/optimum_conversion.py

```diff
@@ -17,16 +17,16 @@
 
 
 class OptimumConversion(Pass):
     """Convert a Hugging Face PyTorch model to ONNX model using the Optimum export function."""
 
     _requires_user_script = True
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "target_opset": PassConfigParam(
                 type_=int, default_value=14, description="The version of the default (ai.onnx) opset to target."
             ),
             "components": PassConfigParam(
                 type_=List[str],
                 default_value=None,
```

## olive/passes/onnx/optimum_merging.py

```diff
@@ -14,22 +14,23 @@
 from olive.passes.pass_config import PassConfigParam
 
 
 class OptimumMerging(Pass):
     """Merges a decoder_model with its decoder_with_past_model via the Optimum library."""
 
     _accepts_composite_model = True
+    run_on_target = True
 
     @staticmethod
     def is_accelerator_agnostic(accelerator_spec: AcceleratorSpec) -> bool:
         """Override this method to return False by using the accelerator spec information."""
         return False
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "strict": PassConfigParam(
                 type_=bool,
                 default_value=True,
                 description=(
                     "When set, the decoder and decoder_with_past are expected to have strictly"
                     " the same number of outputs. When False, the decoder is allowed to have"
```

## olive/passes/onnx/perf_tuning.py

```diff
@@ -5,30 +5,27 @@
 import copy
 import itertools
 import json
 import logging
 import tempfile
 import time
 from pathlib import Path
-from typing import TYPE_CHECKING, Any, Callable, Dict, Optional, Union
+from typing import Any, Callable, Dict, Union
 
 from olive.common.ort_inference import check_and_normalize_provider_args
 from olive.data.config import DataConfig
 from olive.evaluator.metric import LatencySubType, Metric, MetricType, joint_metric_key
 from olive.evaluator.metric_config import get_user_config_properties_from_metric_type
 from olive.exception import EXCEPTIONS_TO_RAISE
 from olive.hardware.accelerator import AcceleratorLookup, AcceleratorSpec
 from olive.model import ONNXModelHandler
 from olive.passes import Pass
 from olive.passes.pass_config import ParamCategory, PassConfigParam
 from olive.resource_path import OLIVE_RESOURCE_ANNOTATIONS
 
-if TYPE_CHECKING:
-    from olive.systems.olive_system import OliveSystem
-
 logger = logging.getLogger(__name__)
 
 
 PERFTUNING_BASELINE = "pretuning-baseline"
 
 
 def generate_tuning_combos(config):
@@ -193,31 +190,23 @@
     return len(affinities)
 
 
 class OrtPerfTuning(Pass):
     """Optimize ONNX Runtime inference settings."""
 
     _requires_user_script = True
-
-    def __init__(
-        self, accelerator_spec: AcceleratorSpec, config: Dict[str, Any], disable_search: Optional[bool] = False
-    ):
-        super().__init__(accelerator_spec, config, disable_search)
-        self.target = None
-
-    def set_target(self, target: "OliveSystem"):
-        self.target = target
+    run_on_target = True
 
     @staticmethod
     def is_accelerator_agnostic(accelerator_spec: AcceleratorSpec) -> bool:
         """Override this method to return False by using the accelerator spec information."""
         return False
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         device = accelerator_spec.accelerator_type
         execution_provider = accelerator_spec.execution_provider
 
         return {
             "data_dir": PassConfigParam(
                 type_=OLIVE_RESOURCE_ANNOTATIONS,
                 category=ParamCategory.DATA,
@@ -305,27 +294,24 @@
     def _run_for_config(
         self, model: ONNXModelHandler, data_root: str, config: Dict[str, Any], output_model_path: str
     ) -> ONNXModelHandler:
         config = self._config_class(**config)
         # TODO(jambayk): decide on whether to ignore the output_model_path
         # if we want to ignore it, we can just return the model
         # otherwise save or symlink the original model to the output_model_path
-        runner = PerfTuningRunner(self.accelerator_spec, self.target, config, data_root)
+        runner = PerfTuningRunner(self.accelerator_spec, config, data_root)
         return runner.tune_onnx_model(model)
 
 
 class PerfTuningRunner:
-    def __init__(
-        self, accelerator_spec: AcceleratorSpec, target: "OliveSystem", config: Dict[str, Any], data_root: str = None
-    ):
+    def __init__(self, accelerator_spec: AcceleratorSpec, config: Dict[str, Any], data_root: str = None):
         assert accelerator_spec, "accelerator_spec should not be None"
         assert config, "config should not be None"
 
         self.accelerator_spec = accelerator_spec
-        self.target = target
         self.config = config
         self.data_root = data_root
 
     def tune_onnx_model(self, model):
         latency_user_config = {}
         # which should be the same as the config in the metric
         config_dict = self.config.dict()
@@ -417,59 +403,48 @@
         test_params=None,
         io_bind=False,
         tuning_result=None,
     ):
         import onnxruntime as ort
 
         from olive.evaluator.olive_evaluator import OliveEvaluatorFactory
-        from olive.model.config.model_config import ModelConfig
 
         # prepare the inference_settings for metrics.
         tuning_result_file = None
         if test_params:
             assert "provider_options" in test_params, "provider_options should be in test_params"
             inference_settings = test_params
         else:
             inference_settings = copy.deepcopy(model.inference_settings) if model.inference_settings else {}
             # put the execution_provider and provider_options in inference_settings for baseline evaluation
-            if self.target is None:
-                available_eps = ort.get_available_providers()
-            else:
-                available_eps = None
+            available_eps = ort.get_available_providers()
             execution_providers, provider_options = check_and_normalize_provider_args(
                 self.config.providers_list, None, available_eps
             )
             inference_settings["execution_provider"] = execution_providers
             inference_settings["provider_options"] = provider_options
 
         if self.config.enable_profiling:
             if "session_options" not in inference_settings:
                 inference_settings["session_options"] = {}
             inference_settings["session_options"]["enable_profiling"] = True
 
         with tempfile.TemporaryDirectory() as temp_dir:
             enable_rocm_op_tuning(inference_settings, tuning_result, temp_dir)
-            # set the session_options for metrics so that the evalute will use them by default
+            # set the session_options for metrics so that the evaluate will use them by default
             latency_metric.user_config.io_bind = io_bind
             latency_metric.user_config.inference_settings = {"onnx": inference_settings}
 
             session_name = generate_test_name(test_params, io_bind)
             logger.debug("Run benchmark for: %s", session_name)
             joint_key = joint_metric_key(latency_metric.name, latency_metric.sub_types[0].name)
 
             start_time = time.perf_counter()
-            # TODO(myguo): consider set the ep in accelrator to None for local system
-            if self.target:
-                model_config = ModelConfig.from_json(model.to_json())
-                metric_result = self.target.evaluate_model(
-                    model_config, self.data_root, [latency_metric], self.accelerator_spec
-                )
-            else:
-                evaluator = OliveEvaluatorFactory.create_evaluator_for_model(model)
-                metric_result = evaluator.evaluate(model, self.data_root, [latency_metric], self.config.device, None)
+            evaluator = OliveEvaluatorFactory.create_evaluator_for_model(model)
+            metric_result = evaluator.evaluate(model, self.data_root, [latency_metric], self.config.device, None)
 
             end_time = time.perf_counter()
             latency_ms = metric_result[joint_key].value
             logger.debug("It takes %.5f seconds to benchmark for: %s", end_time - start_time, session_name)
 
             session_options = inference_settings.get("session_options")
```

## olive/passes/onnx/qnn_preprocess.py

```diff
@@ -1,53 +1,123 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
+import logging
 from pathlib import Path
-from typing import Any, Callable, Dict
+from typing import Any, Dict
 
 from olive.hardware import AcceleratorSpec
 from olive.model import ONNXModelHandler
 from olive.model.utils import resolve_onnx_path
 from olive.passes.olive_pass import Pass
+from olive.passes.onnx.common import get_external_data_config
 from olive.passes.pass_config import PassConfigParam
 
+logger = logging.getLogger(__name__)
+
 
 class QNNPreprocess(Pass):
     """Preprocess ONNX model for quantization targeting QNN Execution Provider."""
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
-        return {
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+        config = {
             "fuse_layernorm": PassConfigParam(
                 type_=bool,
                 default_value=False,
                 required=False,
                 description=("Whether to fuse ReduceMean sequence into a single LayerNormalization node."),
-            )
+            ),
+            "inputs_to_make_channel_last": PassConfigParam(
+                type_=list,
+                default_value=None,
+                required=False,
+                description="""inputs_to_make_channel_last: List of graph input names to transpose to be
+                "channel-last". For example, if "input0" originally has the shape (N, C, D1, D2, ..., Dn),
+                the resulting model will change input0's shape to (N, D1, D2, ..., Dn, C) and add a transpose
+                node after it.
+
+                Original:
+                    input0 (N, C, D1, D2, ..., Dn) --> <Nodes>
+
+                Updated:
+                    input0 (N, D1, D2, ..., Dn, C) --> Transpose
+                        --> input0_chanfirst (N, C, D1, D2, ..., Dn) --> <Nodes>
+
+                This can potentially improve inference latency for QDQ models running on QNN EP because the
+                additional transpose node may allow other transpose nodes inserted during ORT layout
+                transformation to cancel out.""",
+            ),
+            "outputs_to_make_channel_last": PassConfigParam(
+                type_=list,
+                default_value=None,
+                required=False,
+                description="""List of graph output names to transpose to be "channel-last". For example,
+            if "output0" originally has the shape (N, C, D1, D2, ..., Dn), the resulting model will change
+            output0's shape to (N, D1, D2, ..., Dn, C) and add a transpose node before it.
+
+            Original:
+                <Nodes> --> output0 (N, C, D1, D2, ..., Dn)
+
+            Updated:
+                <Nodes> --> output0_chanfirst (N, C, D1, D2, ..., Dn) --> Transpose
+                    --> output0 (N, D1, D2, ..., Dn, C)
+
+            This can potentially improve inference latency for QDQ models running on QNN EP because the
+            additional transpose node may allow other transpose nodes inserted during ORT layout transformation
+            to cancel out.""",
+            ),
         }
-
-    @staticmethod
-    def _validators() -> Dict[str, Callable[..., Any]]:
-        pass
+        config.update(get_external_data_config())
+        return config
 
     def _run_for_config(
         self,
         model: ONNXModelHandler,
         data_root: str,
         config: Dict[str, Any],
         output_model_path: str,
     ) -> ONNXModelHandler:
         from onnxruntime import __version__ as OrtVersion
         from packaging import version
 
         if version.parse(OrtVersion) < version.parse("1.17.0"):
             raise RuntimeError("QNNPreprocess only supports ONNXRuntime version 1.17.0 or later")
 
+        output_model_path = resolve_onnx_path(output_model_path, Path(model.model_path).name)
+        external_data_location = config["external_data_name"] or f"{Path(output_model_path).name}.data"
+
+        # only 1.18.0 or later adds the following parameters
+        extra_kwargs = {
+            "save_as_external_data": config["save_as_external_data"],
+            "all_tensors_to_one_file": config["all_tensors_to_one_file"],
+            "external_data_size_threshold": config["size_threshold"],
+            "external_data_location": external_data_location,
+            "external_data_convert_attribute": config["convert_attribute"],
+            "inputs_to_make_channel_last": config["inputs_to_make_channel_last"],
+            "outputs_to_make_channel_last": config["outputs_to_make_channel_last"],
+        }
+        if version.parse(OrtVersion) < version.parse("1.18.0"):
+            removed_config = [
+                "inputs_to_make_channel_last",
+                "outputs_to_make_channel_last",
+                *get_external_data_config(),
+            ]
+            logger.info(
+                "Following config settings will be ignored as they are not supported in ONNXRuntime < 1.18.0: %s",
+                ", ".join(removed_config),
+            )
+            extra_kwargs = {}
+
         from onnxruntime.quantization.execution_providers.qnn import qnn_preprocess_model
 
-        output_model_path = resolve_onnx_path(output_model_path, Path(model.model_path).name)
-        modified = qnn_preprocess_model(model.model_path, output_model_path, **config)
+        modified = qnn_preprocess_model(
+            model.model_path,
+            output_model_path,
+            fuse_layernorm=config["fuse_layernorm"],
+            **extra_kwargs,
+        )
         if not modified:
             return model
         return ONNXModelHandler(output_model_path)
```

## olive/passes/onnx/quantization.py

```diff
@@ -1,22 +1,24 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import logging
 import tempfile
 from copy import deepcopy
+from functools import partial
 from pathlib import Path
 from typing import Any, Callable, Dict, Union
 
 import onnx
 from packaging import version
 
 from olive.cache import get_local_path_from_root
 from olive.common.config_utils import validate_config
+from olive.common.pydantic_v1 import validator
 from olive.common.utils import hash_string
 from olive.data.config import DataConfig
 from olive.exception import OlivePassError
 from olive.hardware.accelerator import AcceleratorSpec
 from olive.model import ONNXModelHandler
 from olive.model.utils import resolve_onnx_path
 from olive.passes import Pass
@@ -134,15 +136,15 @@
             {list(_exposed_extra_options_config.keys())}, it will be overwritten by the corresponding config parameter
             value.
         """,
     ),
 }
 
 # static quantization specific config
-_static_dataloader_config = {
+_dataloader_config = {
     "data_dir": PassConfigParam(
         type_=OLIVE_RESOURCE_ANNOTATIONS,
         category=ParamCategory.DATA,
         description="""
             Path to the directory containing the dataset.
             For local data, it is required if quant_mode is 'static' and dataloader_func is provided.
         """,
@@ -224,26 +226,42 @@
             Whether to generate a suitable quantization config for the input model.
             Should be set to True if model is targeted for QNN EP.
         """,
     ),
 }
 
 
+def get_calibration_dataloader(data_root, user_module_loader, config):
+    if config["dataloader_func"]:
+        # TODO(trajep): replace legacy dataloader_func with data config
+        data_dir = get_local_path_from_root(data_root, config["data_dir"])
+        dataloader = user_module_loader.call_object(
+            config["dataloader_func"],
+            data_dir,
+            config["batch_size"],
+            **(config["dataloader_func_kwargs"] or {}),
+        )
+    elif config["data_config"]:
+        data_config = validate_config(config["data_config"], DataConfig)
+        dataloader = data_config.to_data_container().create_calibration_dataloader(data_root)
+    return dataloader
+
+
 class OnnxQuantization(Pass):
     """Quantize ONNX model with static/dynamic quantization techniques."""
 
     _requires_user_script = True
 
     def _initialize(self):
         super()._initialize()
         # pylint: disable=attribute-defined-outside-init
         self.tmp_dir = tempfile.TemporaryDirectory(prefix="olive_tmp")
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "quant_mode": PassConfigParam(
                 type_=str,
                 default_value="static",
                 searchable_values=Categorical(["dynamic", "static"]),
                 description="""
                     Onnx Quantization mode. 'dynamic' for dynamic quantization,
@@ -252,15 +270,15 @@
             )
         }
 
         # common quantization config
         config.update(deepcopy(_onnx_quantization_config))
 
         # static quantization config
-        config.update(deepcopy(_static_dataloader_config))
+        config.update(deepcopy(_dataloader_config))
         static_optional_config = deepcopy(_static_optional_config)
         for value in static_optional_config.values():
             # default value is conditional on quant_mode
             # if quant_mode is static, use the default value in static_optional_config
             # if quant_mode is dynamic, set default value as ignored. dynamic quantization doesn't use this parameter
             value.default_value = ConditionalDefault(
                 parents=("quant_mode",),
@@ -328,14 +346,20 @@
         # start with a copy of the config
         run_config = deepcopy(config)
         is_static = run_config["quant_mode"] == "static"
         if is_static:
             assert (
                 config["dataloader_func"] or config["data_config"]
             ), "dataloader_func or data_config is required for static quantization."
+            # whether to prepare qnn config
+            # we do the version check here and not in `validate_search_point` since search point validation
+            # is done by the engine. Unless the host is local system, the ort version of the host is
+            # not known by the engine when the search point is validated.
+            if config["prepare_qnn_config"] and version.parse(OrtVersion) < version.parse("1.17.0"):
+                raise OlivePassError("prepare_qnn_config is only supported by onnxruntime>=1.17.0")
 
         output_model_path = resolve_onnx_path(output_model_path, Path(model.model_path).name)
 
         # extra config
         extra_options = deepcopy(config["extra_options"]) if config["extra_options"] else {}
         # keys in extra_options that are already exposed
         intersection = set(extra_options.keys()).intersection(set(_exposed_extra_options_config.keys()))
@@ -361,43 +385,39 @@
                 # overwrite the model path with the preprocessed model path
                 logger.info("Preprocessing model for quantization")
                 model = self._quant_preprocess(model, preprocessed_temp_model_path)
             else:
                 logger.info("Already processed model for quantization, skipping preprocessing")
                 model = ONNXModelHandler(LocalFile({"path": preprocessed_temp_model_path}))
 
-        # whether to prepare qnn config
-        if run_config.get("prepare_qnn_config", False) and version.parse(OrtVersion) < version.parse("1.17.0"):
-            raise OlivePassError("prepare_qnn_config is only supported for onnxruntime-qnn>=1.17.0")
-
         # keys not needed for quantization
         to_delete = [
             "quant_mode",
             "script_dir",
             "user_script",
             "quant_preprocess",
             "data_config",
             "prepare_qnn_config",
         ]
         to_delete += list(get_external_data_config().keys())
 
         # update string values to enum values
         if is_static:
-            to_delete += list(_static_dataloader_config.keys())
+            to_delete += list(_dataloader_config.keys())
             run_config.update(
                 {
                     "calibrate_method": CalibrationMethod[run_config["calibrate_method"]],
                     "quant_format": QuantFormat[run_config["quant_format"]],
                     "activation_type": QuantType[run_config["activation_type"]],
                     "weight_type": QuantType[run_config["weight_type"]],
                     "extra_options": extra_options,
                 }
             )
         else:
-            to_delete += list(_static_dataloader_config.keys())
+            to_delete += list(_dataloader_config.keys())
             to_delete += list(_static_optional_config.keys())
             run_config.update(
                 {
                     "weight_type": QuantType[run_config["weight_type"]],
                     "extra_options": extra_options,
                 }
             )
@@ -419,27 +439,15 @@
         # TODO(jambayk): don't default to use_external_data_format=True if the loading and saving model makes
         # the pass inefficient
         new_tmp_dir = tempfile.TemporaryDirectory(prefix="olive_tmp")
         tmp_model_path = str(Path(new_tmp_dir.name) / Path(output_model_path).name)
 
         if is_static:
             # get the dataloader
-            # TODO(trajep): only use data config
-            if config["dataloader_func"]:
-                data_dir = get_local_path_from_root(data_root, config["data_dir"])
-                dataloader = self._user_module_loader.call_object(
-                    config["dataloader_func"],
-                    data_dir,
-                    config["batch_size"],
-                    **(config["dataloader_func_kwargs"] or {}),
-                )
-            elif config["data_config"]:
-                data_config = validate_config(config["data_config"], DataConfig)
-                dataloader = data_config.to_data_container().create_calibration_dataloader(data_root)
-
+            dataloader = get_calibration_dataloader(data_root, self._user_module_loader, config)
             if config["prepare_qnn_config"]:
                 import inspect
 
                 from onnxruntime.quantization.execution_providers.qnn import get_qnn_qdq_config
 
                 qnn_config = get_qnn_qdq_config(
                     model_input=model.model_path,
@@ -522,16 +530,16 @@
 
 
 class OnnxDynamicQuantization(OnnxQuantization):
     """ONNX Dynamic Quantization Pass."""
 
     _requires_user_script = False
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         if accelerator_spec.execution_provider == "QNNExecutionProvider":
             raise ValueError("QNNExecutionProvider is not supported for dynamic quantization.")
         config = {
             "quant_mode": PassConfigParam(type_=str, default_value="dynamic", description="dynamic quantization mode")
         }
         # common quantization config
         config.update(deepcopy(_onnx_quantization_config))
@@ -542,23 +550,23 @@
         config.update(get_external_data_config())
         return config
 
 
 class OnnxStaticQuantization(OnnxQuantization):
     """ONNX Static Quantization Pass."""
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "quant_mode": PassConfigParam(type_=str, default_value="static", description="static quantization mode")
         }
         # common quantization config
         config.update(deepcopy(_onnx_quantization_config))
         # static quantization specific config
-        config.update(deepcopy(_static_dataloader_config))
+        config.update(deepcopy(_dataloader_config))
         config.update(deepcopy(_static_optional_config))
         # exposed extra options config
         config.update(deepcopy(_exposed_extra_options_config))
         config.update(deepcopy(_extra_options_config))
         # external data config
         config.update(get_external_data_config())
         if accelerator_spec.execution_provider == "QNNExecutionProvider":
@@ -570,16 +578,18 @@
             config["weight_type"].searchable_values = Categorical(["QInt8", "QUInt8", "QUInt16", "QInt16"])
             config["prepare_qnn_config"].default_value = True
             config["quant_preprocess"].default_value = False
         return config
 
 
 class OnnxMatMul4Quantizer(Pass):
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    _requires_user_script = True
+
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "block_size": PassConfigParam(
                 type_=int,
                 default_value=32,
                 description="Block size for quantization. Default value is 32.",
             ),
             "is_symmetric": PassConfigParam(
@@ -588,33 +598,206 @@
                 description="Symmetric quantization. Default value is True.",
             ),
             "nodes_to_exclude": PassConfigParam(
                 type_=list,
                 default_value=None,
                 description="List of node names to exclude from quantization.",
             ),
+            "accuracy_level": PassConfigParam(
+                # TODO(trajep): to make it searchable
+                type_=int,
+                default_value=None,
+                description="Available from onnxruntime>=1.17.0 "
+                "The minimum accuracy level of input A, can be: 0(unset), 1(fp32), 2(fp16), 3(bf16), "
+                "or 4(int8) (default unset when 0 or None). It is used to control how input A is quantized or downcast "
+                "internally while doing computation, for example: 0 means input A will not be quantized "
+                "or downcast while doing computation. 4 means input A can be quantized with the same "
+                "block_size to int8 internally from type T1. "
+                "Refer to the MatMulNBits contrib op's 'accuracy_level' attribute for details "
+                "(https://github.com/microsoft/onnxruntime/blob/main/docs/ContribOperators.md#commicrosoftmatmulnbits).",
+            ),
+            "algorithm": PassConfigParam(
+                type_=str,
+                default_value=None,
+                description="If 'None', the Matmul node with fp32 const weight will be quantize to int4."
+                "1. 'RTN' and 'GPTQ' are available from onnxruntime>=1.17.0 "
+                "- For 4b quantize a model with RTN or GPTQ algorithm. Please refer to "
+                "https://github.com/intel/neural-compressor/blob/master/docs/source/quantization_weight_only.md "
+                "for more details on weight only quantization using Intel Neural Compressor. "
+                "2. 'DEFAULT', 'HQQ' are available from onnxruntime>=1.18.0 "
+                "- `DEFAULT` takes the same effect as `None`"
+                "- For HQQ, please refer to onnxruntime for more details: "
+                "https://github.com/microsoft/onnxruntime/blob/7e613ee821405b1192d0b71b9434a4f94643f1e4/onnxruntime/python/tools/quantization/matmul_4bits_quantizer.py#L102C1-L126C25",
+            ),
+            "weight_only_quant_configs": PassConfigParam(
+                type_=dict,
+                default_value=None,
+                description="""Available from onnxruntime>=1.17.0, if None, the default behavior
+                of given algorithm will be used.
+                The config is binding to the algorithm with following map:
+                1. "algorithm" is "DEFAULT", by default, the weight_only_quant_configs is:
+                    "weight_only_quant_configs": {
+                        "block_size": 128,
+                        "is_symmetric": False,
+                        "accuracy_level": None
+                    }
+                    https://github.com/microsoft/onnxruntime/blob/7e613ee821405b1192d0b71b9434a4f94643f1e4/onnxruntime/python/tools/quantization/matmul_4bits_quantizer.py#L129C1-L140C45
+                2. "algorithm" is "HQQ", by default, the weight_only_quant_configs is:
+                    "weight_only_quant_configs": {
+                        "block_size": 128, // channel number in one block to execute a GPTQ quantization iteration.
+                        "bits": 4, // how many bits to represent weight.
+                        "axis": 1, // 0 or 1. which axis to quantize. https://arxiv.org/pdf/2309.15531.pdf
+                    }
+                    https://github.com/microsoft/onnxruntime/blob/7e613ee821405b1192d0b71b9434a4f94643f1e4/onnxruntime/python/tools/quantization/matmul_4bits_quantizer.py#L129C1-L140C45
+                3. "algorithm" is "RTN", by default, the weight_only_quant_configs is:
+                    "weight_only_quant_configs": {
+                        "ratios": None, // type: dict, percentile of clip. Defaults to None.
+                    }
+                    https://github.com/microsoft/onnxruntime/blob/7e613ee821405b1192d0b71b9434a4f94643f1e4/onnxruntime/python/tools/quantization/matmul_4bits_quantizer.py#L42C1-L60C29
+                4. "algorithm" is "GPTQ", by default, the weight_only_quant_configs is:
+                    "weight_only_quant_configs": {
+                        "percdamp": 0.01, // percent of the average Hessian diagonal to use for dampening.
+                        "block_size": 128,
+                        "actorder": False, // whether rearrange Hessian matrix considering the diag's value.
+                        "mse": False, // whether get scale and zero point with mse error.
+                        "perchannel": True, // whether quantize weight per-channel.
+                    }
+                    For GPTQ's "calibration_data_reader", you can provider a dataloader function or a
+                    data config like what we do for onnx static quantization.
+                    https://github.com/microsoft/onnxruntime/blob/7e613ee821405b1192d0b71b9434a4f94643f1e4/onnxruntime/python/tools/quantization/matmul_4bits_quantizer.py#L63C1-L99C37
+                """,
+            ),
         }
         config.update(get_external_data_config())
+        # static_dataloder_config
+        config.update(deepcopy(_dataloader_config))
         return config
 
+    @classmethod
+    def _validators(cls) -> Dict[str, Callable]:
+        return {
+            "validate_accuracy_level": validator("accuracy_level", allow_reuse=True)(_validate_accuracy_level),
+            "validate_algorithm": validator("algorithm", allow_reuse=True)(_validate_algorithm),
+            "validate_quant_config": validator("weight_only_quant_configs", allow_reuse=True)(
+                _validate_weight_only_quant_config
+            ),
+        }
+
     def _run_for_config(
         self, model: ONNXModelHandler, data_root: str, config: Dict[str, Any], output_model_path: str
     ) -> ONNXModelHandler:
         from onnxruntime import __version__ as OrtVersion
 
         if version.parse(OrtVersion) < version.parse("1.16.2"):
-            raise OlivePassError("MatMul4BitsQuantizer is only supported in onnxruntime >= 1.16.2")
+            raise ValueError("MatMul4BitsQuantizer is only supported in onnxruntime >= 1.16.2")
 
         from onnxruntime.quantization.matmul_4bits_quantizer import MatMul4BitsQuantizer
 
         output_model_path = resolve_onnx_path(output_model_path, Path(model.model_path).name)
 
-        quant = MatMul4BitsQuantizer(
-            model.load_model(), config["block_size"], config["is_symmetric"], config["nodes_to_exclude"]
-        )
+        weight_only_quant_config_class = None
+        weight_only_quant_config = None
+        algo_config = deepcopy(config["weight_only_quant_configs"] or {})
+        if version.parse(OrtVersion) >= version.parse("1.17.0"):
+            from onnxruntime.quantization.matmul_4bits_quantizer import (
+                GPTQWeightOnlyQuantConfig,
+                RTNWeightOnlyQuantConfig,
+            )
+
+            if config["algorithm"] == "RTN":
+                weight_only_quant_config_class = RTNWeightOnlyQuantConfig
+            elif config["algorithm"] == "GPTQ":
+                if "block_size" in algo_config and version.parse(OrtVersion) < version.parse("1.18.0"):
+                    # ort 1.17.0+ uses blocksize instead of block_size :(
+                    algo_config["blocksize"] = algo_config["block_size"]
+                    algo_config.pop("block_size")
+                dataloader = get_calibration_dataloader(data_root, self._user_module_loader, config)
+                weight_only_quant_config_class = partial(GPTQWeightOnlyQuantConfig, calibration_data_reader=dataloader)
+
+            if version.parse(OrtVersion) >= version.parse("1.18.0"):
+                from onnxruntime.quantization.matmul_4bits_quantizer import (
+                    DefaultWeightOnlyQuantConfig,
+                    HQQWeightOnlyQuantConfig,
+                )
+
+                if config["algorithm"] == "DEFAULT":
+                    weight_only_quant_config_class = DefaultWeightOnlyQuantConfig
+                elif config["algorithm"] == "HQQ":
+                    weight_only_quant_config_class = HQQWeightOnlyQuantConfig
+            elif config["algorithm"] in ("HQQ", "DEFAULT"):
+                raise ValueError("HQQ and DEFAULT algorithm are only supported in onnxruntime >= 1.18.0")
+
+            if weight_only_quant_config_class:
+                weight_only_quant_config = weight_only_quant_config_class(**algo_config)
+            quant = MatMul4BitsQuantizer(
+                model.load_model(),
+                block_size=config["block_size"],
+                is_symmetric=config["is_symmetric"],
+                nodes_to_exclude=config["nodes_to_exclude"],
+                accuracy_level=config["accuracy_level"],
+                algo_config=weight_only_quant_config,
+            )
+        else:
+            # TODO(trajep): remove this block once we migrate customer to onnxruntime>=1.17.0 all
+            quant = MatMul4BitsQuantizer(
+                model.load_model(),
+                block_size=config["block_size"],
+                is_symmetric=config["is_symmetric"],
+                nodes_to_exclude=config["nodes_to_exclude"],
+            )
         quant.process()
         # topologically sort the graph at the end since previous optimizations may have broken it
         quant.model.topological_sort()
         # quant.model._check_init is not needed since it's only meant for float8 quantization
 
         # save the model to the output path and return the model
         return model_proto_to_olive_model(quant.model.model, output_model_path, config)
+
+
+def _validate_accuracy_level(v, values, field):
+    if not v:
+        return v
+
+    if v not in (0, 1, 2, 3, 4):
+        raise ValueError(f"OnnxMatMul4Quantizer {field.name} must be 0(unset), 1(fp32), 2(fp16), 3(bf16) or 4(int8)")
+
+    return v
+
+
+def _validate_algorithm(v, values, field):
+    if not v:
+        return v
+
+    if v not in ("DEFAULT", "HQQ", "RTN", "GPTQ"):
+        raise ValueError(f"OnnxMatMul4Quantizer {field.name} must be 'DEFAULT', 'HQQ', 'RTN', 'GPTQ'")
+
+    return v
+
+
+def _validate_weight_only_quant_config(v, values, field):
+    if values.get("algorithm") is None:
+        logger.debug("algorithm is not set, skip validation for weight_only_quant_configs")
+        return v
+
+    if v is None:
+        v = {}
+
+    config_keys = list(v.keys())
+    if values["algorithm"] == "DEFAULT":
+        default_config_keys = ["block_size", "is_symmetric", "accuracy_level"]
+    elif values["algorithm"] == "RTN":
+        default_config_keys = ["ratios"]
+    elif values["algorithm"] == "HQQ":
+        default_config_keys = ["block_size", "bits", "axis"]
+    elif values["algorithm"] == "GPTQ":
+        default_config_keys = ["percdamp", "block_size", "actorder", "mse", "perchannel"]
+
+    if not all(key in default_config_keys for key in config_keys):
+        invalid_config_keys = set(config_keys) - set(default_config_keys)
+        logger.warning(
+            "Invalid weight_only_quant_configs: %s for algorithm %s. Allowed keys are: %s",
+            invalid_config_keys,
+            values["algorithm"],
+            default_config_keys,
+        )
+        v = {key: v[key] for key in default_config_keys if key in v}
+    return v
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## olive/passes/onnx/transformer_optimization.py

```diff
@@ -26,24 +26,26 @@
 class OrtTransformersOptimization(Pass):
     """Use ONNX Transformer Optimizer to optimize transformer based models.
 
     Optimize transformer based models in scenarios where ONNX Runtime does not apply the optimization at load time.
     It is based on onnxruntime.transformers.optimizer.
     """
 
+    run_on_target = True
+
     @staticmethod
     def is_accelerator_agnostic(accelerator_spec: AcceleratorSpec) -> bool:
         """Override this method to return False by using the accelerator spec information."""
         from onnxruntime import __version__ as OrtVersion
         from packaging import version
 
         return version.parse(OrtVersion) < version.parse("1.17.0")
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         from onnxruntime.transformers.fusion_options import FusionOptions
 
         is_gpu = accelerator_spec.accelerator_type == Device.GPU
 
         config = {
             "model_type": PassConfigParam(
                 type_=str,
@@ -224,24 +226,28 @@
         if model.model_attributes:
             model_attributes = model.model_attributes
             input_model_type = model_attributes.get("model_type")
             if input_model_type:
                 model_type = MODEL_TYPE_MAPPING.get(input_model_type, input_model_type)
             else:
                 model_type = None
+            if not run_config["model_type"] and model_type:
+                logger.debug("model_type is set to %s from model attributes", model_type)
             run_config["model_type"] = run_config["model_type"] or model_type
             if run_config["num_heads"] == 0:
                 for num_heads_name in NUM_HEADS_NAMES:
                     if num_heads_name in model_attributes:
                         run_config["num_heads"] = model_attributes[num_heads_name]
+                        logger.debug("num_heads is set to %d from model attributes", run_config["num_heads"])
                         break
             if run_config["hidden_size"] == 0:
                 for hidden_size_name in HIDDEN_SIZE_NAMES:
                     if hidden_size_name in model_attributes:
                         run_config["hidden_size"] = model_attributes[hidden_size_name]
+                        logger.debug("hidden_size is set to %d from model attributes", run_config["hidden_size"])
                         break
 
         if run_config["model_type"] is None or run_config["model_type"] not in transformers_optimizer.MODEL_TYPES:
             raise ValueError(
                 f"Unsupported model type: {run_config['model_type']}, please select one from "
                 f"[{', '.join(transformers_optimizer.MODEL_TYPES.keys())}] which need to be set under "
                 "OrtTransformersOptimization.config"
@@ -399,8 +405,9 @@
                 name=node.name.replace("MultiHeadAttention", "GroupQueryAttention"),
                 domain="com.microsoft",
                 num_heads=num_heads_mha // world_size,
                 kv_num_heads=num_heads_mha // world_size if kv_num_heads == 0 else kv_num_heads // world_size,
             )
             model.model.graph.node.remove(node)
             model.model.graph.node.extend([gqa_node])
+        logger.info("Replaced %d MultiHeadAttention nodes with GroupQueryAttention", len(mha_nodes))
         return model
```

## olive/passes/onnx/vitis_ai_quantization.py

```diff
@@ -218,26 +218,27 @@
 class VitisAIQuantization(Pass):
     """Quantize ONNX model with onnxruntime.
 
     We can search for best parameters for vai_q_onnx quantization at same time.
     """
 
     _requires_user_script = True
+    run_on_target = True
 
     def _initialize(self):
         super()._initialize()
         self.tmp_dir = tempfile.TemporaryDirectory(prefix="olive_vaiq_tmp")
 
     @staticmethod
     def is_accelerator_agnostic(accelerator_spec: AcceleratorSpec) -> bool:
         """Override this method to return False by using the accelerator spec information."""
         return False
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "quant_mode": PassConfigParam(
                 type_=str,
                 default_value="static",
                 searchable_values=Categorical(["static"]),
                 description="""
                     Onnx Quantization mode.
```

## olive/passes/onnx/vitis_ai/__init__.py

```diff
@@ -3,18 +3,15 @@
 # SPDX-License-Identifier: MIT
 #
 from onnxruntime.quantization.calibrate import CalibrationDataReader
 from onnxruntime.quantization.quant_utils import QuantFormat, QuantType
 
 from olive.passes.onnx.vitis_ai.quant_utils import PowerOfTwoMethod
 from olive.passes.onnx.vitis_ai.quantize import quantize_static
-from olive.passes.onnx.vitis_ai.quantizer import VitisQDQQuantizer, VitisQOpQuantizer
 
 __all__ = [
     "CalibrationDataReader",
-    "VitisQDQQuantizer",
-    "VitisQOpQuantizer",
     "quantize_static",
     "PowerOfTwoMethod",
     "QuantFormat",
     "QuantType",
 ]
```

## olive/passes/openvino/__init__.py

```diff
@@ -1,11 +1,4 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
-from olive.passes.openvino.conversion import OpenVINOConversion
-from olive.passes.openvino.quantization import OpenVINOQuantization
-
-__all__ = [
-    "OpenVINOConversion",
-    "OpenVINOQuantization",
-]
```

## olive/passes/openvino/conversion.py

```diff
@@ -13,16 +13,16 @@
 
 
 class OpenVINOConversion(Pass):
     """Converts PyTorch, ONNX or TensorFlow Model to OpenVino Model."""
 
     _requires_user_script = True
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "input": PassConfigParam(
                 type_=Union[Callable, str, List],
                 required=False,
                 description=(
                     "Set or override shapes for model inputs. "
                     "It configures dynamic and static dimensions in model inputs "
```

## olive/passes/openvino/quantization.py

```diff
@@ -67,16 +67,16 @@
     """Post-training quantization for OpenVINO model.
 
     Please refer to https://docs.openvino.ai/2023.3/ptq_introduction.html for more details.
     """
 
     _requires_user_script = True
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "dataloader_func": PassConfigParam(
                 type_=Union[Callable, str],
                 required=False,
                 category=ParamCategory.OBJECT,
                 description=(
                     "Function/function name to generate dataloader for calibration, required if data_config is None."
@@ -215,17 +215,14 @@
             kwargs = {config.get("ignored_scope_type"): config.get("ignored_scope")}
             extra_params["ignored_scopes"] = nncf.IgnoredScope(**kwargs)
 
         return extra_params
 
 
 class OpenVINOQuantization(OpenVINOQuantizationBase):
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
-        return OpenVINOQuantizationBase._default_config(accelerator_spec)
 
     def _run_for_config(
         self, model: OpenVINOModelHandler, data_root: str, config: Dict[str, Any], output_model_path: str
     ) -> OpenVINOModelHandler:
         try:
             import nncf
             import openvino as ov
@@ -243,16 +240,16 @@
         model_name = "ov_model"
         output_dir = Path(output_model_path) / model_name
         ov.save_model(quantized_model, output_model=output_dir.with_suffix(".xml"))
         return OpenVINOModelHandler(model_path=output_model_path)
 
 
 class OpenVINOQuantizationWithAccuracy(OpenVINOQuantizationBase):
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "validation_func": PassConfigParam(
                 type_=Union[Callable, str],
                 required=False,
                 category=ParamCategory.OBJECT,
                 description=(
                     "Used to compute accuracy metric. "
@@ -276,15 +273,15 @@
                 default_value=DropTypeEnum.ABSOLUTE,
                 description=(
                     "Defines the type of the max_drop. Supported values: 'ABSOLUTE', 'RELATIVE'. "
                     "The default value is 'ABSOLUTE'."
                 ),
             ),
         }
-        config.update(OpenVINOQuantizationBase._default_config(accelerator_spec))
+        config.update(super()._default_config(accelerator_spec))
         return config
 
     def _run_for_config(
         self, model: OliveModelHandler, data_root: str, config: Dict[str, Any], output_model_path: str
     ) -> OliveModelHandler:
         try:
             import nncf
```

## olive/passes/pytorch/__init__.py

```diff
@@ -1,19 +1,4 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
-from olive.passes.pytorch.lora import LoftQ, LoRA, QLoRA
-from olive.passes.pytorch.quantization_aware_training import QuantizationAwareTraining
-from olive.passes.pytorch.sparsegpt import SparseGPT
-from olive.passes.pytorch.tensor_parallel import PyTorchTensorParallel
-from olive.passes.pytorch.torch_trt_conversion import TorchTRTConversion
-
-__all__ = [
-    "LoftQ",
-    "LoRA",
-    "PyTorchTensorParallel",
-    "QLoRA",
-    "QuantizationAwareTraining",
-    "SparseGPT",
-    "TorchTRTConversion",
-]
```

## olive/passes/pytorch/lora.py

```diff
@@ -153,16 +153,16 @@
 class LoRABase(Pass):
     """Base class for LoRA and QLoRA fine-tuning passes."""
 
     # these are the attributes of the model (in hf_config) that will be overwritten by the pass
     # values from the input model will be ignored and new values will be set based on the pass config
     model_overwrites: ClassVar[tuple] = ("torch_dtype", "device_map")
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "use_ort_trainer": PassConfigParam(
                 type_=bool, default_value=False, description="Whether or not to use ORTTrainer."
             ),
             "ortmodule_onnx_opset_version": PassConfigParam(
                 type_=int,
                 default_value=16,
@@ -667,15 +667,17 @@
 
         if torch.cuda.is_available():
             torch.backends.cuda.matmul.allow_tf32 = allow_tf32
 
         # save adapter weights
         adapter_path = Path(output_model_path) / "adapter"
         adapter_path.mkdir(parents=True, exist_ok=True)
-        model.save_pretrained(adapter_path)
+        # don't save embedding layers since only adapter weights are trained
+        # if we don't provide as False, it defaults to "auto" which checks if the vocab size changed
+        model.save_pretrained(adapter_path, save_embedding_layers=False)
 
         # remove loaded model
         output_model.model = None
         del model
         if torch.cuda.is_available():
             torch.cuda.empty_cache()
         # remove the device map since we don't want "auto" device map
@@ -689,15 +691,20 @@
         output_model.set_resource("adapter_path", adapter_path)
         return output_model
 
     @staticmethod
     def smart_tokenizer_and_embedding_resize(
         special_tokens_dict: Dict, tokenizer: "PreTrainedTokenizer", model: "PreTrainedModel"
     ):
-        """Resize the tokenizer and the model embedding layer to take into account new special tokens."""
+        """Resize the tokenizer and the model embedding layer to take into account new special tokens.
+
+        NOTE: This is only used to ensure we have a pad token. The new embeddings don't get training signals
+        the pad tokens are masked out in the attention mask and loss calculation. Moreover, only the adapter weights
+        are set as trainable and saved in the final checkpoint.
+        """
         # resize tokenizer
         num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)
         # resize model embedding layer
         model.resize_token_embeddings(len(tokenizer))
         if num_new_tokens > 0:
             logger.info("Added %d new tokens to tokenizer and resized model embedding layer.", num_new_tokens)
             input_embeddings_data = model.get_input_embeddings().weight.data
@@ -767,20 +774,20 @@
 
 class LoRA(LoRABase):
     """Run LoRA fine-tuning on a Hugging Face PyTorch model.
 
     This pass only supports PyTorchModelHandler with hf_config.
     """
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "target_modules": PassConfigParam(type_=List[str], default_value=None, description="Target modules"),
         }
-        config.update(LoRABase._default_config(accelerator_spec))
+        config.update(super()._default_config(accelerator_spec))
         return config
 
     def _run_for_config(
         self, model: PyTorchModelHandler, data_root: str, config: Dict[str, Any], output_model_path: str
     ) -> PyTorchModelHandler:
         # convert config to pass config class
         # this will validate the config and convert to the correct types
@@ -816,27 +823,27 @@
 
 
 class QLoRABase(LoRABase):
     """Base class for QLoRA and LoftQ fine-tuning passes."""
 
     model_overwrites: ClassVar[tuple] = ("torch_dtype", "device_map", "quantization_method", "quantization_config")
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             # quantization parameters
             "compute_dtype": PassConfigParam(
                 type_=str,
                 description=(
                     "Computation data type for the quantized modules. If not provided, will use the same dtype as"
                     " torch_dtype"
                 ),
             )
         }
-        config.update(LoRABase._default_config(accelerator_spec))
+        config.update(super()._default_config(accelerator_spec))
         return config
 
     def _run_for_config(
         self, model: PyTorchModelHandler, data_root: str, config: Dict[str, Any], output_model_path: str
     ) -> PyTorchModelHandler:
         # convert config to pass config class
         # this will validate the config and convert to the correct types
@@ -879,16 +886,16 @@
 
 class QLoRA(QLoRABase):
     """Run QLoRA fine-tuning on a Hugging Face PyTorch model.
 
     This pass only supports PyTorchModelHandler with hf_config.
     """
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             # quantization parameters
             "double_quant": PassConfigParam(
                 type_=bool,
                 default_value=False,
                 description=(
                     "Whether to use nested quantization where the quantization constants from the first quantization"
@@ -897,15 +904,15 @@
             ),
             "quant_type": PassConfigParam(
                 type_=str,
                 default_value="nf4",
                 description="Quantization data type to use. Should be one of `fp4` or `nf4`.",
             ),
         }
-        config.update(QLoRABase._default_config(accelerator_spec))
+        config.update(super()._default_config(accelerator_spec))
         return config
 
     def get_model_tokenizer(
         self, model: PyTorchModelHandler, config: ConfigBase, output_model_path: str
     ) -> Tuple[PyTorchModelHandler, "PreTrainedModel", "PreTrainedTokenizer", List[str]]:
         """Get the model handler, LoRA model and tokenizer for QLoRA fine-tuning.
 
@@ -944,31 +951,31 @@
         pytorch_model = self.enable_lora(
             pytorch_model, tokenizer, new_model_handler.hf_config.task, config, target_modules=quantized_modules
         )
 
         return new_model_handler, pytorch_model, tokenizer, quantized_modules
 
 
-class LoftQ(QLoRA):
+class LoftQ(QLoRABase):
     """Run LoftQ fine-tuning on a Hugging Face PyTorch model.
 
     This pass only supports PyTorchModelHandler with hf_config.
     """
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             # quantization parameters
             "loftq_iter": PassConfigParam(
                 type_=int,
                 default_value=1,
                 description="Number of LoftQ iterations.",
             ),
         }
-        config.update(QLoRABase._default_config(accelerator_spec))  # pylint: disable=protected-access
+        config.update(super()._default_config(accelerator_spec))
         return config
 
     @classmethod
     def check_dependencies(cls, config: ConfigBase, is_qlora: bool = False):
         """Check dependencies for the pass."""
         super().check_dependencies(config, is_qlora=is_qlora)
```

## olive/passes/pytorch/quantization_aware_training.py

```diff
@@ -13,16 +13,16 @@
 
 
 class QuantizationAwareTraining(Pass):
     """Run quantization aware training on PyTorch model."""
 
     _requires_user_script = True
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         import pytorch_lightning
         from packaging import version
 
         if version.parse(pytorch_lightning.__version__) >= version.parse("1.9.0"):
             from pytorch_lightning.loggers import Logger
         else:
             from pytorch_lightning.loggers import LightningLoggerBase as Logger
```

## olive/passes/pytorch/sparsegpt.py

```diff
@@ -34,16 +34,16 @@
 
     See https://arxiv.org/abs/2301.00774 for more details on the algorithm.
 
     This pass only supports PyTorchModelHandler with hf_config. The transformers model type
     must be one of [bloom, gpt2, gpt_neox, llama, opt].
     """
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "sparsity": PassConfigParam(
                 type_=Union[float, List[int]],
                 description=(
                     "Target sparsity. This can be a float or a list of two integers. Float is the target sparsity per"
                     " layer. List [n,m] applies semi-structured (n:m) sparsity patterns. Refer to"
                     " https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/"
@@ -66,17 +66,17 @@
             ),
             "layer_name_filter": PassConfigParam(
                 type_=Union[str, List[str]],
                 default_value=None,
                 description="Only prune layers whose name contains the given string(s).",
             ),
             # this is not the same as accelerator_spec.device which is the target device for inference
-            # compute_device is the device we want to run the algorithm on, does not affect the final model
-            # so accelerator_spec.device can be cpu but compute_device can be cuda for faster pass execution
-            "compute_device": PassConfigParam(
+            # device is the device we want to run the algorithm on, does not affect the final model
+            # so accelerator_spec.device can be cpu but device can be cuda for faster pass execution
+            "device": PassConfigParam(
                 type_=str,
                 default_value="auto",
                 description=(
                     "Device to use for performing computations. Can be 'auto, 'cpu', 'cuda', 'cuda:0', etc. If 'auto',"
                     " will use cuda if available. Does not affect the final model."
                 ),
             ),
@@ -104,15 +104,15 @@
         elif isinstance(config["sparsity"], list):
             assert len(config["sparsity"]) == 2, "Sparsity must be a float or a list of two integers."
         mode = "unstructured" if isinstance(config["sparsity"], float) else "structured"
         sparsity = config["sparsity"]
         n, m = sparsity if mode == "structured" else [0, 0]
 
         # get device to use for computations
-        device = config["compute_device"]
+        device = config["device"]
         if device == "auto":
             device = "cuda" if torch.cuda.is_available() else "cpu"
         logger.debug(
             "Running SparseGPT on %s with model_type: %s, mode: %s, sparsity: %s", device, model_type, mode, sparsity
         )
 
         # load_data
```

## olive/passes/pytorch/tensor_parallel.py

```diff
@@ -44,16 +44,16 @@
 
     @abstractmethod
     def load_rank_weights(self, model: torch.nn.Module):
         raise NotImplementedError
 
 
 class PyTorchTensorParallel(Pass):
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         # Note : The default world_size should be the no of gpus (AcceleratorSpec.Device == GPU)
         # in the target OliveSystem
         return {
             "script_dir": PassConfigParam(
                 type_=str,
                 required=False,
                 category=ParamCategory.PATH,
@@ -91,21 +91,17 @@
     @staticmethod
     def _validate_world_size(v):
         if int(v) < 2:
             raise ValueError("world_size should be >= 2")
 
         return v
 
-    @staticmethod
-    def _validators() -> Dict[str, Callable]:
-        return {
-            "validate_distributor_config": validator("world_size", allow_reuse=True)(
-                PyTorchTensorParallel._validate_world_size
-            )
-        }
+    @classmethod
+    def _validators(cls) -> Dict[str, Callable]:
+        return {"validate_distributor_config": validator("world_size", allow_reuse=True)(cls._validate_world_size)}
 
     @staticmethod
     def _generate_one(params):
         model_config, rank, world_size, output_filepath = params
 
         logger.debug("Exporting tensor parallel model for rank: %d, %s", rank, output_filepath)
 
@@ -168,15 +164,15 @@
         max_parallel_jobs = min(world_size, config["parallel_jobs"] or multiprocessing.cpu_count())
         if max_parallel_jobs <= 1:
             results = [PyTorchTensorParallel._generate_one(_) for _ in params]
         else:
             with multiprocessing.Pool(processes=max_parallel_jobs) as pool:
                 results = pool.map(PyTorchTensorParallel._generate_one, params)
 
-        if self.accelerator_spec.accelerator_type == Device.GPU and torch.cuda.is_available():
+        if self.host_device == Device.GPU and torch.cuda.is_available():
             torch.cuda.empty_cache()
 
         if world_size != sum(results):
             raise RuntimeError("Failed to create ranked tensor parallel models")
 
         # Finally, create DistributedPyTorchModel from ranked models for each rank
         model_config = model.to_json()["config"]
```

## olive/passes/pytorch/torch_trt_conversion.py

```diff
@@ -33,16 +33,16 @@
     The entire model is saved using `torch.save` and can be loaded using `torch.load`. Loading the model requires
     `torch-tensorrt` and Olive to be installed.
 
     This pass only supports PyTorchModelHandler with hf_config. The transformers model type
     must be one of [bloom, gpt2, gpt_neox, llama, opt].
     """
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "min_layer": PassConfigParam(
                 type_=int, default_value=None, description="Convert all layers with id >= min_layer."
             ),
             "max_layer": PassConfigParam(
                 type_=int, default_value=None, description="Convert all layers with id < max_layer."
             ),
```

## olive/passes/qnn/__init__.py

```diff
@@ -1,10 +1,4 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
-
-from olive.passes.qnn.context_binary_generator import QNNContextBinaryGenerator
-from olive.passes.qnn.conversion import QNNConversion
-from olive.passes.qnn.model_lib_generator import QNNModelLibGenerator
-
-__all__ = ["QNNConversion", "QNNModelLibGenerator", "QNNContextBinaryGenerator"]
```

## olive/passes/qnn/context_binary_generator.py

```diff
@@ -2,39 +2,38 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 import logging
 import platform
 from pathlib import Path
-from typing import Any, Callable, Dict
+from typing import Any, Dict
 
 from olive.constants import ModelFileFormat
 from olive.hardware import AcceleratorSpec
 from olive.model import QNNModelHandler
 from olive.passes.olive_pass import Pass
 from olive.passes.pass_config import PassConfigParam
-from olive.passes.qnn.common import get_env_config
 from olive.platform_sdk.qualcomm.runner import QNNSDKRunner
 
 logger = logging.getLogger(__name__)
 
 
 class QNNContextBinaryGenerator(Pass):
     """Create QNN context binary from a QNN model library using a particular backend.
 
     Uses qnn-context-binary-generator tool from the QNN SDK.
     """
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         if platform.system() == "Windows":
             raise NotImplementedError("QNNContextBinaryGenerator is not supported on Windows.")
 
-        config = {
+        return {
             "backend": PassConfigParam(
                 type_=str,
                 required=True,
                 description=("Path to a QNN backend .so library to create the context binary."),
             ),
             "binary_file": PassConfigParam(
                 type_=str,
@@ -46,20 +45,14 @@
                     " is created."
                 ),
             ),
             "extra_args": PassConfigParam(
                 type_=str, default_value=None, description="Extra arguments to qnn-context-binary-generator"
             ),
         }
-        config.update(get_env_config())
-        return config
-
-    @staticmethod
-    def _validators() -> Dict[str, Callable[..., Any]]:
-        pass
 
     def _run_for_config(
         self,
         model: QNNModelHandler,
         data_root: str,
         config: Dict[str, Any],
         output_model_path: str,
@@ -82,13 +75,13 @@
             f"--model {model.model_path}",
             f"--backend {config['backend']}",
             f"--output_dir {output_model_path}",
             f"--binary_file {binary_file}" if binary_file else "",
             config["extra_args"] or "",
         ]
 
-        runner.run(" ".join(cmd_list), use_olive_env=config["use_olive_env"])
+        runner.run(" ".join(cmd_list))
         return QNNModelHandler(
             output_model_full_path,
             model_file_format=ModelFileFormat.QNN_SERIALIZED_BIN,
             model_attributes={"backend": config["backend"]},
         )
```

## olive/passes/qnn/conversion.py

```diff
@@ -1,36 +1,35 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 import platform
 from pathlib import Path
-from typing import Any, Callable, Dict, List, Union
+from typing import Any, Dict, List, Union
 
 from olive.constants import ModelFileFormat
 from olive.hardware import AcceleratorSpec
 from olive.model import ONNXModelHandler, PyTorchModelHandler, QNNModelHandler, TensorFlowModelHandler
 from olive.model.utils import normalize_path_suffix
 from olive.passes.olive_pass import Pass
 from olive.passes.pass_config import PassConfigParam
-from olive.passes.qnn.common import get_env_config
 from olive.platform_sdk.qualcomm.runner import QNNSDKRunner
 
 
 class QNNConversion(Pass):
     """Convert ONNX, TensorFlow, or PyTorch model to QNN C++ model.
 
     Quantize the model if `--input_list` is provided as extra_args.
     Uses qnn-[framework]-converter tool from the QNN SDK.
     """
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
-        config = {
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+        return {
             # input_network is required for qnn conversion, but we don't have it in the config.
             # The `input_network` will be set in the runtime.
             "input_dim": PassConfigParam(
                 type_=List[str],
                 required=False,
                 description=(
                     "The names and dimensions of the network input layers specified in the format"
@@ -59,20 +58,14 @@
                 description=(
                     "Extra arguments to pass to qnn-[framework]-converter tool, e.g."
                     " --show_unconsumed_nodes --custom_io CUSTOM_IO. See the documentation for more details:"
                     " https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/tools.html"
                 ),
             ),
         }
-        config.update(get_env_config())
-        return config
-
-    @staticmethod
-    def _validators() -> Dict[str, Callable[..., Any]]:
-        pass
 
     def _run_for_config(
         self,
         model: Union[TensorFlowModelHandler, PyTorchModelHandler, ONNXModelHandler],
         data_root: str,
         config: Dict[str, Any],
         output_model_path: str,
@@ -115,9 +108,9 @@
             converter_program,
             f"--input_network {model.model_path}",
             f"--output_path {output_model_path}",
             " ".join([f"--input_dim {i}" for i in input_dims]) if input_dims else "",
             " ".join([f"--out_node {o}" for o in out_nodes]) if out_nodes else "",
             config["extra_args"] or "",
         ]
-        runner.run(" ".join(cmd_list), use_olive_env=config["use_olive_env"])
+        runner.run(" ".join(cmd_list))
         return QNNModelHandler(output_model_path, model_file_format=ModelFileFormat.QNN_CPP)
```

## olive/passes/qnn/model_lib_generator.py

```diff
@@ -2,36 +2,35 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 
 import logging
 import platform
 from pathlib import Path
-from typing import Any, Callable, Dict
+from typing import Any, Dict
 
 from olive.constants import ModelFileFormat
 from olive.hardware import AcceleratorSpec
 from olive.model import QNNModelHandler
 from olive.passes.olive_pass import Pass
 from olive.passes.pass_config import PassConfigParam
-from olive.passes.qnn.common import get_env_config
 from olive.platform_sdk.qualcomm.runner import QNNSDKRunner
 
 logger = logging.getLogger(__name__)
 
 
 class QNNModelLibGenerator(Pass):
     """Compile QNN C++ model source code into QNN model library for a specific target.
 
     Uses qnn-model-lib-generator tool from the QNN SDK.
     """
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
-        config = {
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+        return {
             "lib_targets": PassConfigParam(
                 type_=str,
                 required=False,
                 description=(
                     "Specifies the targets to build the models for. Default: aarch64-android x86_64-linux-clang"
                 ),
             ),
@@ -40,20 +39,14 @@
                 required=False,
                 description=(
                     "Specifies the name to use for libraries. Default: uses name in <model.bin> if provided, "
                     " else generic qnn_model.so"
                 ),
             ),
         }
-        config.update(get_env_config())
-        return config
-
-    @staticmethod
-    def _validators() -> Dict[str, Callable[..., Any]]:
-        pass
 
     def _run_for_config(
         self,
         model: QNNModelHandler,
         data_root: str,
         config: Dict[str, Any],
         output_model_path: str,
@@ -83,9 +76,9 @@
             f"-c {model.model_path}",
             f"-b {input_model_bin}" if input_model_bin else "",
             f"-t {config['lib_targets']}" if config.get("lib_targets") else "",
             f"-l {config['lib_name']}" if config.get("lib_name") else "",
             f"-o {output_model_path}",
         ]
         runner = QNNSDKRunner(use_dev_tools=True)
-        runner.run(" ".join(cmd_list), use_olive_env=config["use_olive_env"])
+        runner.run(" ".join(cmd_list))
         return QNNModelHandler(output_model_path, model_file_format=ModelFileFormat.QNN_LIB)
```

## olive/passes/snpe/__init__.py

```diff
@@ -1,9 +1,4 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
-from olive.passes.snpe.conversion import SNPEConversion
-from olive.passes.snpe.quantization import SNPEQuantization
-from olive.passes.snpe.snpe_to_onnx import SNPEtoONNXConversion
-
-__all__ = ["SNPEConversion", "SNPEQuantization", "SNPEtoONNXConversion"]
```

## olive/passes/snpe/conversion.py

```diff
@@ -40,16 +40,16 @@
 
 class SNPEConversion(Pass):
     """Convert ONNX or TensorFlow model to SNPE DLC.
 
     Uses snpe-tensorflow-to-dlc or snpe-onnx-to-dlc tools from the SNPE SDK.
     """
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "input_names": PassConfigParam(type_=List[str], required=True, description="List of input names."),
             "input_shapes": PassConfigParam(
                 type_=List[List[int]],
                 required=True,
                 description="List of input shapes. Must be the same length as input_names.",
             ),
@@ -80,30 +80,28 @@
                     " snpe-tensorflow-to-dlc at https://developer.qualcomm.com/sites/default/files/docs/snpe/tools.html"
                     " for more additional arguments. The value is a string that will be passed as is to the tool."
                     " e.g.: --enable_cpu_fallback --priority_hint low"
                 ),
             ),
         }
 
-    @staticmethod
-    def _validators() -> Dict[str, Callable]:
+    @classmethod
+    def _validators(cls) -> Dict[str, Callable]:
         return {
             "validate_input_types_layouts": validator("input_types", "input_layouts", allow_reuse=True)(
                 _validate_input_types_layouts
             )
         }
 
     def _run_for_config(
         self,
         model: Union[ONNXModelHandler, TensorFlowModelHandler],
         data_root: str,
         config: Dict[str, Any],
         output_model_path: str,
     ) -> SNPEModelHandler:
-        config = self._config_class(**config)
-
         if Path(output_model_path).suffix != ".dlc":
             output_model_path += ".dlc"
 
-        to_dlc(model.model_path, model.framework, config.dict(), output_model_path)
-        io_config = get_dlc_io_config(output_model_path, config.input_names, config.output_names)
+        to_dlc(model.model_path, model.framework, config, output_model_path)
+        io_config = get_dlc_io_config(output_model_path, config["input_names"], config["output_names"])
         return SNPEModelHandler(model_path=LocalFile({"path": output_model_path}), **io_config)
```

## olive/passes/snpe/quantization.py

```diff
@@ -22,16 +22,16 @@
     """Quantize SNPE model.
 
     Uses snpe-dlc-quantize tool from the SNPE SDK.
     """
 
     _requires_user_script = True
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         return {
             "data_dir": PassConfigParam(
                 type_=OLIVE_RESOURCE_ANNOTATIONS,
                 required=False,
                 category=ParamCategory.DATA,
                 description="Path to the data directory. Required is data_config is None.",
             ),
@@ -47,15 +47,14 @@
             ),
             "dataloader_func_kwargs": PassConfigParam(
                 type_=Dict[str, Any],
                 description="Keyword arguments for dataloader_func.",
             ),
             "data_config": PassConfigParam(
                 type_=Union[DataConfig, Dict],
-                required=True,
                 description="Data config for quantization, required if dataloader_func is None",
             ),
             "use_enhanced_quantizer": PassConfigParam(
                 type_=bool,
                 default_value=False,
                 searchable_values=Boolean(),
                 description=(
@@ -64,15 +63,15 @@
                     " have long tails in the distribution of the data being quantized."
                 ),
             ),
             "enable_htp": PassConfigParam(
                 type_=bool,
                 default_value=False,
                 searchable_values=Boolean(),
-                description="Pack HTP information in quantized DLC.",
+                description="Pack HTP information in quantized DLC, which is not available in Windows.",
             ),
             "htp_socs": PassConfigParam(
                 type_=List[str], default_value=None, description="List of SoCs to generate HTP Offline cache for."
             ),
             "extra_args": PassConfigParam(
                 type_=str,
                 default_value=None,
```

## olive/passes/snpe/snpe_to_onnx.py

```diff
@@ -24,32 +24,32 @@
 
 class SNPEtoONNXConversion(Pass):
     """Convert a SNPE DLC to ONNX to use with SNPE Execution Provider.
 
     Creates a ONNX graph with the SNPE DLC as a node.
     """
 
-    @staticmethod
-    def _default_config(accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
+    @classmethod
+    def _default_config(cls, accelerator_spec: AcceleratorSpec) -> Dict[str, PassConfigParam]:
         config = {
             "target_device": PassConfigParam(
                 type_=str,
                 default_value="cpu",
                 description=(
                     "Target device for the ONNX model. Refer to"
                     " oliveolive.platform_sdk.qualcomm.constants.SNPEDevice for valid values."
                 ),
             ),
             "target_opset": PassConfigParam(type_=int, default_value=12, description="Target ONNX opset version."),
         }
         config.update(get_external_data_config())
         return config
 
-    @staticmethod
-    def _validators() -> Dict[str, Callable]:
+    @classmethod
+    def _validators(cls) -> Dict[str, Callable]:
         return {
             "validate_target_device": validator("target_device", allow_reuse=True)(_validate_target_device),
         }
 
     def _run_for_config(
         self, model: SNPEModelHandler, data_root: str, config: Dict[str, Any], output_model_path: str
     ) -> ONNXModelHandler:
```

## olive/platform_sdk/qualcomm/runner.py

```diff
@@ -1,21 +1,27 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import logging
+import os
+import shlex
+import shutil
 import time
 from pathlib import Path
 
 from olive.common.utils import run_subprocess
 from olive.platform_sdk.qualcomm.qnn.env import QNNSDKEnv
 from olive.platform_sdk.qualcomm.snpe.env import SNPESDKEnv
 
 logger = logging.getLogger(__name__)
 
+USE_OLIVE_ENV = "USE_OLIVE_ENV"
+USE_OLIVE_ENV_DEFAULT_VALUE = "1"
+
 
 class SDKRunner:
     def __init__(
         self,
         platform,
         use_dev_tools: bool = False,
         runs: int = 1,
@@ -30,39 +36,48 @@
         if self.platform not in ("SNPE", "QNN"):
             raise ValueError(f"Unsupported platform {platform}")
         elif self.platform == "SNPE":
             self.sdk_env = SNPESDKEnv(use_dev_tools=self.use_dev_tools)
         elif self.platform == "QNN":
             self.sdk_env = QNNSDKEnv(use_dev_tools=self.use_dev_tools)
 
+    def _use_olive_env(self):
+        return os.environ.get(USE_OLIVE_ENV, USE_OLIVE_ENV_DEFAULT_VALUE) == USE_OLIVE_ENV_DEFAULT_VALUE
+
     def _resolve_cmd(self, cmd: str):
         import platform
 
         if platform.system() == "Windows" and cmd.startswith(("snpe-", "qnn-")):
             logger.debug("Resolving command %s on Windows.", cmd)
-            cmd_path = Path(self.sdk_env.sdk_root_path) / "bin" / self.sdk_env.target_arch
+            cmd_dir = Path(self.sdk_env.sdk_root_path) / "bin" / self.sdk_env.target_arch
             cmd_name = cmd.split(" ")[0]
-            if (cmd_path / cmd_name).exists():
-                cmd = str(cmd_path / cmd_name) + cmd[len(cmd_name) :]
+            cmd_full_path = cmd_dir / cmd_name
+            if cmd_full_path.with_suffix(".exe").exists():
+                cmd_full_path = cmd_full_path.with_suffix(".exe")
+            if cmd_full_path.exists():
+                cmd = str(cmd_full_path) + cmd[len(cmd_name) :]
                 try:
-                    with (cmd_path / cmd_name).open() as f:
+                    with cmd_full_path.open() as f:
                         first_line = f.readline()
                         if "python" in first_line:
                             cmd = f"python {cmd}"
                 except UnicodeDecodeError as e:
                     logger.warning(
                         "Failed to read the first line of %s: %s. Will ignore to wrap it with python.", cmd_name, e
                     )
-
+        if isinstance(cmd, str):
+            cmd = shlex.split(cmd, posix=(platform.system() != "Windows"))
+            path_env = self.sdk_env.env.get("PATH") if self._use_olive_env() else None
+            cmd[0] = shutil.which(cmd[0], path=path_env) or cmd[0]
         return cmd
 
-    def run(self, cmd: str, use_olive_env: bool = True):
+    def run(self, cmd: str):
+        env = self.sdk_env.env if self._use_olive_env() else None
         cmd = self._resolve_cmd(cmd)
 
-        env = self.sdk_env.env if use_olive_env else None
         for run in range(self.runs):
             run_log_msg = "" if self.runs == 1 else f" (run {run + 1}/{self.runs})"
             logger.debug("Running %s command%s: ", self.platform, run_log_msg)
             _, stdout, stderr = run_subprocess(cmd, env, check=True)
             if self.sleep > 0 and run < self.runs - 1:
                 time.sleep(self.sleep)
```

## olive/platform_sdk/qualcomm/snpe/tools/dev.py

```diff
@@ -1,21 +1,25 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import csv
+import logging
+import platform
 import tempfile
 from pathlib import Path
 from typing import List
 
 import onnx
 from onnx import TensorProto, helper
 
 from olive.platform_sdk.qualcomm.runner import SNPESDKRunner as SNPERunner
 
+logger = logging.getLogger(__name__)
+
 
 def get_snpe_version() -> str:
     """Get the version of the SNPE SDK at SNPE_ROOT."""
     cmd = "snpe-net-run --version"
     stdout, _ = SNPERunner().run(cmd)
     return stdout.split("SNPE v")[1].strip()
 
@@ -182,19 +186,27 @@
     input_list: path to the input list file for trial inputs.
     config: a config dict with the following keys:
         use_enhanced_quantizer: bool = whether to use the enhanced quantizer.
         enable_htp: bool = whether to enable HTP.
         htp_socs: List[str] = list of HTP SoCs.
         extra_args: str = extra arguments to pass to the quantizer.
     """
-    cmd = f"snpe-dlc-quantize --input_dlc {dlc_path} --input_list {input_list} --output_dlc {output_file}"
+    quant_cmd = "snpe-dlc-quantize"
+    if platform.system() == "Windows":
+        # snpe-dlc-quant is the Windows version of the quantizer tool
+        # and it does not support the --enable_htp flag
+        quant_cmd = "snpe-dlc-quant"
+    cmd = f"{quant_cmd} --input_dlc {dlc_path} --input_list {input_list} --output_dlc {output_file}"
     if config["use_enhanced_quantizer"]:
         cmd += " --use_enhanced_quantizer"
     if config["enable_htp"]:
-        cmd += " --enable_htp"
+        if platform.system() == "Windows":
+            logger.warning("--enable_htp is not supported on Windows")
+        else:
+            cmd += " --enable_htp"
     if config["htp_socs"] is not None:
         cmd += f" --htp_socs {','.join(config['htp_socs'])}"
     if config["extra_args"] is not None:
         cmd += " " + config["extra_args"]
 
     _, stderr = SNPERunner(use_dev_tools=True).run(cmd)
```

## olive/strategy/search_space.py

```diff
@@ -1,14 +1,14 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from copy import deepcopy
 from random import Random
-from typing import Any, Dict, List, Optional, Tuple
+from typing import Any, Dict, Iterator, List, Optional, Tuple
 
 from olive.strategy.search_parameter import Categorical, Conditional, SearchParameter, SpecialParamValue
 from olive.strategy.utils import order_search_parameters
 
 
 class SearchSpace:
     """Search space for a search algorithm."""
@@ -55,15 +55,15 @@
             if search_point[space_name][param_name] == SpecialParamValue.INVALID:
                 return self.random_sample()
 
         return search_point
 
     def _iterate_util(
         self, full_iter_order: List[Tuple[str, str]], search_point: Dict[str, Dict[str, Any]], index: int
-    ):
+    ) -> Iterator[Dict[str, Dict[str, Any]]]:
         if index == len(full_iter_order):
             yield deepcopy(search_point)
             return
 
         space_name, param_name = full_iter_order[index]
         param = self._search_space[space_name][param_name]
 
@@ -74,15 +74,15 @@
             options = param.get_support()
         for option in options:
             if option == SpecialParamValue.INVALID:
                 continue
             search_point[space_name][param_name] = option
             yield from self._iterate_util(full_iter_order, search_point, index + 1)
 
-    def iterate(self) -> Dict[str, Dict[str, Any]]:
+    def iterate(self) -> Iterator[Dict[str, Dict[str, Any]]]:
         """Iterate over all possible configurations in the search space."""
         # initialize search point
         search_point = deepcopy(self._empty_search_point)
 
         # iterate over search space
         yield from self._iterate_util(self._iter_order, search_point, 0)
 
@@ -97,11 +97,11 @@
             size += 1
         return size
 
     def empty_search_point(self) -> Dict[str, Dict[str, Any]]:
         """Get an empty search point."""
         return deepcopy(self._empty_search_point)
 
-    def iter_params(self) -> Tuple[str, str, SearchParameter]:
+    def iter_params(self) -> Iterator[Tuple[str, str, SearchParameter]]:
         """Iterate over the search parameters in topological order."""
         for space_name, param_name in self._iter_order:
             yield space_name, param_name, self._search_space[space_name][param_name]
```

## olive/strategy/search_strategy.py

```diff
@@ -45,15 +45,15 @@
 
     @validator("search_algorithm_config", pre=True, always=True)
     def _validate_search_algorithm_config(cls, v, values):
         if "search_algorithm" not in values:
             raise ValueError("Invalid search_algorithm")
 
         config_class = REGISTRY[values["search_algorithm"]].get_config_class()
-        return validate_config(v, ConfigBase, config_class)
+        return validate_config(v, config_class)
 
     @validator("stop_when_goals_met", "max_iter", "max_time", pre=True)
     def _validate_stop_when_goals_met(cls, v, values, field):
         if "execution_order" not in values:
             raise ValueError("Invalid execution_order")
         if v and values["execution_order"] != "joint":
             logger.info("%s is only supported for joint execution order. Ignoring...", field.name)
```

## olive/strategy/search_algorithm/exhaustive.py

```diff
@@ -8,16 +8,16 @@
 
 
 class ExhaustiveSearchAlgorithm(SearchAlgorithm):
     """Exhaustive Search Algorithm. Does a grid search over the search space."""
 
     name = "exhaustive"
 
-    @staticmethod
-    def _default_config():
+    @classmethod
+    def _default_config(cls):
         return {}
 
     def initialize(self):
         """Initialize the searcher."""
         self._iterator = self._search_space.iterate()
 
     def suggest(self) -> Dict[str, Dict[str, Any]]:
```

## olive/strategy/search_algorithm/optuna_sampler.py

```diff
@@ -20,16 +20,16 @@
 
 
 class OptunaSearchAlgorithm(SearchAlgorithm):
     """Optuna sampler for search algorithms."""
 
     name = "optuna_sampler"
 
-    @staticmethod
-    def _default_config() -> Dict[str, ConfigParam]:
+    @classmethod
+    def _default_config(cls) -> Dict[str, ConfigParam]:
         return {
             "num_samples": ConfigParam(type_=int, default_value=1, description="Number of samples to suggest."),
             "seed": ConfigParam(type_=int, default_value=1, description="Seed for the rng."),
         }
 
     def initialize(self):
         """Initialize the searcher."""
```

## olive/strategy/search_algorithm/random_sampler.py

```diff
@@ -9,16 +9,16 @@
 
 
 class RandomSearchAlgorithm(SearchAlgorithm):
     """Random Searcher. Samples random points from the search space with or without replacement."""
 
     name = "random"
 
-    @staticmethod
-    def _default_config() -> Dict[str, ConfigParam]:
+    @classmethod
+    def _default_config(cls) -> Dict[str, ConfigParam]:
         return {
             "num_samples": ConfigParam(type_=int, default_value=1, description="Number of samples to suggest."),
             "seed": ConfigParam(type_=int, default_value=1, description="Seed for the rng."),
             "with_replacement": ConfigParam(type_=bool, default_value=False, description="Sample with replacement."),
         }
 
     def initialize(self):
```

## olive/strategy/search_algorithm/search_algorithm.py

```diff
@@ -12,22 +12,17 @@
 if TYPE_CHECKING:
     from olive.strategy.search_parameter import SearchParameter
 
 
 class SearchAlgorithm(AutoConfigClass):
     """Abstract base class for searchers."""
 
+    name: str = None
     registry: ClassVar[Dict[str, Type["SearchAlgorithm"]]] = {}
 
-    @classmethod
-    @property
-    @abstractmethod
-    def name(cls):
-        raise NotImplementedError
-
     def __init__(
         self,
         search_space: Dict[str, Dict[str, "SearchParameter"]],
         objectives: Optional[List[str]] = None,
         higher_is_betters: Optional[List[bool]] = None,
         config: Optional[Union[Dict[str, Any], ConfigBase]] = None,
     ):
```

## olive/strategy/search_algorithm/tpe_sampler.py

```diff
@@ -15,18 +15,18 @@
 
     Refer to https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html
     for more details about the sampler.
     """
 
     name = "tpe"
 
-    @staticmethod
-    def _default_config() -> Dict[str, ConfigParam]:
+    @classmethod
+    def _default_config(cls) -> Dict[str, ConfigParam]:
         return {
-            **OptunaSearchAlgorithm._default_config(),
+            **super()._default_config(),
             "multivariate": ConfigParam(
                 type_=bool, default_value=True, description="Use multivariate TPE when suggesting parameters."
             ),
             "group": ConfigParam(
                 type_=bool,
                 default_value=True,
                 description=(
```

## olive/systems/common.py

```diff
@@ -1,27 +1,38 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from enum import Enum
 from pathlib import Path
-from typing import Optional, Union
+from typing import List, Optional, Union
 
 from olive.common.config_utils import ConfigBase
 from olive.common.pydantic_v1 import validator
 
 
 class SystemType(str, Enum):
     Docker = "Docker"
     Local = "LocalSystem"
     AzureML = "AzureML"
     PythonEnvironment = "PythonEnvironment"
     IsolatedORT = "IsolatedORT"
 
 
+class AcceleratorConfig(ConfigBase):
+    device: str = None
+    execution_providers: List[str] = None
+
+    @validator("execution_providers", always=True)
+    def validate_device_and_execution_providers(cls, v, values):
+        if v is None and values.get("device") is None:
+            raise ValueError("Either device or execution_providers must be provided")
+        return v
+
+
 class AzureMLDockerConfig(ConfigBase):
     base_image: Optional[str] = None
     dockerfile: Optional[str] = None
     build_context_path: Optional[Union[Path, str]] = None
     conda_file_path: Optional[Union[Path, str]] = None
     name: Optional[str] = None
     version: Optional[str] = None
```

## olive/systems/local.py

```diff
@@ -14,17 +14,14 @@
     from olive.evaluator.metric import Metric, MetricResult
     from olive.passes.olive_pass import Pass
 
 
 class LocalSystem(OliveSystem):
     system_type = SystemType.Local
 
-    def __init__(self, accelerators: List[str] = None, hf_token: bool = None):
-        super().__init__(accelerators=accelerators, olive_managed_env=False)
-
     def run_pass(
         self,
         the_pass: "Pass",
         model_config: ModelConfig,
         data_root: str,
         output_model_path: str,
         point: Optional[Dict[str, Any]] = None,
@@ -51,8 +48,8 @@
     def get_supported_execution_providers(self) -> List[str]:
         """Get the available execution providers."""
         import onnxruntime as ort
 
         return ort.get_available_providers()
 
     def remove(self):
-        raise ValueError("Local system does not support system removal")
+        raise NotImplementedError("Local system does not support system removal")
```

## olive/systems/olive_system.py

```diff
@@ -1,16 +1,17 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import logging
 from abc import ABC, abstractmethod
-from typing import TYPE_CHECKING, Any, Dict, List, Optional
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union
 
-from olive.systems.common import SystemType
+from olive.common.config_utils import validate_config
+from olive.systems.common import AcceleratorConfig, SystemType
 
 if TYPE_CHECKING:
     from olive.evaluator.metric import Metric, MetricResult
     from olive.hardware.accelerator import AcceleratorSpec
     from olive.model import ModelConfig
     from olive.passes.olive_pass import Pass
 
@@ -19,20 +20,27 @@
 
 
 class OliveSystem(ABC):
     system_type: SystemType
 
     def __init__(
         self,
-        accelerators: List[str] = None,
-        olive_managed_env: bool = False,
+        accelerators: Union[List[AcceleratorConfig], List[Dict[str, Any]]] = None,
         hf_token: bool = None,
     ):
-        self.accelerators = accelerators
-        self.olive_managed_env = olive_managed_env
+        # TODO(anyone): Is it possible to expose the arguments to
+        # let user set the system environment in Olive config?
+        # For example, in some qualcomm cases, the user may need to set
+        # SDK root path outside of Olive.
+        if accelerators:
+            assert all(
+                isinstance(validate_config(accelerator, AcceleratorConfig), AcceleratorConfig)
+                for accelerator in accelerators
+            )
+
         self.hf_token = hf_token
 
     @abstractmethod
     def run_pass(
         self,
         the_pass: "Pass",
         model_config: "ModelConfig",
```

## olive/systems/system_config.py

```diff
@@ -7,29 +7,37 @@
 from pathlib import Path
 from typing import Dict, List, Union
 
 import olive.systems.system_alias as system_alias
 from olive.azureml.azureml_client import AzureMLClientConfig
 from olive.common.config_utils import ConfigBase, validate_config
 from olive.common.pydantic_v1 import root_validator, validator
-from olive.systems.common import AzureMLDockerConfig, AzureMLEnvironmentConfig, LocalDockerConfig, SystemType
+from olive.systems.common import (
+    AcceleratorConfig,
+    AzureMLDockerConfig,
+    AzureMLEnvironmentConfig,
+    LocalDockerConfig,
+    SystemType,
+)
 
 
 class TargetUserConfig(ConfigBase):
-    accelerators: List[str] = None
+    accelerators: List[AcceleratorConfig] = None
     hf_token: bool = None
 
-    __hash__ = object.__hash__
+    class Config:
+        validate_assignment = True
 
 
 class LocalTargetUserConfig(TargetUserConfig):
     pass
 
 
 class DockerTargetUserConfig(TargetUserConfig):
+    # the local_docker_config is optional for managed environments and required for normal docker system
     local_docker_config: LocalDockerConfig = None
     is_dev: bool = False
     olive_managed_env: bool = False
     requirements_file: Union[Path, str] = None
 
 
 class AzureMLTargetUserConfig(TargetUserConfig):
@@ -51,35 +59,61 @@
     environment_variables: Dict[str, str] = None  # os.environ will be updated with these variables
     prepend_to_path: List[str] = None  # paths to prepend to os.environ["PATH"]
 
     @validator("python_environment_path", "prepend_to_path", pre=True, each_item=True)
     def _get_abspath(cls, v):
         return str(Path(v).resolve()) if v else None
 
-    @validator("python_environment_path")
-    def _validate_python_environment_path(cls, v):
-        if v:
-            # check if the path exists
-            if not Path(v).exists():
-                raise ValueError(f"Python path {v} does not exist")
-
-            # check if python exists in the path
-            python_path = shutil.which("python", path=v)
-            if not python_path:
-                raise ValueError(f"Python executable not found in the path {v}")
-        return v
-
 
 class PythonEnvironmentTargetUserConfig(CommonPythonEnvTargetUserConfig):
     olive_managed_env: bool = False  # if True, the environment will be created and managed by Olive
     requirements_file: Union[Path, str] = None  # path to the requirements.txt file
 
+    @root_validator(pre=True)
+    def _validate_python_environment_path(cls, values):
+        # if olive_managed_env is True, python_environment_path is not required
+        if values.get("olive_managed_env"):
+            return values
+
+        python_environment_path = values.get("python_environment_path")
+        if python_environment_path is None:
+            raise ValueError("python_environment_path is required for PythonEnvironmentSystem native mode")
+
+        # check if the path exists
+        if not Path(python_environment_path).exists():
+            raise ValueError(f"Python path {python_environment_path} does not exist")
+
+        # check if python exists in the path
+        python_path = shutil.which("python", path=python_environment_path)
+        if not python_path:
+            raise ValueError(f"Python executable not found in the path {python_environment_path}")
+        return values
+
 
 class IsolatedORTTargetUserConfig(CommonPythonEnvTargetUserConfig):
-    pass
+    # Please refer to https://github.com/pydantic/pydantic/issues/1223
+    # In Pydantic v1, missing a optional field will skip the validation. But if the field is specified as None
+    # The validation will be triggered. As the result, we cannot use the following line to make the field as required
+    # since the validation will still be triggered if user pass it as None.
+    # A better approach is to use always=True to check it is required.
+    # python_environment_path: Union[Path, str]
+    @validator("python_environment_path", always=True)
+    def _validate_python_environment_path(cls, v):
+        if v is None:
+            raise ValueError("python_environment_path is required for IsolatedORTSystem")
+
+        # check if the path exists
+        if not Path(v).exists():
+            raise ValueError(f"Python path {v} does not exist")
+
+        # check if python exists in the path
+        python_path = shutil.which("python", path=v)
+        if not python_path:
+            raise ValueError(f"Python executable not found in the path {v}")
+        return v
 
 
 _type_to_config = {
     SystemType.Local: LocalTargetUserConfig,
     SystemType.AzureML: AzureMLTargetUserConfig,
     SystemType.Docker: DockerTargetUserConfig,
     SystemType.PythonEnvironment: PythonEnvironmentTargetUserConfig,
@@ -108,15 +142,16 @@
 
     @root_validator(pre=True)
     def validate_config_type(cls, values):
         type_name = values.get("type")
         system_alias_class = getattr(system_alias, type_name, None)
         if system_alias_class:
             values["type"] = system_alias_class.system_type
-            values["config"]["accelerators"] = system_alias_class.accelerators
+            if system_alias_class.accelerators:
+                values["config"]["accelerators"] = [{"device": acc} for acc in system_alias_class.accelerators]
             # TODO(myguo): consider how to use num_cpus and num_gpus in distributed inference.
         return values
 
     @validator("config", pre=True, always=True)
     def validate_config(cls, v, values):
         if "type" not in values:
             raise ValueError("Invalid type")
@@ -131,8 +166,10 @@
             raise ValueError("azureml_client is required for AzureML system")
         return system_class(**self.config.dict())
 
     @property
     def olive_managed_env(self):
         return getattr(self.config, "olive_managed_env", False)
 
+    # the __hash__ is needed so to create_managed_system_with_cache, otherwise the following error will be raised:
+    # unhashable type: 'SystemConfig'
     __hash__ = object.__hash__
```

## olive/systems/azureml/aml_pass_runner.py

```diff
@@ -13,14 +13,15 @@
 
 from olive.common.config_utils import ParamCategory, validate_config
 from olive.common.utils import aml_runner_hf_login, copy_dir
 from olive.data.config import DataConfig
 from olive.hardware import AcceleratorSpec
 from olive.logging import set_verbosity_from_env
 from olive.model import ModelConfig
+from olive.package_config import OlivePackageConfig
 from olive.passes import REGISTRY as PASS_REGISTRY
 from olive.passes import FullPassConfig, Pass
 from olive.resource_path import create_resource_path
 from olive.systems.utils import get_common_args
 
 
 def parse_pass_config_arg(raw_args):
@@ -104,14 +105,18 @@
     pass_config_arg, extra_args = parse_pass_config_arg(extra_args)
 
     # pass config
     with open(pass_config_arg.pass_config) as f:
         pass_config = json.load(f)
     pass_type = pass_config["type"].lower()
 
+    # Import the pass package configuration from the package_config
+    package_config = OlivePackageConfig.load_default_config()
+    package_config.import_pass_module(pass_config["type"])
+
     if version.parse(ort_version) < version.parse("1.16.0"):
         # In onnxruntime, the following PRs will make the optimize_model save external data in the temporary folder
         # * https://github.com/microsoft/onnxruntime/pull/16531
         # * https://github.com/microsoft/onnxruntime/pull/16716
         # * https://github.com/microsoft/onnxruntime/pull/16912
         # So, in 1.16.0 afterwards, we don't need to copy the model to a temp directory
```

## olive/systems/azureml/aml_system.py

```diff
@@ -29,16 +29,17 @@
     LOCAL_RESOURCE_TYPES,
     OLIVE_RESOURCE_ANNOTATIONS,
     AzureMLModel,
     ResourcePath,
     ResourceType,
     create_resource_path,
 )
-from olive.systems.common import AzureMLDockerConfig, AzureMLEnvironmentConfig, SystemType
+from olive.systems.common import AcceleratorConfig, AzureMLDockerConfig, AzureMLEnvironmentConfig, SystemType
 from olive.systems.olive_system import OliveSystem
+from olive.systems.system_config import AzureMLTargetUserConfig
 
 if TYPE_CHECKING:
     from olive.hardware.accelerator import AcceleratorSpec
     from olive.passes.olive_pass import Pass
 
 
 logger = logging.getLogger(__name__)
@@ -85,20 +86,22 @@
         aml_compute: str,
         aml_docker_config: Union[Dict[str, Any], AzureMLDockerConfig] = None,
         aml_environment_config: Union[Dict[str, Any], AzureMLEnvironmentConfig] = None,
         tags: Dict = None,
         resources: Dict = None,
         instance_count: int = 1,
         is_dev: bool = False,
-        accelerators: List[str] = None,
-        olive_managed_env: bool = False,
+        accelerators: List[AcceleratorConfig] = None,
         hf_token: bool = None,
         **kwargs,
     ):
-        super().__init__(accelerators, olive_managed_env=olive_managed_env, hf_token=hf_token)
+        super().__init__(accelerators, hf_token=hf_token)
+
+        self.config = AzureMLTargetUserConfig(**locals(), **kwargs)
+
         self.instance_count = instance_count
         self.tags = tags or {}
         self.resources = resources
         self.is_dev = is_dev
         self.compute = aml_compute
         self.azureml_client_config = validate_config(azureml_client_config, AzureMLClientConfig)
         if not aml_docker_config and not aml_environment_config:
```

## olive/systems/docker/docker_system.py

```diff
@@ -15,16 +15,17 @@
 
 import olive.systems.docker.utils as docker_utils
 from olive.cache import get_local_path_from_root
 from olive.common.config_utils import ParamCategory, validate_config
 from olive.evaluator.metric import Metric, MetricResult
 from olive.hardware import Device
 from olive.model import ModelConfig
-from olive.systems.common import LocalDockerConfig, SystemType
+from olive.systems.common import AcceleratorConfig, LocalDockerConfig, SystemType
 from olive.systems.olive_system import OliveSystem
+from olive.systems.system_config import DockerTargetUserConfig
 
 if TYPE_CHECKING:
     from olive.hardware.accelerator import AcceleratorSpec
     from olive.passes import Pass
 
 logger = logging.getLogger(__name__)
 
@@ -33,36 +34,34 @@
     system_type = SystemType.Docker
 
     BASE_DOCKERFILE = "Dockerfile"
 
     def __init__(
         self,
         local_docker_config: Union[Dict[str, Any], LocalDockerConfig],
-        accelerators: List[str] = None,
+        accelerators: List[AcceleratorConfig] = None,
         is_dev: bool = False,
-        olive_managed_env: bool = False,
         hf_token: bool = None,
         requirements_file: Optional[Union[Path, str]] = None,
-        **kwargs,  # used to hold the rest of the arguments which is not used by dockersystem
+        **kwargs,  # used to hold the rest of the arguments not used by dockersystem.
     ):
-        super().__init__(
-            accelerators=accelerators,
-            olive_managed_env=olive_managed_env,
-            hf_token=hf_token,
-        )
+        super().__init__(accelerators=accelerators, hf_token=hf_token)
+
         logger.info("Initializing Docker System...")
         self.is_dev = is_dev
         self.docker_client = docker.from_env()
         if local_docker_config is None:
             raise ValueError("local_docker_config cannot be None.")
 
         local_docker_config = validate_config(local_docker_config, LocalDockerConfig)
         if not local_docker_config.build_context_path and not local_docker_config.dockerfile and not requirements_file:
             raise ValueError("build_context_path, dockerfile and requirements_file cannot be None at the same time.")
 
+        self.config = DockerTargetUserConfig(**locals(), **kwargs)
+
         self.run_params = local_docker_config.run_params
         try:
             self.image = self.docker_client.images.get(local_docker_config.image_name)
             logger.info("Image %s found", local_docker_config.image_name)
 
         except docker.errors.ImageNotFound:
             with tempfile.TemporaryDirectory() as tempdir:
```

## olive/systems/docker/runner.py

```diff
@@ -8,14 +8,15 @@
 import os
 import sys
 from pathlib import Path
 
 from olive.common.utils import huggingface_login
 from olive.logging import set_verbosity_from_env
 from olive.model import ModelConfig
+from olive.package_config import OlivePackageConfig
 from olive.passes.olive_pass import FullPassConfig
 
 logger = logging.getLogger("olive")
 
 
 def runner_entry(config, output_path, output_name):
     with open(config) as f:
@@ -25,14 +26,19 @@
     if hf_token:
         huggingface_login(hf_token)
 
     model_json = config_json["model"]
     model = ModelConfig.from_json(model_json).create_model()
 
     pass_config = config_json["pass"]
+
+    # Import the pass package configuration from the package_config
+    package_config = OlivePackageConfig.load_default_config()
+    package_config.import_pass_module(pass_config["type"])
+
     the_pass = FullPassConfig.from_json(pass_config).create_pass()
     output_model = the_pass.run(model, None, output_path)
     # save model json
     output_json = output_model.to_json()
     output_json_path = Path(output_path) / output_name
     with output_json_path.open("w") as f:
         json.dump(output_json, f, indent=4)
```

## olive/systems/isolated_ort/isolated_ort_system.py

```diff
@@ -1,27 +1,28 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import collections
 import json
 import logging
+import shutil
 import tempfile
 from pathlib import Path
 from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union
 
 import numpy as np
 import torch
 from torch.utils.data import Dataset
 
 from olive.common.utils import run_subprocess
 from olive.evaluator.metric import get_latency_config_from_metric
 from olive.evaluator.olive_evaluator import OliveEvaluator, OliveModelOutput, OnnxEvaluatorMixin
 from olive.hardware import Device
-from olive.systems.common import SystemType
+from olive.systems.common import AcceleratorConfig, SystemType
 from olive.systems.olive_system import OliveSystem
 from olive.systems.system_config import IsolatedORTTargetUserConfig
 from olive.systems.utils import create_new_environ, run_available_providers_runner
 
 if TYPE_CHECKING:
     from olive.evaluator.metric import Metric, MetricResult
     from olive.hardware.accelerator import AcceleratorSpec
@@ -35,28 +36,26 @@
     system_type = SystemType.IsolatedORT
 
     def __init__(
         self,
         python_environment_path: Union[Path, str] = None,
         environment_variables: Dict[str, str] = None,
         prepend_to_path: List[str] = None,
-        accelerators: List[str] = None,
+        accelerators: List[AcceleratorConfig] = None,
         hf_token: bool = None,
     ):
-        super().__init__(accelerators=accelerators, olive_managed_env=False)
-        self.config = IsolatedORTTargetUserConfig(
+        if python_environment_path is None:
+            raise ValueError("python_environment_path is required for PythonEnvironmentSystem.")
+
+        super().__init__(accelerators=accelerators, hf_token=hf_token)
+        self.config = IsolatedORTTargetUserConfig(**locals())
+        self.environ = create_new_environ(
             python_environment_path=python_environment_path,
             environment_variables=environment_variables,
             prepend_to_path=prepend_to_path,
-            accelerators=accelerators,
-        )
-        self.environ = create_new_environ(
-            python_environment_path=self.config.python_environment_path,
-            environment_variables=self.config.environment_variables,
-            prepend_to_path=self.config.prepend_to_path,
         )
 
         # available eps. This will be populated the first time self.get_supported_execution_providers() is called.
         # used for caching the available eps
         self.available_eps = None
 
     def run_pass(
@@ -91,23 +90,25 @@
         if self.available_eps:
             return self.available_eps
 
         self.available_eps = run_available_providers_runner(self.environ)
         return self.available_eps
 
     def remove(self):
-        raise ValueError("ORT inference system does not support system removal")
+        raise NotImplementedError("ORT inference system does not support system removal")
 
 
 class IsolatedORTEvaluator(OliveEvaluator, OnnxEvaluatorMixin, framework="ort_inference"):
     def __init__(self, environ: Dict[str, str]):
         super().__init__()
 
+        assert environ, "environ should not be None"
         self.environ = environ
         self.inference_runner_path = Path(__file__).parent.resolve() / "inference_runner.py"
+        self.executable = shutil.which("python", path=self.environ["PATH"])
 
     @classmethod
     def _get_common_config(
         cls, model: "ONNXModelHandler", metric: "Metric", device: Device, execution_providers: Union[str, List[str]]
     ) -> Dict:
         inference_settings = cls.get_inference_settings(metric, model)
         inference_settings = model.merge_inference_settings(inference_settings, execution_providers)
@@ -124,15 +125,15 @@
         self,
         config_path: Union[str, Path],
         model_path: Union[str, Path],
         input_dir: Union[str, Path],
         output_dir: Union[str, Path],
     ):
         command = [
-            "python",
+            self.executable,
             str(self.inference_runner_path),
             "--config_path",
             str(config_path),
             "--model_path",
             str(model_path),
             "--input_dir",
             str(input_dir),
```

## olive/systems/python_environment/pass_runner.py

```diff
@@ -5,14 +5,15 @@
 import argparse
 import json
 from pathlib import Path
 
 from olive.common.utils import set_tempdir
 from olive.logging import set_verbosity_from_env
 from olive.model import ModelConfig
+from olive.package_config import OlivePackageConfig
 from olive.passes.olive_pass import FullPassConfig
 
 
 def get_args(raw_args):
     parser = argparse.ArgumentParser(description="Onnx model inference")
 
     parser.add_argument("--model_config", type=str, required=True, help="Path to input model json file")
@@ -29,15 +30,21 @@
     set_verbosity_from_env()
 
     args = get_args(raw_args)
 
     set_tempdir(args.tempdir)
 
     model = ModelConfig.parse_file(args.model_config).create_model()
-    the_pass = FullPassConfig.parse_file(args.pass_config).create_pass()
+    pass_config = FullPassConfig.parse_file(args.pass_config)
+
+    # Import the pass package configuration from the package_config
+    package_config = OlivePackageConfig.load_default_config()
+    package_config.import_pass_module(pass_config.type)
+
+    the_pass = pass_config.create_pass()
 
     # run pass
     output_model = the_pass.run(model, args.data_root, args.output_model_path)
 
     # save model json
     with Path(args.output_path).open("w") as f:
         json.dump(output_model.to_json(), f, indent=4)
```

## olive/systems/python_environment/python_environment_system.py

```diff
@@ -2,22 +2,23 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import json
 import logging
 import os
 import platform
+import shutil
 import tempfile
 from pathlib import Path
 from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union
 
 from olive.common.utils import run_subprocess
 from olive.evaluator.metric import MetricResult
 from olive.model import ModelConfig
-from olive.systems.common import SystemType
+from olive.systems.common import AcceleratorConfig, SystemType
 from olive.systems.olive_system import OliveSystem
 from olive.systems.system_config import PythonEnvironmentTargetUserConfig
 from olive.systems.utils import create_new_environ, get_package_name_from_ep, run_available_providers_runner
 
 if TYPE_CHECKING:
     from olive.evaluator.metric import Metric
     from olive.hardware.accelerator import AcceleratorSpec
@@ -31,42 +32,39 @@
     system_type = SystemType.PythonEnvironment
 
     def __init__(
         self,
         python_environment_path: Union[Path, str] = None,
         environment_variables: Dict[str, str] = None,
         prepend_to_path: List[str] = None,
-        accelerators: List[str] = None,
+        accelerators: List[AcceleratorConfig] = None,
         olive_managed_env: bool = False,
         requirements_file: Union[Path, str] = None,
         hf_token: bool = None,
     ):
-        super().__init__(accelerators=accelerators, olive_managed_env=olive_managed_env)
-        self.config = PythonEnvironmentTargetUserConfig(
+        if python_environment_path is None:
+            raise ValueError("python_environment_path is required for PythonEnvironmentSystem.")
+
+        super().__init__(accelerators=accelerators, hf_token=hf_token)
+        self.config = PythonEnvironmentTargetUserConfig(**locals())
+        self.environ = create_new_environ(
             python_environment_path=python_environment_path,
             environment_variables=environment_variables,
             prepend_to_path=prepend_to_path,
-            accelerators=accelerators,
-            olive_managed_env=olive_managed_env,
-            requirements_file=requirements_file,
-        )
-        self.environ = create_new_environ(
-            python_environment_path=self.config.python_environment_path,
-            environment_variables=self.config.environment_variables,
-            prepend_to_path=self.config.prepend_to_path,
         )
-        if self.config.olive_managed_env:
+        if olive_managed_env:
             if platform.system() == "Linux":
                 temp_dir = os.path.join(os.environ.get("HOME", ""), "tmp")
                 if not os.path.exists(temp_dir):
                     os.makedirs(temp_dir)
                 self.environ["TMPDIR"] = temp_dir
             else:
                 self.environ["TMPDIR"] = tempfile.TemporaryDirectory().name  # pylint: disable=consider-using-with
 
+        self.executable = shutil.which("python", path=self.environ["PATH"])
         # available eps. This will be populated the first time self.get_supported_execution_providers() is called.
         # used for caching the available eps
         self.available_eps = None
 
         # path to inference script
         parent_dir = Path(__file__).parent.resolve()
         self.pass_runner_path = parent_dir / "pass_runner.py"
@@ -74,15 +72,15 @@
 
     def _run_command(self, script_path: Path, config_jsons: Dict[str, Any], **kwargs) -> Dict[str, Any]:
         """Run a script with the given config jsons and return the output json."""
         with tempfile.TemporaryDirectory() as tmp_dir:
             tmp_dir_path = Path(tmp_dir).resolve()
 
             # command to run
-            command = ["python", str(script_path)]
+            command = [self.executable, str(script_path)]
 
             # write config jsons to files
             for key, config_json in config_jsons.items():
                 config_json_path = tmp_dir_path / f"{key}.json"
                 with config_json_path.open("w") as f:
                     json.dump(config_json, f, indent=4)
                 command.extend([f"--{key}", str(config_json_path)])
@@ -167,26 +165,26 @@
             packages.append(f"-r {self.config.requirements_file}")
 
         # install onnxruntime package
         onnxruntime_package = get_package_name_from_ep(accelerator.execution_provider)[0]
         packages.append(onnxruntime_package)
 
         _, stdout, _ = run_subprocess(
-            f"pip install --cache-dir {self.environ['TMPDIR']} {' '.join(packages)}",
+            f"{self.executable} -m pip install --cache-dir {self.environ['TMPDIR']} {' '.join(packages)}",
             env=self.environ,
             check=True,
         )
         log_stdout(stdout)
 
-        _, stdout, _ = run_subprocess(f"pip show {onnxruntime_package}", env=self.environ, check=True)
+        _, stdout, _ = run_subprocess(
+            f"{self.executable} -m pip show {onnxruntime_package}", env=self.environ, check=True
+        )
         log_stdout(stdout)
 
     def remove(self):
-        import shutil
-
         vitual_env_path = Path(self.config.python_environment_path).resolve().parent
 
         try:
             shutil.rmtree(vitual_env_path)
             logger.info("Virtual environment '%s' removed.", vitual_env_path)
         except FileNotFoundError:
             pass
```

## olive/systems/utils/__init__.py

```diff
@@ -1,21 +1,21 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 from olive.systems.utils.arg_parser import get_common_args
 from olive.systems.utils.misc import (
+    create_managed_system,
+    create_managed_system_with_cache,
     create_new_environ,
-    create_new_system,
-    create_new_system_with_cache,
     get_package_name_from_ep,
     run_available_providers_runner,
 )
 
 __all__ = [
-    "get_common_args",
+    "create_managed_system",
+    "create_managed_system_with_cache",
     "create_new_environ",
-    "create_new_system",
-    "create_new_system_with_cache",
+    "get_common_args",
     "get_package_name_from_ep",
     "run_available_providers_runner",
 ]
```

## olive/systems/utils/misc.py

```diff
@@ -6,35 +6,49 @@
 import logging
 import os
 import shutil
 import tempfile
 from copy import deepcopy
 from functools import lru_cache
 from pathlib import Path
-from typing import Dict, List, Optional, Union
+from typing import TYPE_CHECKING, Dict, List, Optional, Union
 
 from olive.common.utils import hash_dir, run_subprocess
+from olive.hardware import DEFAULT_CPU_ACCELERATOR, AcceleratorSpec
 from olive.hardware.constants import PROVIDER_DOCKERFILE_MAPPING, PROVIDER_PACKAGE_MAPPING
 from olive.systems.common import SystemType
 
+if TYPE_CHECKING:
+    from olive.systems.system_config import SystemConfig
+
 logger = logging.getLogger(__name__)
 
 
 def get_package_name_from_ep(execution_provider: str) -> str:
     """Get the package name from the execution provider."""
     return PROVIDER_PACKAGE_MAPPING.get(execution_provider, ("onnxruntime", "ort-nightly"))
 
 
 @lru_cache(maxsize=8)
-def create_new_system_with_cache(system_config, accelerator):
-    return create_new_system(system_config, accelerator)
+def create_managed_system_with_cache(system_config, accelerator):
+    return create_managed_system(system_config, accelerator)
 
 
-def create_new_system(system_config, accelerator):
+def create_managed_system(system_config: "SystemConfig", accelerator: "AcceleratorSpec"):
     # pylint: disable=consider-using-with
+    assert system_config.olive_managed_env, "system_config.olive_managed_env must be True"
+
+    # for host system, use the first available accelerator
+    if accelerator:
+        accelerator_cfg = [
+            {"device": accelerator.accelerator_type, "execution_providers": [accelerator.execution_provider]}
+        ]
+    else:
+        accelerator_cfg = None
+        accelerator = DEFAULT_CPU_ACCELERATOR
 
     # create a new system with the same type as the origin system
     if system_config.type in [SystemType.Local, SystemType.IsolatedORT]:
         raise NotImplementedError(f"olive_managed_env is not supported for {system_config.type} System")
 
     elif system_config.type == SystemType.PythonEnvironment:
         import platform
@@ -55,34 +69,33 @@
 
         if platform.system() == "Windows":
             python_environment_path = f"{venv_path}/Scripts"
         else:
             python_environment_path = f"{venv_path}/bin"
         new_system = PythonEnvironmentSystem(
             python_environment_path=python_environment_path,
-            accelerators=[accelerator.accelerator_type],
+            accelerators=accelerator_cfg,
             environment_variables=system_config.config.environment_variables,
             prepend_to_path=system_config.config.prepend_to_path,
             olive_managed_env=True,
             requirements_file=system_config.config.requirements_file,
         )
         new_system.install_requirements(accelerator)
-
     elif system_config.type == SystemType.Docker:
         from olive.systems.docker import DockerSystem
 
         dockerfile = PROVIDER_DOCKERFILE_MAPPING.get(accelerator.execution_provider, "Dockerfile.cpu")
         # TODO(myguo): create a temp dir for the build context
         new_system = DockerSystem(
             local_docker_config={
                 "image_name": f"olive_{accelerator.execution_provider[:-17].lower()}",
                 "dockerfile": dockerfile,
                 "build_context_path": Path(__file__).parents[1] / "docker",
             },
-            accelerators=[accelerator.accelerator_type],
+            accelerators=accelerator_cfg,
             is_dev=system_config.config.is_dev,
             olive_managed_env=True,
             requirements_file=(
                 str(system_config.config.requirements_file) if system_config.config.requirements_file else None
             ),
         )
 
@@ -101,15 +114,15 @@
 
         env_hash = hash_dir(build_context_path)
         name = f"olive-managed-env-{env_hash}"
         new_system = AzureMLSystem(
             azureml_client_config=system_config.config.azureml_client_config,
             aml_compute=system_config.config.aml_compute,
             instance_count=system_config.config.instance_count,
-            accelerators=[accelerator.accelerator_type],
+            accelerators=accelerator_cfg,
             aml_environment_config={
                 "name": name,
                 "label": "latest",
             },
             aml_docker_config={
                 "name": name,
                 "dockerfile": dockerfile,
@@ -145,16 +158,17 @@
     environ["OLIVE_LOG_LEVEL"] = log_level
     return environ
 
 
 def run_available_providers_runner(environ: Dict) -> List[str]:
     """Run the available providers runner script with the given environment and return the available providers."""
     runner_path = Path(__file__).parent / "available_providers_runner.py"
+    python_path = shutil.which("python", path=environ["PATH"])
     with tempfile.TemporaryDirectory() as temp_dir:
         output_path = Path(temp_dir).resolve() / "available_eps.json"
         run_subprocess(
-            f"python {runner_path} --output_path {output_path}",
+            f"{python_path} {runner_path} --output_path {output_path}",
             env=environ,
             check=True,
         )
         with output_path.open("r") as f:
             return json.load(f)
```

## olive/workflows/run/__main__.py

```diff
@@ -5,17 +5,27 @@
 import argparse
 
 from olive.common.utils import set_tempdir
 from olive.workflows import run
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser("Olive Workflow: Custom Run")
-    parser.add_argument("--config", type=str, help="Path to json config file", required=True)
+    parser.add_argument(
+        "--package-config",
+        type=str,
+        required=False,
+        help=(
+            "For advanced users. Path to optional package (json) config file with location "
+            "of individual pass module implementation and corresponding dependencies."
+            "Configuration might also include user owned/proprietary/private pass implementations."
+        ),
+    )
+    parser.add_argument("--run-config", "--config", type=str, help="Path to json config file", required=True)
     parser.add_argument("--setup", help="Whether run environment setup", action="store_true")
-    parser.add_argument("--data_root", help="The data root path for optimization", required=False)
+    parser.add_argument("--data-root", "--data_root", help="The data root path for optimization", required=False)
     parser.add_argument("--tempdir", type=str, help="Root directory for tempfile directories and files", required=False)
 
     args = parser.parse_args()
 
     set_tempdir(args.tempdir)
 
     var_args = vars(args)
```

## olive/workflows/run/config.py

```diff
@@ -12,15 +12,15 @@
 from olive.common.pydantic_v1 import validator
 from olive.data.config import DataConfig
 from olive.data.container.huggingface_container import HuggingfaceContainer
 from olive.engine import Engine, EngineConfig
 from olive.engine.packaging.packaging_config import PackagingConfig
 from olive.evaluator.olive_evaluator import OliveEvaluatorConfig
 from olive.model import ModelConfig
-from olive.passes import FullPassConfig, Pass
+from olive.passes import FullPassConfig
 from olive.passes.pass_config import PassParamDefault
 from olive.resource_path import AZUREML_RESOURCE_TYPES
 from olive.systems.system_config import SystemConfig
 
 logger = logging.getLogger(__name__)
 
 
@@ -31,25 +31,24 @@
     output_name: str = None
 
 
 class RunEngineConfig(EngineConfig):
     evaluate_input_model: bool = True
     output_dir: Union[Path, str] = None
     output_name: str = None
-    packaging_config: PackagingConfig = None
+    packaging_config: Union[PackagingConfig, List[PackagingConfig]] = None
     log_severity_level: int = 1
     ort_log_severity_level: int = 3
     ort_py_log_severity_level: int = 3
     log_to_file: bool = False
 
     def create_engine(self):
         config = self.dict()
         to_del = [
             "evaluate_input_model",
-            "execution_providers",
             "output_dir",
             "output_name",
             "packaging_config",
             "log_severity_level",
             "ort_log_severity_level",
             "ort_py_log_severity_level",
             "log_to_file",
@@ -217,40 +216,44 @@
             if disable_search is None:
                 disable_search = True
         else:
             # disable search if user explicitly set disable_search to True
             disable_search = disable_search or False
 
         v["disable_search"] = disable_search
-        pass_cls = Pass.registry.get(v["type"].lower(), None)
-        if pass_cls:
-            if not v.get("config"):
-                return v
-
-            searchable_configs = set()
-            for param_name in v["config"]:
-                if v["config"][param_name] == PassParamDefault.SEARCHABLE_VALUES:
-                    searchable_configs.add(param_name)
-                if param_name.endswith("data_config"):
-                    # we won't auto insert the input model data config for pass
-                    # user must explicitly set the data config to INPUT_MODEL_DATA_CONFIG if needed
-                    v["config"] = _resolve_data_config(v["config"], values, param_name, auto_insert=False)
+        if not v.get("config"):
+            return v
 
-            data_dir_config = v["config"].get("data_dir", None)
-            if isinstance(data_dir_config, dict):
-                if _have_aml_client(data_dir_config, values):
-                    data_dir_config["config"]["azureml_client"] = values["azureml_client"]
-                v["config"]["data_dir"] = data_dir_config
+        searchable_configs = set()
+        for param_name in v["config"]:
+            if v["config"][param_name] == PassParamDefault.SEARCHABLE_VALUES:
+                searchable_configs.add(param_name)
+            if param_name.endswith("data_config"):
+                # we won't auto insert the input model data config for pass
+                # user must explicitly set the data config to INPUT_MODEL_DATA_CONFIG if needed
+                v["config"] = _resolve_data_config(v["config"], values, param_name, auto_insert=False)
+
+        data_dir_config = v["config"].get("data_dir", None)
+        if isinstance(data_dir_config, dict):
+            if _have_aml_client(data_dir_config, values):
+                data_dir_config["config"]["azureml_client"] = values["azureml_client"]
+            v["config"]["data_dir"] = data_dir_config
+
+        if disable_search and searchable_configs:
+            raise ValueError(
+                f"You cannot disable search for {v['type']} and"
+                f" set {searchable_configs} to SEARCHABLE_VALUES at the same time."
+                " Please remove SEARCHABLE_VALUES or enable search(needs search strategy configs)."
+            )
+        return v
 
-            if disable_search and searchable_configs:
-                raise ValueError(
-                    f"You cannot disable search for {v['type']} and"
-                    f" set {searchable_configs} to SEARCHABLE_VALUES at the same time."
-                    " Please remove SEARCHABLE_VALUES or enable search(needs search strategy configs)."
-                )
+    @validator("auto_optimizer_config", always=True)
+    def validate_auto_optimizer_config(cls, v, values):
+        if not v:
+            v = AutoOptimizerConfig()
         return v
 
 
 def _resolve_config_str(v, values, alias, component_name):
     """Resolve string value for alias in v to corresponding component config in values.
 
     values: {
```

## olive/workflows/run/run.py

```diff
@@ -1,33 +1,31 @@
 # -------------------------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License.
 # --------------------------------------------------------------------------
 import importlib.metadata
-import json
 import logging
-import os
 import subprocess
 import sys
+from copy import deepcopy
 from pathlib import Path
 from typing import List, Union
 
 from olive.auto_optimizer import AutoOptimizer
 from olive.hardware.accelerator import create_accelerators
 from olive.logging import enable_filelog, set_default_logger_severity, set_ort_logger_severity, set_verbosity_info
+from olive.package_config import OlivePackageConfig
 from olive.systems.common import SystemType
 from olive.workflows.run.config import RunConfig, RunPassConfig
 
 logger = logging.getLogger(__name__)
 
 
-def dependency_setup(config):
-    here = os.path.abspath(os.path.dirname(__file__))
-    with open(os.path.join(here, "../../extra_dependencies.json")) as f:
-        extras = json.load(f)
+def dependency_setup(package_config: OlivePackageConfig, run_config: RunConfig):
+    extras = deepcopy(package_config.extra_dependencies)
 
     def get_system_extras(host_type, accelerators, execution_providers):
         extra_name = None
         if host_type is None:
             extra_name = "cpu"
         elif host_type == SystemType.AzureML:
             extra_name = "azureml"
@@ -41,67 +39,54 @@
                     extra_name = "gpu"
             else:
                 extra_name = "cpu"
 
         return extra_name
 
     def get_pass_extras(pass_type):
-        pass_to_extra = {
-            "OnnxFloatToFloat16": ["onnxconverter-common"],
-            "OrtPerfTuning": ["psutil"],
-            "QuantizationAwareTraining": ["pytorch-lightning"],
-        }
-
-        pass_to_extra_names = {
-            "OpenVINOConversion": ["openvino"],
-            "OpenVINOQuantization": ["openvino"],
-            "IncQuantization": ["inc"],
-            "IncDynamicQuantization": ["inc"],
-            "IncStaticQuantization": ["inc"],
-            "OptimumConversion": ["optimum"],
-            "OptimumMerging": ["optimum"],
-            "TorchTRTConversion": ["torch-tensorrt"],
-            "LoRA": ["lora"],
-            "QLoRA": ["bnb", "lora"],
-        }
+        pass_module_config = package_config.passes.get(pass_type)
 
         extra_results = []
-        extra_results.extend(pass_to_extra.get(pass_type, []))
-        for extra_name in pass_to_extra_names.get(pass_type, []):
-            extra_results.extend(extras.get(extra_name))
+        extra_results.extend(pass_module_config.module_dependencies)
+        for extra_name in pass_module_config.extra_dependencies:
+            extra_results.extend(package_config.extra_dependencies.get(extra_name, []))
         return extra_results
 
-    ort_packages = ["onnxruntime", "onnxruntime-directml", "onnxruntime-gpu", "onnxruntime-openvino"]
+    ort_packages = extras.get("ort", [])
 
     local_packages = []
     remote_packages = []
 
     # add dependencies for passes
-    if config.passes:
-        for pass_config in config.passes.values():
-            host = pass_config.host or config.engine.host
+    if run_config.passes:
+        for pass_config in run_config.passes.values():
+            host = pass_config.host or run_config.engine.host
             if (host and host.type == SystemType.Local) or not host:
                 local_packages.extend(get_pass_extras(pass_config.type))
             else:
                 remote_packages.extend(get_pass_extras(pass_config.type))
             if pass_config.type in ["SNPEConversion", "SNPEQuantization", "SNPEtoONNXConversion"]:
                 logger.info(
                     "Please refer to https://microsoft.github.io/Olive/tutorials/passes/snpe.html to install SNPE"
                     " prerequisites for pass %s",
                     pass_config.type,
                 )
 
     # add dependencies for engine
     host_type = None
-    accelerators = None
-    if config.engine.host:
-        host_type = config.engine.host.type
-        accelerators = config.engine.host.config.accelerators
+    accelerators = []
+    execution_providers = []
+    if run_config.engine.host:
+        host_type = run_config.engine.host.type
+        if run_config.engine.host.config.accelerators:
+            for acc in run_config.engine.host.config.accelerators:
+                accelerators.append(acc.device)
+                if acc.execution_providers:
+                    execution_providers.extend(acc.execution_providers)
 
-    execution_providers = config.engine.execution_providers if config.engine.execution_providers else None
     system_extra_name = get_system_extras(host_type, accelerators, execution_providers)
     if system_extra_name:
         local_packages.extend(extras.get(system_extra_name))
 
     # install missing packages to local or tell user to install packages in their environment
     logger.info("The following packages are required in the local environment: %s", local_packages)
     packages_install = []
@@ -125,82 +110,100 @@
         logger.info("Running: %s", " ".join(cmd))
         subprocess.check_call(cmd)
         logger.info("Successfully installed %s.", packages_install)
 
     if remote_packages:
         logger.info(
             "Please make sure the following packages are installed in %s environment: %s",
-            config.engine.host.type,
+            run_config.engine.host.type,
             remote_packages,
         )
 
 
-def run_engine(config: RunConfig, data_root: str = None):
+def run_engine(package_config: OlivePackageConfig, run_config: RunConfig, data_root: str = None):
     import onnxruntime as ort
 
     from olive.passes import Pass
 
     # for onnxruntime
     # ort_py_log_severity_level: python logging levels
-    set_ort_logger_severity(config.engine.ort_py_log_severity_level)
+    set_ort_logger_severity(run_config.engine.ort_py_log_severity_level)
 
     # ort_log_severity_level: C++ logging levels
-    ort.set_default_logger_severity(config.engine.ort_log_severity_level)
+    ort.set_default_logger_severity(run_config.engine.ort_log_severity_level)
 
     # input model
-    input_model = config.input_model
+    input_model = run_config.input_model
 
     # Azure ML Client
-    if config.azureml_client:
-        config.engine.azureml_client_config = config.azureml_client
+    if run_config.azureml_client:
+        run_config.engine.azureml_client_config = run_config.azureml_client
 
-    engine = config.engine.create_engine()
+    engine = run_config.engine.create_engine()
 
-    # config file will be uploaded to AML job
-    is_azureml_system = (config.engine.host is not None and config.engine.host.type == SystemType.AzureML) or (
-        config.engine.target is not None and config.engine.target.type == SystemType.AzureML
+    # run_config file will be uploaded to AML job
+    is_azureml_system = (run_config.engine.host is not None and run_config.engine.host.type == SystemType.AzureML) or (
+        run_config.engine.target is not None and run_config.engine.target.type == SystemType.AzureML
     )
 
     if is_azureml_system:
         from olive.systems.azureml.aml_system import AzureMLSystem
 
-        AzureMLSystem.olive_config = config.to_json()
+        AzureMLSystem.olive_config = run_config.to_json()
 
-    no_evaluation = engine.evaluator_config is None and all(
-        pass_config.evaluator is None for pass_config in config.passes.values()
-    )
-    accelerator_specs = create_accelerators(
-        engine.target_config, config.engine.execution_providers, skip_supported_eps_check=no_evaluation
+    no_evaluation = (
+        engine.evaluator_config is None
+        and run_config.passes
+        and all(pass_config.evaluator is None for pass_config in run_config.passes.values())
     )
+    accelerator_specs = create_accelerators(engine.target_config, skip_supported_eps_check=no_evaluation)
 
     pass_list = []
     acc_list = []
     if (
-        not config.passes
-        and config.auto_optimizer_config is not None
-        and not config.auto_optimizer_config.disable_auto_optimizer
+        not run_config.passes
+        and run_config.auto_optimizer_config is not None
+        and not run_config.auto_optimizer_config.disable_auto_optimizer
     ):
+        # For auto optimizer, Olive generates passes and pass_flows for each accelerator
+        # that means, the passes and pass_flows might be different for each accelerator
         for acc_spec in accelerator_specs:
             _passes, pass_flows = AutoOptimizer(
                 input_model,
                 engine.evaluator_config,
                 acc_spec,
-                config.auto_optimizer_config,
-                config.data_configs,
+                run_config.auto_optimizer_config,
+                run_config.data_configs,
             ).suggest()
             pass_list.append(({k: RunPassConfig.parse_obj(v) for k, v in _passes.items()}, pass_flows))
             acc_list.append([acc_spec])
     else:
-        pass_list.append((config.passes, config.pass_flows))
+        # For non-auto-optimizer case, Olive uses the same passes and pass_flows for all accelerators
+        # if user needs different passes and pass_flows for each accelerator, they need to write multiple
+        # config files.
+        pass_list.append((run_config.passes, run_config.pass_flows))
         acc_list.append(accelerator_specs)
 
     run_rls = {}
+    # Note that, in Olive, there are two positions where the accelerator_specs are looped over:
+    # 1. olive workflow run level: this is where the accelerator_specs are created and passed to
+    # the engine. In this level, accelerator specs can be used to generate passes and pass_flows.
+    # 2. engine level: this is where the accelerator_specs are looped over to run the passes.
+    # TODO(anyone): refactor the code to remove the engine level loop if possible.
+    # For time being, we are keeping both loops, but in future, we might want to refactor the code
+    # to remove engine level loop and pass the accelerator_specs to the engine directly.
     for accelerator_spec, (passes, pass_flows) in zip(acc_list, pass_list):
         engine.reset_passes()
         if passes:
+            # First pass registers the necessary module implementation
+            for pass_config in passes.values():
+                logger.info("Importing pass module %s", pass_config.type)
+                package_config.import_pass_module(pass_config.type)
+
+            # Second pass, initializes the pass and registers it with the engine
             for pass_name, pass_config in passes.items():
                 host = pass_config.host.create_system() if pass_config.host is not None else None
                 engine.register(
                     Pass.registry[pass_config.type.lower()],
                     config=pass_config.config,
                     disable_search=pass_config.disable_search,
                     name=pass_name,
@@ -208,50 +211,65 @@
                     evaluator_config=pass_config.evaluator,
                     clean_run_cache=pass_config.clean_run_cache,
                     output_name=pass_config.output_name,
                 )
             engine.set_pass_flows(pass_flows)
 
         if data_root is None:
-            data_root = config.data_root
+            data_root = run_config.data_root
 
         # run
         run_rls.update(
             engine.run(
                 input_model,
                 accelerator_spec,
                 data_root,
-                config.engine.packaging_config,
-                config.engine.output_dir,
-                config.engine.output_name,
-                config.engine.evaluate_input_model,
+                run_config.engine.packaging_config,
+                run_config.engine.output_dir,
+                run_config.engine.output_name,
+                run_config.engine.evaluate_input_model,
             )
         )
     return run_rls
 
 
-def run(config: Union[str, Path, dict], setup: bool = False, data_root: str = None):
+def run(
+    run_config: Union[str, Path, dict],
+    setup: bool = False,
+    data_root: str = None,
+    package_config: Union[str, Path, dict] = None,
+):
+    if package_config is None:
+        package_config = OlivePackageConfig.get_default_config_path()
+
     # we use parse_file and parse_obj to be safe. If implemented as expected, both should be equivalent.
-    if isinstance(config, (str, Path)):
-        config = RunConfig.parse_file(config)
+    if isinstance(package_config, (str, Path)):
+        logger.info("Loading Olive module configuration from: %s", package_config)
+        package_config = OlivePackageConfig.parse_file(package_config)
+    else:
+        package_config = OlivePackageConfig.parse_obj(package_config)
+
+    if isinstance(run_config, (str, Path)):
+        logger.info("Loading run configuration from: %s", run_config)
+        run_config = RunConfig.parse_file(run_config)
     else:
-        config = RunConfig.parse_obj(config)
+        run_config = RunConfig.parse_obj(run_config)
 
     # set log level for olive
-    set_default_logger_severity(config.engine.log_severity_level)
-    if config.engine.log_to_file:
-        enable_filelog(config.engine.log_severity_level)
+    set_default_logger_severity(run_config.engine.log_severity_level)
+    if run_config.engine.log_to_file:
+        enable_filelog(run_config.engine.log_severity_level)
 
     if setup:
         # set the log level to INFO for setup
         set_verbosity_info()
-        dependency_setup(config)
+        dependency_setup(package_config, run_config)
         return None
     else:
-        return run_engine(config, data_root)
+        return run_engine(package_config, run_config, data_root)
 
 
 def check_local_ort_installation(package_name: str):
     """Check whether ORT is installed. If not, will return current package name to install."""
     local_ort_packages = get_local_ort_packages()
 
     if not local_ort_packages:
```

## Comparing `olive_ai-0.5.0.dist-info/LICENSE` & `olive_ai-0.5.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `olive_ai-0.5.0.dist-info/METADATA` & `olive_ai-0.5.1.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: olive-ai
-Version: 0.5.0
+Version: 0.5.1
 Summary: Olive is an easy-to-use hardware-aware model optimization tool that composes industry-leading techniques across model compression, optimization, and compilation.
 Home-page: https://microsoft.github.io/Olive/
 Download-URL: https://github.com/microsoft/Olive/tags
 Author: Microsoft Corporation
 Author-email: olivedevteam@microsoft.com
 License: MIT License
 Classifier: Development Status :: 3 - Alpha
@@ -57,14 +57,19 @@
 Requires-Dist: peft ; extra == 'lora'
 Requires-Dist: scipy ; extra == 'lora'
 Provides-Extra: openvino
 Requires-Dist: openvino ==2023.2.0 ; extra == 'openvino'
 Requires-Dist: nncf ==2.7.0 ; extra == 'openvino'
 Provides-Extra: optimum
 Requires-Dist: optimum ; extra == 'optimum'
+Provides-Extra: ort
+Requires-Dist: onnxruntime ; extra == 'ort'
+Requires-Dist: onnxruntime-directml ; extra == 'ort'
+Requires-Dist: onnxruntime-gpu ; extra == 'ort'
+Requires-Dist: onnxruntime-openvino ; extra == 'ort'
 Provides-Extra: ort-training
 Requires-Dist: onnxruntime-training ; extra == 'ort-training'
 Requires-Dist: torch-ort ; extra == 'ort-training'
 Provides-Extra: tf
 Requires-Dist: tensorflow ==1.15.0 ; extra == 'tf'
 Provides-Extra: torch-tensorrt
 Requires-Dist: torch-tensorrt ; extra == 'torch-tensorrt'
```

## Comparing `olive_ai-0.5.0.dist-info/NOTICE.txt` & `olive_ai-0.5.1.dist-info/NOTICE.txt`

 * *Files identical despite different names*

## Comparing `olive_ai-0.5.0.dist-info/RECORD` & `olive_ai-0.5.1.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,28 +1,29 @@
-olive/__init__.py,sha256=A5Jkbo11-OVSYRiMoAKfEPSBneGMSCcyUouAqCw_fY0,608
-olive/cache.py,sha256=BISfV9wvrzengrQNAbOPDdTzE5-rDp62Mc_G4osesvY,9889
+olive/__init__.py,sha256=srNwPJte4clvSqTC73GA3slcx1kTDs6NxNW6LNhuqHk,608
+olive/cache.py,sha256=8fTz93_iPpzM6UGr0y04fhVw-Xpip4dBLxVKxHM_2kQ,9998
 olive/constants.py,sha256=20IZURQJHfX4spfzUjcLZ-HsFqgdM2-JrwcJ8QnFKJo,1113
-olive/extra_dependencies.json,sha256=XbqiZGZgwgrUjjrDFrb5pb2qFO-igkiUg3hMWGctF0E,797
 olive/logging.py,sha256=VcJnHwDqDsWuw46HqgMOCfCgCNi2axFQYaxVFhrd8lI,2626
-olive/resource_path.py,sha256=woZrsnroSKJGRI8HkKvFzdndsKeHb4B6Bo9RTih3Wg0,22594
+olive/olive_config.json,sha256=aRrvxMiUiz2-tt6h54nZrLSI8s30rK5XlWew2TTa5SY,7817
+olive/package_config.py,sha256=ZsgHwNHGwjh0zIzlZn7qBmLiz8yB9nXXfA6azwh_S28,1406
+olive/resource_path.py,sha256=dPBcwg9KqYib7N75O8-ZT-_MxlXqGdHC2kiY2oDzLiU,22999
 olive/auto_optimizer/__init__.py,sha256=DC-U5sReV5weVF-A6Rn2-OA9BGxeBpYpg1VBpNrygCw,5181
-olive/auto_optimizer/regulate_mixins.py,sha256=AivrU6vPQT10v-i1QN69bCGu0DIV7yxs6ABfb1JlMCI,5663
+olive/auto_optimizer/regulate_mixins.py,sha256=X7G3UOOSH7Z9ZhD3QfCVpfIMtWksP_84UlK6ijItjJQ,5871
 olive/auto_optimizer/template_mapping.py,sha256=6wgYD_X61GilGYRVCEvkwcYXSuhEWokgsdBKNxujaiQ,3615
 olive/auto_optimizer/config_template/opt_level_passes.yaml,sha256=A2ZE_u8mrWQ1I30tcHuft7FdY70eWFxuD6r_VMEgn90,1171
 olive/auto_optimizer/config_template/pass_capability.yaml,sha256=FfmafalzQSkImDnqCnJvZCgqXaDdZlMh9J2VIClWhOo,2062
 olive/azureml/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/azureml/azureml_client.py,sha256=Fccb0afmMc2PKXi1bH-8V9Nj4Z_mMBtfyRrC5LZnuPY,6412
 olive/common/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-olive/common/auto_config.py,sha256=KzKM9-CZN2pu3a8Sb5GAHlMpgxj3K2BH2BJrbbBzcZE,3441
-olive/common/config_utils.py,sha256=Z28001zAn33QVikIq8u0f_RoL0q_4l0CCChSo07NpBc,10777
+olive/common/auto_config.py,sha256=yNfcSrh1z_gQLSsiYFn63k5x-qT33t-S1y97J42uvnA,3430
+olive/common/config_utils.py,sha256=57gPIim-aDFScw3AamYnRZccXz8sc4KC_uOAUfvTObQ,10881
 olive/common/import_lib.py,sha256=KrWnGR2Pe1RWfOqe3DL96Aq0O8zXzYHxJc0d2zgMdlU,1488
 olive/common/ort_inference.py,sha256=hVS2ep47Nx3d8WNhnT6BhsA3VGcU13iSPgFNEDl2LTk,16029
 olive/common/pydantic_v1.py,sha256=vNddf08_5YT58wfDL3A38TaniE6xku-CMa7rIdesOZ8,765
-olive/common/user_module_loader.py,sha256=xtulzltz0oTzclxgMk5zCW9LnEk4DW_3OgEHnGa5nOg,1728
-olive/common/utils.py,sha256=sL8bezq_ZA2CTampUfKWQORiBGFueMOfIlTJRrBuNK8,11765
+olive/common/user_module_loader.py,sha256=FfRLFiEvGw2yT9EKULc4eIr4a0CP_lGms2SkXNg0xa0,1809
+olive/common/utils.py,sha256=GttHdpJPy2PxcA330lIqzs8oVRuyzXsCj0AJPr00pdE,11627
 olive/data/__init__.py,sha256=e6E8AVlCoJ-LNfbshkiyOVopkfgUmg6dRo9LH51Mafw,345
 olive/data/config.py,sha256=Ro9bPuIHe6V1mhm9DcdHggq2gdu1_2bbAJ8krwewOIc,10680
 olive/data/constants.py,sha256=v4_EcRT473PBqIJDJW54HiMWP_OboMKILUd7cf4MwR4,1443
 olive/data/registry.py,sha256=hZi_GKCKWu9SbxpMiJ9OR7q-yNKIunpTUzdXY9cPipI,8119
 olive/data/template.py,sha256=wJQGctfoMiJGIf5gB1Kp2kbJlofYujSg9vl3KGO-47E,3573
 olive/data/component/__init__.py,sha256=fo0PIFNXx16c4CMOTlxaIj8UlZvApJ3-Pca18AK3qQY,444
 olive/data/component/dataloader.py,sha256=beCT0mojWX09OGPyL2cR2pUSXMRv_kHohHDv4tJwj1s,1955
@@ -33,206 +34,207 @@
 olive/data/component/text_generation.py,sha256=gxaKqJAoUpR84WVJUUuBFsSkXRV6MBgIX9pAkV03Vck,29278
 olive/data/container/__init__.py,sha256=LDGROQwmhlEeOzEtSRG0UWushVKko-gu_EvHw0g6XJg,480
 olive/data/container/data_container.py,sha256=CJ8HBjcoDPAOX-zVUaOtS7wJGSnrkvNK5-QkSjSho1E,3385
 olive/data/container/dummy_data_container.py,sha256=4C_mhjK1qUKaytBrCzHR3TUfYYvQxDtgOXP2iRMhPwk,1159
 olive/data/container/huggingface_container.py,sha256=C1nMtdmVkDDviL-jyY5_lK9aN2dzRMcAKASoQ9Iysjk,1690
 olive/data/container/raw_data_container.py,sha256=YqjbFWH6KI51_TEI_YFBjjDMFKwKMx2M7SN8jbbBZYI,1721
 olive/engine/__init__.py,sha256=QSFcQLIESkRdgcFPbgVNHRg3iVSHmlRQWZtpdS_74K4,411
-olive/engine/config.py,sha256=4wAUy3s3vpt_8_oj4kqtgoUAcx1YV0KAuXpe6Tpd5ac,1432
-olive/engine/engine.py,sha256=9_Q8p-fKyemp38uUmTcXGnzDsbqk0DFS4u0WOJijrMw,45918
+olive/engine/config.py,sha256=1kz_aLyD0c7jHQZjgY4Nsk9SJdpSDx4zP7xXJWKa2As,1447
+olive/engine/engine.py,sha256=MSv7V_kuOMwT1nShj54RqE0n39w1JGhl2xdodr8CZkU,46635
 olive/engine/footprint.py,sha256=C_s1OyI9yt8pal76WQ13WbHfu-B1oD0oKb3pvjzdvJI,17410
 olive/engine/packaging/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-olive/engine/packaging/packaging_config.py,sha256=LgBHik2vt7aG9nY-Ov6n-YrBzDQZi_iGghUqcZui084,622
-olive/engine/packaging/packaging_generator.py,sha256=cAGhbYOnoKhw2bXcAJ3S6udGQR8DNDKphM41OdqNkPw,15871
+olive/engine/packaging/packaging_config.py,sha256=LImhOApwoSPlKNV7dbVC1BenR2I_-M9H6td5T4TZAzo,1642
+olive/engine/packaging/packaging_generator.py,sha256=y9we8MVWV5H3V6uIZ0NR5f63Am27YFVmt_ZbUx5r3DA,19748
 olive/engine/packaging/sample_code/ONNXModel/cpp/README.md,sha256=JnLAiNYgEZCvsP0rwG5rTOwn_px6XeLFjrchXUDWO4s,1588
 olive/engine/packaging/sample_code/ONNXModel/cpp/code_sample.cpp,sha256=4FejpIKRXN9-9LuWvkoFGMCgnuhRZxFA6m_9E6yrNQs,4990
 olive/engine/packaging/sample_code/ONNXModel/cs/README.md,sha256=V7tDnmXiisTAbiCec7Us-pUvI3XuJEc6EE1cO2WXt0A,1590
 olive/engine/packaging/sample_code/ONNXModel/cs/code_sample.cs,sha256=eZhEe9P2psAFeT_3Kz-WqHjkNRD-raaW3eILw2Gs1Cw,4991
 olive/engine/packaging/sample_code/ONNXModel/python/README.md,sha256=z2awTxd97vnQwNuQ08WOid4MiuGQghTJ3EI4Fo5Zb38,2296
 olive/engine/packaging/sample_code/ONNXModel/python/code_sample.py,sha256=HYIiEfvQObTDI6Bh3DP3fSbwvEp9PQ7YcdX8YGTGzw8,2545
 olive/evaluator/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/evaluator/accuracy.py,sha256=HV17zTBx2U-H9BkM9cF7EbLWkOUyzHkySWX77mb9DaU,6473
-olive/evaluator/metric.py,sha256=UAwlKCyg4A7nubP0lM_t0DrXmU4lfZAu5J0lKUt9jaw,8747
-olive/evaluator/metric_backend.py,sha256=I40KzdiK_jvIgefaf13pYUOIfhetjHJPNjQrspffBNA,4457
+olive/evaluator/metric.py,sha256=yE3_zK_JanrxKM5LBhPb1zNJwgIt1sofW-4BxhjT6n0,8723
+olive/evaluator/metric_backend.py,sha256=2Trmgtcet7bmJTCKb3jDYzNYdLBwtY4xqnpDGG-O6po,4461
 olive/evaluator/metric_config.py,sha256=HDA9Ek4PgtY3QQj5tMGVHjqu-2d64cUEIOscTLqQkoI,4395
 olive/evaluator/olive_evaluator.py,sha256=ndyiABMBa875RecgQ90zrPR3e-5FaezkAx4BX09ZVNc,47429
 olive/exception/__init__.py,sha256=ycjIjR_iwg4ZcCuRtbHZpEbIw-4RzlurOa6o5tz8NA0,592
 olive/hardware/__init__.py,sha256=89KKeZDoW0jvqeOOrgaLoGFKZxVhh2jFLbAioi0hEi0,621
-olive/hardware/accelerator.py,sha256=7AmA9WbrU9mgi0jScRYYJ-LpZ4PAwNd7trVlxEQjW0o,9413
-olive/hardware/constants.py,sha256=Mu7aFqMPifV_gRH9x66Hgon8IDl-46N0tHTOs8g9IHY,1353
+olive/hardware/accelerator.py,sha256=2i_qGlWhxCsVVmRTnclDr7X_4jkgGDnjRQLFSBQuk6g,15059
+olive/hardware/constants.py,sha256=ZzHG-NRYy6L_XM-CSKSxv7pxcGLyXrKzLapqtafKmV0,1273
 olive/model/__init__.py,sha256=ogykDHRcG2bcvxHJMYLzJb30SbpNmtDpzbIBfboVVV4,451
 olive/model/config/__init__.py,sha256=HqUlWk6oUdFksNgoz9z77mgE8r8Xlb2DkZl2iub40yw,482
-olive/model/config/hf_config.py,sha256=G-1EIbbz0YYzh8CAsH0Rsnu48fOWox2LSucS0howTZc,10899
+olive/model/config/hf_config.py,sha256=ZGMo6BnTMs7M4WfqcDmqTO-Wbuj-NgG8S6IKcvBZuPw,10950
 olive/model/config/io_config.py,sha256=NF3LMWwwaNkSnT7iShk04aISRVJ3T4g_5odt-t8l08A,4843
 olive/model/config/model_config.py,sha256=X-kPWyMmHjB3WI6KUV7fgze2o3sS93SpiMsVpyqCbJk,3943
 olive/model/config/registry.py,sha256=PFa1p9xVBRHduXDa8UyzjXH8M6YSJ91nSyXa5e7Z8aE,1045
 olive/model/handler/__init__.py,sha256=dAx_DTEF4JOGePcPECDBqFWnkh1lHxAU7pYBrsxa9Ko,1139
 olive/model/handler/base.py,sha256=lPZOI9fAkCXpXZp4Yzi8xIhAtqDbR-Izccq5UYqtNiQ,3190
 olive/model/handler/composite.py,sha256=HaKW30CPj6G_nqFbWDQoo-DEHnBBSk1e4vS550Adc68,4461
 olive/model/handler/onnx.py,sha256=EuqtSqiFWg64xeSKROPXIcEpKpFsFxuedH_jw3bwIhg,8706
 olive/model/handler/openvino.py,sha256=JIcZsjZ1wrsUgHVE4sYb8mwXnoS2zN2wGV4OZdE7Ajw,3556
-olive/model/handler/pytorch.py,sha256=zGTv5rKnDZ8y8aLUgqd_LdsP_l1em6Hex6aju_eV-7I,16307
+olive/model/handler/pytorch.py,sha256=WAMqQltXVH48zw6B3oCLWGa5Nvqg9P7GJ8yDhXCterY,16490
 olive/model/handler/qnn.py,sha256=77nhGBAa1KL5mxGWOOmNJcV7vkCqsLoina_5YWrq23M,4704
 olive/model/handler/snpe.py,sha256=QRxfGSQ6FrPCZGtQXjfRKhHHMvvFbuGq_j1Y_AL-IVE,2597
 olive/model/handler/tensorflow.py,sha256=dLLkfXrpuxG0ab93p7k1c3M0c_Kjjub78Rde__lox2o,1474
 olive/model/handler/mixin/__init__.py,sha256=HnupHTH3k-xB5XlpRjx8uuRQdKTiRX1tFu_vhb-xN4w,936
 olive/model/handler/mixin/composite.py,sha256=qpQ41-7XsIz2l5X8ab2-gt6OZJDPZUInacW-1H9HopQ,418
-olive/model/handler/mixin/dummy_inputs.py,sha256=zvc19ctoi4Li-MlSxsrsuN-pE4mUan7QQiq2vXF62KI,3021
-olive/model/handler/mixin/hf_config.py,sha256=-V4PuJIgb_G6J_pjegFPNTQ7-DcnOeBLhSe2n1dpD9A,3959
+olive/model/handler/mixin/dummy_inputs.py,sha256=U-e257lcwqEkLgNTMPO5VczLU92Le7w-CXdBvZh5DDw,3021
+olive/model/handler/mixin/hf_config.py,sha256=hiffIeyfFDJ1klhOwJ8dgRgq6-5ivOMtB2Lio1PLNXc,3967
 olive/model/handler/mixin/io_config.py,sha256=hOIOJjeEhXxGgfUeKovKKS4NjpOm5tX4T2QnZntzPxE,613
 olive/model/handler/mixin/json.py,sha256=LnMzqAbDK7MBJucxVXxq400WxFq9pO6Xe5u69UQ73QI,1472
 olive/model/handler/mixin/onnx_ep.py,sha256=bZ4xKLSDpdR-AZaB2w_785IRU_qStLqs-G9eTPNQeJA,1402
 olive/model/handler/mixin/onnx_graph.py,sha256=PwVntvtKZuxFTPjssee_Hbc-xUrLTUBkbsDEtozuyHA,3946
 olive/model/handler/mixin/resource.py,sha256=VvVsto2k1AfNSzH7zxdgpYvt6gDiTcWlFsaPelA3M_M,2998
 olive/model/utils/__init__.py,sha256=OViqJhejw6VYFg7BO_zhp2WeCJzr9YPjUXumynwhARs,609
 olive/model/utils/hf_mappings.py,sha256=HJLxWMhTXSCmYW9NnCsn4Wr9PmGO39i2H9xEdrZ4L4U,3091
 olive/model/utils/hf_onnx_config.py,sha256=5njt4Ko5NCL_3Ipy0wdUwvjmaxLmwdACT5iuNWXnw9g,4517
-olive/model/utils/hf_utils.py,sha256=NSRt9UuMH8PUAv9K7CEQsjxGH0OZkWyu3rkPIL3Nmec,7888
+olive/model/utils/hf_utils.py,sha256=8LPvDe50b_Eaf6qdmawaGdLOVFwE7eKvmtTXlVUsBo0,8183
 olive/model/utils/onnx_utils.py,sha256=dMC2_m7rvLx9NTt8KxUCucrC70wFkSF48hNv6IlBrN4,2893
 olive/model/utils/path_utils.py,sha256=lu4r8V_PjCyFrlzxfAUZrRKAa3VhV67o9_4noYE1V2g,1510
-olive/passes/__init__.py,sha256=9hMQ4J0O3Pr4W2izpX4BH_NcAcTjX-GXE2LprkJd-p8,693
-olive/passes/olive_pass.py,sha256=L7T2C-3iQlbo6oI0iOY4FCdFyHc0mpzaza5lfjE-xmY,19969
-olive/passes/pass_config.py,sha256=vGDviTs6mJsZx979ATH-Ygb_Gc_NOkcxVahRDgS6BaY,5154
-olive/passes/onnx/__init__.py,sha256=zZKwolvjaXhy7-XCF8h-vbif6ohQ99wLRe7ZHl6Va14,2285
-olive/passes/onnx/append_pre_post_processing_ops.py,sha256=QPmJ7APNvSG3nXw0gbCl6-XVU_xQL8YelWfGjia4HQ4,8404
-olive/passes/onnx/bnb_quantization.py,sha256=tzwJuveSDAqF9yaXz4O3vr92o-st49u0ikZxQjl_J9I,5520
+olive/passes/__init__.py,sha256=kzwYdsbpTo9ipqKgoTRf1hV-Jr2OnJlqdg_3-3iiECM,499
+olive/passes/olive_pass.py,sha256=GjmwtSVu8ZXuJxmyESxczmvs6ALpR58lm6qadIj-7QI,19895
+olive/passes/pass_config.py,sha256=Ijdjp_1NAtMkhrM7jE-024aTdJF98CQsRTtb0xHp8hw,6139
+olive/passes/onnx/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
+olive/passes/onnx/append_pre_post_processing_ops.py,sha256=pYwd1LeAFym37hLknAIi90UqLTeCCzLRuzhbJVfUifQ,8408
+olive/passes/onnx/bnb_quantization.py,sha256=s9SfMEHXizkwP85VBpmPYxPiqTnCgyMA-7Sj0FqzXmI,5524
 olive/passes/onnx/common.py,sha256=Y53MVrBhIKDQLCnvOlsRkxa_A6p9I2djDXVDMTbYnNw,7485
-olive/passes/onnx/conversion.py,sha256=LwtEd-T8lIA84jtWf35kWmKajNqRuzLNufrPvhdfbd8,26410
-olive/passes/onnx/dynamic_to_fixed_shape.py,sha256=edc5U9CTrgM4Fz0dUMx9b9UV5kL8ayoyRli2iQ282y0,4856
-olive/passes/onnx/float16_conversion.py,sha256=sOFW7sZ7YrzPQ8IkCUjzU7g8jAk33w5Eq_5JE2hDfu0,3086
-olive/passes/onnx/genai_model_exporter.py,sha256=mm6nXqNXd7sdeXcZi8ABaSFCEmqmuFCChXFCFxViOHM,3507
-olive/passes/onnx/inc_quantization.py,sha256=n2bYL8bUUn5Z5Bm77GFc_hEXMaI0N_pZxWU_roT0Pio,26480
-olive/passes/onnx/insert_beam_search.py,sha256=JqzYN6SSYGlooN1DERieXqXAKnRX4SiqizVi6V7Anus,12955
+olive/passes/onnx/conversion.py,sha256=TkyQrRrO1Tcc6DOi3U8C3PScmHLHte1g1rOqTtJsKi0,27072
+olive/passes/onnx/dynamic_to_fixed_shape.py,sha256=Qf9WEa088IZgFddNX-aFANEy_2N6aWaHXg9-Bwo5Qqc,4862
+olive/passes/onnx/float16_conversion.py,sha256=zzBB4q8I_MXExjwVJTFD_W4KGMtAJRxowwU5sEqKrRY,3090
+olive/passes/onnx/genai_model_exporter.py,sha256=cQ3HLl93BdvqBr7xZ8aNHnHNUvuG4J8qJDHfBoYYCl4,3491
+olive/passes/onnx/inc_quantization.py,sha256=owgy3tnFxDO57RIzV2xGPDEfe1OFRMH9xXnLK4xYFjg,27095
+olive/passes/onnx/insert_beam_search.py,sha256=6hEavIrzEK8VoSzhrFp05hprSCKzNn6UFdjyHg_bkJU,15972
 olive/passes/onnx/merge_decoders.py,sha256=NNKKa0-FVa1vog_g-4LPSf916Zk3aFVA1wEVUhnj6tw,18495
-olive/passes/onnx/mixed_precision.py,sha256=ge9PVN5XZcngPE2dKCy3GA1JyJ6izOX5XEjdO4SXV2o,9050
-olive/passes/onnx/model_optimizer.py,sha256=6Qwn9RkUAoT2eAPNIdE5iuztdPTJvqy7j8zsMvq7luk,10007
-olive/passes/onnx/moe_experts_distributor.py,sha256=ikWJkEjP_NUdjHrMQS9Ts9SXIHjUiyIjnCe1pu2OfLI,15816
-olive/passes/onnx/optimum_conversion.py,sha256=0crWkCxTKbKtF6aUgqctRiof2FnIrAkI5TtvhL03rgE,6158
-olive/passes/onnx/optimum_merging.py,sha256=iXVdjjiW41hhzyQT0NnzgMiAqBclFGSVg9olusoVLr4,3680
-olive/passes/onnx/perf_tuning.py,sha256=cVrEoi_Pc0iJnrR8JzT1MoG8M557Ur60lAsMugipMSc,29999
-olive/passes/onnx/qnn_preprocess.py,sha256=Ndx-RH4kxAcutz2CtiUHoHiXbhrNLtqxMebOEWPsIaE,1962
-olive/passes/onnx/quantization.py,sha256=KfJBwrIrdfuIyO6mnlseha_tvcGV2GXTxGgWraU8XAE,27601
-olive/passes/onnx/transformer_optimization.py,sha256=7iyKN9Coo_QCKrB2b_2iZRF_V8o_PsDpF2BCmFsOELE,18748
-olive/passes/onnx/vitis_ai_quantization.py,sha256=3mADyquo7hBFESMfwjIsQ47dyqiWOeHvNXGP0Fo1nAw,15105
+olive/passes/onnx/mixed_precision.py,sha256=RS7Rmaoknc76_0qdMw_tEnIdJxvlAIku7LHunjjj4oU,9054
+olive/passes/onnx/model_optimizer.py,sha256=0l2BmghwWBCK-08F90TO7VY1swU6_AY3-LqVXyQCH-Q,10011
+olive/passes/onnx/moe_experts_distributor.py,sha256=GBdDgDpiTjWCVwHFnwmubZGxNJP8smz6KCqzTraJHiw,15752
+olive/passes/onnx/optimum_conversion.py,sha256=HDcMkyrfeIFSvuQbD33YGzqS7A2JDn_B1zKJGUbDedk,6162
+olive/passes/onnx/optimum_merging.py,sha256=OSh2gSAOjXj_BIFX_NfRMWWlDPTHlKWmEjYIcAXhSps,3709
+olive/passes/onnx/perf_tuning.py,sha256=HaEHnIkQK5nVJe2dVwfhAvLi0jRE0LtQaznDj9tKKRc,29003
+olive/passes/onnx/qnn_preprocess.py,sha256=C3KsKo7DpJP-L030LvVnoCzW1ax4CDefn03yRiCy8BM,5375
+olive/passes/onnx/quantization.py,sha256=suwQHlhS5emvQ5Ru8TBJ_HPWCarM8bSqVp2fcWN0gHg,37646
+olive/passes/onnx/transformer_optimization.py,sha256=HYxWW5kBQ0zVCn4nLsQvCbeE41cuNHJcC6kRO9eVCtQ,19253
+olive/passes/onnx/vitis_ai_quantization.py,sha256=F0jveoEo8Qb1Jbg4EZwadhQv26PpWD6naq-BC4glvks,15134
 olive/passes/onnx/pipeline/__init__.py,sha256=hy6ZqdDw4q53FSvRAM5nh7dCOGt1ESZagOqThowBZAI,3239
 olive/passes/onnx/pipeline/step_utils.py,sha256=YxajZp5gVU0UVpHrbonNJwZL8CHiNI5TVpye0WGpIVw,8457
-olive/passes/onnx/vitis_ai/__init__.py,sha256=qj_elRUSfLaHn5AtuigzPt8seP0cAKuVNLc7AGYaauY,644
+olive/passes/onnx/vitis_ai/__init__.py,sha256=WcngHdIXOp4kglwsc_G4qqBKU-wsyNk94ibGQkSv-eU,508
 olive/passes/onnx/vitis_ai/calibrate.py,sha256=IXLi6rSAwJ4fX-POa9dwHSlwPYSKsXN62g7tdUat5CY,7762
 olive/passes/onnx/vitis_ai/quant_utils.py,sha256=5YrlKa3QLpCd8rgZ1uSnRFApXlH9tQ68J4RPASX73U0,15332
 olive/passes/onnx/vitis_ai/quantize.py,sha256=2p6fpiJ8WGxeMhH9h1rWo83ocfc9lHtp2fcrCnf3A2w,13580
 olive/passes/onnx/vitis_ai/quantizer.py,sha256=n8IK8jb3b7PIFDmBK1WCxzsq3LvWxCUUgoxgg2td4LI,36359
 olive/passes/onnx/vitis_ai/refine.py,sha256=BcB69YUImoFVaafwzrjG4wEd3xGbQyyCA-wF007Hs8c,18700
-olive/passes/openvino/__init__.py,sha256=dgWxpxB8h0gZV6j_5qQCE-U0F9O3cVyqkmFZUKnUdYU,448
-olive/passes/openvino/conversion.py,sha256=BsXlwt_yIgh-5SB46E8HdzwPlkvQLTF5Ra9tXZvtZ5w,4551
-olive/passes/openvino/quantization.py,sha256=vejQv2rX_mdJCx6YpUk6Y6MM5NVzrb__v5MdbD2cPG4,12801
-olive/passes/pytorch/__init__.py,sha256=38L_3NQ3YO0GNdjtEYpjRVQURA65VTc1RFvqmf5to3A,746
+olive/passes/openvino/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
+olive/passes/openvino/conversion.py,sha256=qFxo55aCHtIIe2w8TWU403QORZsr55BdgcJLxPbd9Fs,4555
+olive/passes/openvino/quantization.py,sha256=s-zPf2zId_ucfKwV3mhbP7bwKda6d6CY5XQ0IZ_WgAU,12610
+olive/passes/pytorch/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/passes/pytorch/cluster.py,sha256=wPZilm6bpu8_KvhVYUdy4fM9b1US3bE-3IWPqDqi9gs,7058
-olive/passes/pytorch/lora.py,sha256=4G6vqRILjENb5dy7G5t-5AWNQhm1rfmhWDOVN5aCmgw,48323
+olive/passes/pytorch/gptq.py,sha256=Ytq51gMdHVfOVJR0SCFlGnBMhvuH35CtIBNF9twgFJQ,9842
+olive/passes/pytorch/gptq_utils.py,sha256=oCTV62B4jmffUDVQm2ceBN4iY91cAhSntLn8EKF6jEM,11528
+olive/passes/pytorch/lora.py,sha256=ljq2FmlNvjL1asoZZtXhn-oWpfNEJFUTkgbHb5ZsHYI,48812
 olive/passes/pytorch/pytorch_lightning_utils.py,sha256=JyHsdwapI9Z1d7b95MMCEGqpT3FvwzPMT8iOO84CsZU,1021
 olive/passes/pytorch/qat_utils.py,sha256=f08zUGQTvEMsSTDKmIl1Aa2VkRjAW4_sFxhrGZOMp_o,8038
-olive/passes/pytorch/quantization_aware_training.py,sha256=llz2oA-q0q_F1taosJCPuvqV0Artc2h0i2Q47lRAYlc,6440
-olive/passes/pytorch/sparsegpt.py,sha256=3JghVIXnCvDIdNYl3_yBLAAzPTZOs1kIW2T6eGsvEJ4,9049
+olive/passes/pytorch/quantization_aware_training.py,sha256=dl2YSOJoXuTWG8vQM_uQqvnrqBu86IGXEQrH9J4UPU4,6444
+olive/passes/pytorch/sparsegpt.py,sha256=96vnAWMe4KMcG4-LcdNlxluyUBuO9fIpH2mjx4s5JPE,9021
 olive/passes/pytorch/sparsegpt_utils.py,sha256=PgqZh_HYV9DTzURT0T269r984KxDNhKZQ10sfDDEjpc,10774
-olive/passes/pytorch/tensor_parallel.py,sha256=qyIrfScg4A33-KXYNOczRkVsE2Vp_IsRTYqTOkhSXnk,7118
+olive/passes/pytorch/tensor_parallel.py,sha256=Ft798VoHjXKLlDItvwydPnvF9ak2FKyOYbVPg7rWkiU,7032
 olive/passes/pytorch/tensor_parallel_layers.py,sha256=91Cc4y_xfP_TNBg83izDXxD7Ot8g4zWkc-ajMc-8AmM,5372
 olive/passes/pytorch/tensor_parallel_llama2.py,sha256=X3lbqPif5VoVia03lma5AuLYrx8SCp5NGXKXh6DS7Zw,17239
-olive/passes/pytorch/torch_trt_conversion.py,sha256=eItirBMQz2aZG3qi1gAyTbMdOm-r21eMWWQqVL9wXxQ,8049
+olive/passes/pytorch/torch_trt_conversion.py,sha256=QhZUtkmPCCMeAilZrobpW95hwYz_yQ4j6gQH_C2tpCw,8053
 olive/passes/pytorch/trt_utils.py,sha256=yXqmVoRruHl8raI3ySj8GItaPEbRstqbZo7G-HLZiBI,3185
-olive/passes/qnn/__init__.py,sha256=KGNTuo9WTB-eXfWc0z4FCaMvo0sKkLukT7sWWINQhiM,534
-olive/passes/qnn/common.py,sha256=iGuaT2tSQjJEX8X8riHgZw7Nh4yQGojv-CIKCCBqHOY,1460
-olive/passes/qnn/context_binary_generator.py,sha256=n-Hp3Y_wra83SreY-ZrlTtBKRy0JQx7QX9aS45Bg-Rs,3440
-olive/passes/qnn/conversion.py,sha256=f2ELsqqYw4inEhB5ZP0qVQ_AI9-b9n712iN2rcfGP7A,5516
-olive/passes/qnn/model_lib_generator.py,sha256=sfu0sqgab3pRJklo1bXPNQCKB0_eNRiO-lY2JqV0fgg,3546
-olive/passes/snpe/__init__.py,sha256=glmLIaMVKkjct_PqWk7x9Av-twMYU08ez9eJyNdUNgM,501
-olive/passes/snpe/conversion.py,sha256=BMqC22XDrU2xoKkGAU7aut9TRSyxt0XPxwo0SIbcUyw,4844
-olive/passes/snpe/quantization.py,sha256=reZ39eG1o-QOaYjbMgYWg-fsFCawNdYjxyQj-WQn0dU,5246
-olive/passes/snpe/snpe_to_onnx.py,sha256=q3Ib1DuNK4RT1Gide2i_et3VZ8XWLNrsK2ly-0zQ7pM,2631
+olive/passes/qnn/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
+olive/passes/qnn/context_binary_generator.py,sha256=2EEBfThotk2288WXsU-eTjZWq410sv8bMfdqG_ICBx0,3192
+olive/passes/qnn/conversion.py,sha256=atW7a7OsXL3YvxeeOr7q1X0hx5QnSkN5RwMCs4QGRys,5268
+olive/passes/qnn/model_lib_generator.py,sha256=aDjHkUZ4To8JEu-9wo7RHbF2j278dCPwhSSiGnrG0RQ,3298
+olive/passes/snpe/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
+olive/passes/snpe/conversion.py,sha256=0zUfbIP_3NKgyDxK5bKrm4AA5lSbJAF0yVXO-R8n7S8,4802
+olive/passes/snpe/quantization.py,sha256=bTkAnbP4DhJk24FmV_TzsBCWM771nSVZq-NH-X7eK5I,5254
+olive/passes/snpe/snpe_to_onnx.py,sha256=-K11FhE31t3LFb6hzjFn0NRK4y6lU5DlgJ7Umo30uzY,2637
 olive/passes/utils/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/passes/utils/whisper_prepost.py,sha256=8osxHqa0wt6qY-EyG3hfEeBXH6v-QRsYZ_FH7MrPQ-Y,1619
 olive/platform_sdk/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/platform_sdk/qualcomm/__init__.py,sha256=4AO2N1wSJzJPtPuEk2eWpRE3LrSybtJqep1vs0RPUPY,341
 olive/platform_sdk/qualcomm/configure.py,sha256=JbkQn71ClNhNAkvO0BoEPgS0qPaODwaXYY9QIL7J7i8,3740
 olive/platform_sdk/qualcomm/constants.py,sha256=rmj27zwJFo7k2yirMbpE876eGW7brw7MFlSSFOI3PA0,1348
 olive/platform_sdk/qualcomm/copy_libcdsprpc.ps1,sha256=uJfKdyPWGWZyVHy_pvjhGzYK1pKUocI5LTAo9_QkkX0,1046
 olive/platform_sdk/qualcomm/create_python_env.ps1,sha256=UjbVZ9q9QnKYZUzjFkjBelTG4INyIu8-mq5GRd2uK_4,2387
 olive/platform_sdk/qualcomm/create_python_env.sh,sha256=MBEu4cyIltBIKMxgP27cPMgJCMVMjX_njV5h1zPWZ-I,2431
 olive/platform_sdk/qualcomm/env.py,sha256=iGRpJt6rxXxKVFjHLEfh-z5RFNecyc30aK4yvb-NsTQ,4034
-olive/platform_sdk/qualcomm/runner.py,sha256=mXVOOzYSQKRaXW1csBMtWgLVQ9DmBap-zIXr4zPM-7Y,3022
+olive/platform_sdk/qualcomm/runner.py,sha256=pgC1iuUOmGLGn1wPbETwqObgP1ERJBXyETFKzY6OU9w,3652
 olive/platform_sdk/qualcomm/qnn/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/platform_sdk/qualcomm/qnn/env.py,sha256=AgrAnb8XAvyNvAXypZc98MicwJA9sW7Bv49ZlY3tIEQ,2034
 olive/platform_sdk/qualcomm/qnn/qnn.py,sha256=_XOE8eByRsEGxuRebctpr9jcPZeyVrbIwalEtsh3d3w,7269
 olive/platform_sdk/qualcomm/qnn/utils/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/platform_sdk/qualcomm/snpe/__init__.py,sha256=IWmOScWKUAm6utpxkEbcDzqQd9JTJ9c39_g3wZW_RiE,396
 olive/platform_sdk/qualcomm/snpe/env.py,sha256=Ma9k7CCeuUDeSgDBEcugcPXIwzWv1T2o23MkVdbdFjI,2117
 olive/platform_sdk/qualcomm/snpe/snpe.py,sha256=0Abgu0BAYKawvSpP6mWIQMCuHNdosUnh9dZv7wOTC8s,3969
 olive/platform_sdk/qualcomm/snpe/tools/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-olive/platform_sdk/qualcomm/snpe/tools/dev.py,sha256=eqq91efbB5lxSI7faUadV-fF8G2obqZ3x8TkhEoSGro,10518
+olive/platform_sdk/qualcomm/snpe/tools/dev.py,sha256=wO4sEHbtH0fWJMo6606lzhXUrAOxfbYfMIVu8GZUvaQ,10951
 olive/platform_sdk/qualcomm/snpe/tools/inference.py,sha256=W0Om9Z203cISj38813WuVXuP8Wif0DWOYwtcTEJZ_qg,19039
 olive/platform_sdk/qualcomm/snpe/utils/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/platform_sdk/qualcomm/snpe/utils/adb.py,sha256=fW9nG-7YARNAglJIRry4t-9UwcycbFhSYWoByNHKxhE,6978
 olive/platform_sdk/qualcomm/utils/__init__.py,sha256=nU7Vcr7_IqPRj7pYSd2ApPT_KRQxpQUapEQAlVR3yG8,374
 olive/platform_sdk/qualcomm/utils/data_loader.py,sha256=BAWOMxoDKKZESo0Owlegae3gtBl8WWnDvGgDSSt1YSs,14746
 olive/platform_sdk/qualcomm/utils/input_list.py,sha256=mvQQlu154du1f4iawpw-FwxPWEHMfSXR-2EPxDtlUCs,8878
 olive/scripts/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/scripts/manage_compute_instance.py,sha256=9yRsfSbIQDMrlI90wUMk1SWr8-yFnxRm9vLwKg8710I,6435
 olive/strategy/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/strategy/search_parameter.py,sha256=FmKvyblJHZbS1gAh1S7589WE6yL2MbIKL38-io_yDyQ,11837
 olive/strategy/search_results.py,sha256=fD0uXHefoLDWdRzM5I36yGyM8OBLHrY9NFqGqBiTTs0,5631
-olive/strategy/search_space.py,sha256=igTl04mxQvEtCEbtEixdGqUdQVY7ysSZ1jlOHAsRq2Q,4633
-olive/strategy/search_strategy.py,sha256=PIk9w5F5SLobXF-Sb62LGxsrxhQpRZ7nI6rJPM4KLzc,12885
+olive/strategy/search_space.py,sha256=jYGOECHWZkkVahQVTUbPe09cXoiSIxcr4iZBCoWbOFs,4702
+olive/strategy/search_strategy.py,sha256=nO9T1dy06O4-yZ0FX_9c-j8eRgr3syREF1R5GmpDg8c,12873
 olive/strategy/utils.py,sha256=5_JB0POwrCN30LFehSafjzeK6I5AJUT0Qfm0xznjZV0,2817
 olive/strategy/search_algorithm/__init__.py,sha256=K5eStkEgcBM1yNTvOAqk042RO44JO7bwYvwaoqTDG74,705
-olive/strategy/search_algorithm/exhaustive.py,sha256=Wp3aZaPfnLfAnwyiusOyTBd1KbJPHMTT0GD3dr6wW-c,1089
-olive/strategy/search_algorithm/optuna_sampler.py,sha256=hTdtxj0OWbAwLPXyaluNRnq2hp8rhGSTcBOhUBQFsxM,4332
-olive/strategy/search_algorithm/random_sampler.py,sha256=2kMV6sRulx660rvgPn2iOABPrSgIujOhsBSJX0CMctQ,2419
-olive/strategy/search_algorithm/search_algorithm.py,sha256=0aaU0RYZvRir7_KV0bCXpPTjoSE_Ecnv-9LRRpcxUaI,2390
-olive/strategy/search_algorithm/tpe_sampler.py,sha256=fagifcBfHBURHctw5GaPipzJ1CLYtBiLC75jeQm68t0,1853
+olive/strategy/search_algorithm/exhaustive.py,sha256=YXu2iXFfsf8SdzlN_G0yjpMopIrAWn5oFWY08xUJ6Gw,1091
+olive/strategy/search_algorithm/optuna_sampler.py,sha256=kwc1eKIHalLK6pTdYmIKor1LLCdMlFd3QWQaO_303GI,4334
+olive/strategy/search_algorithm/random_sampler.py,sha256=zbZ63YzQ9AwSF5zp-l55kWpH5Ab-oxAy-5rMSW8B2NU,2421
+olive/strategy/search_algorithm/search_algorithm.py,sha256=SA51NjNp4-fI3o0a4hwk2Shgq_T-jIZuLzpZqnUp1ds,2306
+olive/strategy/search_algorithm/tpe_sampler.py,sha256=_8mksflxg3pSsPWUK-5xZoa1Za9ve3oDnlxN9YbCjsM,1841
 olive/systems/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-olive/systems/common.py,sha256=3P2zCtMd0xnMBSF3R51FfSVI9UznZdy-OPIyxOC3iKA,1842
-olive/systems/local.py,sha256=qjlPmRGTwlFELSG0e4GRcREQsEdbgjR0nDl9YXqokIU,2448
-olive/systems/olive_system.py,sha256=tGYuoEM1Kot45oypsLtoL-7K5PP-PVok-HigMcV1hF8,1718
+olive/systems/common.py,sha256=QqZoabZxxSLdxCuAlnALlzLYYc9n2uu4I-DqMw8M0qA,2227
+olive/systems/local.py,sha256=kWfHWzfPLxybq35FqchqJ7v2PuKIpPzG23qekTH8mUU,2300
+olive/systems/olive_system.py,sha256=guo9D784bL9k9Va9tWOfmtHDDhytrEe0A8Hplnoruvs,2158
 olive/systems/system_alias.py,sha256=rqDxUPiJZNFArMX_mg5No3W7MWOlyppiyG0pk1SmITE,3140
-olive/systems/system_config.py,sha256=tMp1wZUEtFl3K-rJMhmR97hUJizIWN0WbDNh1_xQHkI,5151
+olive/systems/system_config.py,sha256=M6axCyIKodee8bBYvBJHEaIQRXO4AW9fIZ2aAbJQq0o,7048
 olive/systems/azureml/__init__.py,sha256=XMk00ngVPkLq8SAjhtDDXDUC6FDin5K_s_eu1FGi3qA,411
 olive/systems/azureml/aml_evaluation_runner.py,sha256=nI6kKyA9xsV7oJO5jd_B9h2zZ_5Ez0lknwGyXiZQrZQ,2883
-olive/systems/azureml/aml_pass_runner.py,sha256=-C7eHRLJOBW_K25SQi-abj-ltTuATlpL-pffe48EE6Q,8521
-olive/systems/azureml/aml_system.py,sha256=KPTMlMAkaTkQCEogkBVT0f4HTq8RkNvWV9W2yhXWENo,31742
+olive/systems/azureml/aml_pass_runner.py,sha256=rb4KVQS_23l-ogpiV-0TjuOnlRyUI1jJSW8eIXzxEC4,8763
+olive/systems/azureml/aml_system.py,sha256=KP3E2wXj6bu8sz4CqhiPye4Xs3e3mdmFCxowqRFt58w,31831
 olive/systems/docker/Dockerfile,sha256=XAQOT_ruAGlgmeoXG2E7n6clsjMEV03BSwyVps5njpo,717
 olive/systems/docker/Dockerfile.cpu,sha256=o5F5Pc2J_FhtZZT2YrXnLDb1vwhx0OYlTQ_m6LqV7VQ,464
 olive/systems/docker/Dockerfile.gpu,sha256=GdrY3B1IZVB-PADSXF2cEwJWwXjxomJP-qG29a1G45w,1006
 olive/systems/docker/Dockerfile.openvino,sha256=kFtqfQys13L0hx66YxuCMY9XXx-i0D0jtgSL5bASOng,2531
 olive/systems/docker/__init__.py,sha256=nrCMDPi17qCk3iGuH-Hbpq9JIOYkR8nOdvWsfXy9uqM,407
-olive/systems/docker/docker_system.py,sha256=R6VBB7OWUGTgiqSrc5yIY6QDBmbPuN62UyAOU3LGNGQ,18212
+olive/systems/docker/docker_system.py,sha256=eHFtKIIp8GhtLoCRe4QOKGgUkOzFH4TEA3DYhv-KDzM,18244
 olive/systems/docker/eval.py,sha256=bfeDsxCb6H3P8EqtQGzlzHWpE10D_gUyoZzAZlo4sds,2187
-olive/systems/docker/runner.py,sha256=0rGja8p8dDknapJL3-qhPbBkzZVNLQZgipfHkH_QHz8,1713
+olive/systems/docker/runner.py,sha256=E9bahyiRo56ZfADz9iiF1FbDDlHwpk1QlEdlRu1IxkQ,1956
 olive/systems/docker/utils.py,sha256=eU0Khr8nvn_BO9j3z6Kp3JRPZpSXVxRSs9qu2Om3XvY,6200
 olive/systems/isolated_ort/__init__.py,sha256=vITx_9_O5lJ6lVD-Eh8f0mbLB5d5sYHX__HgjpAYOLg,357
 olive/systems/isolated_ort/inference_runner.py,sha256=4zxrduxioctdc-80iVjqeqUkKUlegiSBuBE_BrtUqJE,3077
-olive/systems/isolated_ort/isolated_ort_system.py,sha256=Nx8tNaB7Aya7uwymX79kWndAapSrx7_2Nd50RuOqo8Q,10786
+olive/systems/isolated_ort/isolated_ort_system.py,sha256=CuZf_qhnIXE7RDkdGm4r3ME6N0dtA21Owlr6jgXaTSk,10877
 olive/systems/python_environment/__init__.py,sha256=cu6jFeXKLees7MkoNddzPVre_eE-FwXwA3pihv4yWi4,381
 olive/systems/python_environment/common_requirements.txt,sha256=chggo_d82Zdffa3G3ZKCCpI27wcCn2f17Cziy-x4f0Q,37
 olive/systems/python_environment/evaluation_runner.py,sha256=isVzvA2aHVuCK-qfIyAqjnS03c7vzIXmYIhFjGtiCME,2279
-olive/systems/python_environment/pass_runner.py,sha256=AZcmBFJyz5Nc2GO3YLdAQosgEO5M7NqqMPkYLEYIreg,1767
-olive/systems/python_environment/python_environment_system.py,sha256=W1igaXKs5OCf-FQQyzxoX-GjLXtK60ICYK6eLXds1Ao,7944
-olive/systems/utils/__init__.py,sha256=P3HekYJc4zSk1HvFq3KOZvXHqWro06VuaJH1NXSYjSs,689
+olive/systems/python_environment/pass_runner.py,sha256=IdvBa0Nd3D8WspdWj8ops3OVJl8b6SDnFYfVVtAlbSM,2037
+olive/systems/python_environment/python_environment_system.py,sha256=PTNWHKu8CJ8KqnUHxpqYLhjVwi-xktz4m5aCGTBLAkY,7892
+olive/systems/utils/__init__.py,sha256=VCiDPtACqpIXj-k-8rpHv2xsW4JYiRVfylM5qkF36RY,705
 olive/systems/utils/arg_parser.py,sha256=ZyFvQ9o6i7SqrNffnmuYvn_jjnGlWcRNSkGkZk9HZhE,2082
 olive/systems/utils/available_providers_runner.py,sha256=0A2C61-D5r9ke5ya2hOBEp1RP_6jgeG67cJC4NlqTH8,913
-olive/systems/utils/misc.py,sha256=sddFoufuhmKwR-nqW_RtfTI32pxsh0ovOagos2Wn7NY,6783
+olive/systems/utils/misc.py,sha256=YFgTeaBiKXx3ibnDMpORj3U4-dwsqqQ4mkEtJ-sKx-A,7419
 olive/workflows/__init__.py,sha256=2mLegObegOEd5p6MDGX_4-Eto2XvWBFPyr3HIrz4D0g,306
 olive/workflows/run/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
-olive/workflows/run/__main__.py,sha256=FTtvBnAxbhx_v3Uf3VFMfpiwHh3Lbji6BEI158oioWM,980
-olive/workflows/run/config.py,sha256=2emAbt2sX0bo5xZH9EEmCpFqU9t-jvA3bOi0cbOXq6U,13705
-olive/workflows/run/run.py,sha256=K1TFsztB7pmtryUqyrCvbzj3cQjaa2dH9j8UbUruTRE,11781
+olive/workflows/run/__main__.py,sha256=6awG6OZneIa769QDU6X86X5BxSzBtFByd0K5YQ671RY,1418
+olive/workflows/run/config.py,sha256=PRPdFfa_86Kel4ilOdVAxEy7RQX-Y9VbFsP-po6edjc,13710
+olive/workflows/run/run.py,sha256=_LyCQGMTcz21iP7GrydJgqr-xtXQsTinN9RU3ZSvXQw,13451
 olive/workflows/snpe/__init__.py,sha256=wqFn-4s3Sxzuy0Oa2euleTB7qUO-4g63bV2h7TW5XGU,431
 olive/workflows/snpe/convertquantize/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/workflows/snpe/convertquantize/__main__.py,sha256=re1UWNmJI_E9ubcrAEs4JeP_A4C3LBQMnVofw9TvI40,1339
 olive/workflows/snpe/convertquantize/convertquantize.py,sha256=GaM0LpjcZVgi89vh8r0losJDltNC6sKZtRB1rUVxXbU,4445
 olive/workflows/snpe/evaluate/__init__.py,sha256=WSNxt3l4lkWA_0vhw1ED4I7KOO0mnAuHwSgFYqbq0Lg,247
 olive/workflows/snpe/evaluate/__main__.py,sha256=UAmzVqOnC90Ns3l8oBhHPheyEC2kncFgC-Gl7qXAoEQ,955
 olive/workflows/snpe/evaluate/evaluate.py,sha256=NGRS_9whYc4wvSola46jLiQduSb4mVr1AKlpCr-bmB0,2974
-olive_ai-0.5.0.dist-info/LICENSE,sha256=ws_MuBL-SCEBqPBFl9_FqZkaaydIJmxHrJG2parhU4M,1141
-olive_ai-0.5.0.dist-info/METADATA,sha256=cpAjCPe0OKKjeLV_8-eimMf6tgDU-mlci3M6cXt0VAU,3220
-olive_ai-0.5.0.dist-info/NOTICE.txt,sha256=h7RtTa8A8uYhoKJ-nO_kScaD8qHLYj0en8UYEqpy5_E,764423
-olive_ai-0.5.0.dist-info/WHEEL,sha256=oiQVh_5PnQM0E3gPdiz09WCNmwiHDMaGer_elqB3coM,92
-olive_ai-0.5.0.dist-info/entry_points.txt,sha256=XVTpnzIsU7uElE03r2zsuqXrvUKWbMw-ndKgwSBYAoM,101
-olive_ai-0.5.0.dist-info/top_level.txt,sha256=V9j-sv9fsgSHbZLD-a-cxoocQvfzNaWA8w_3snFQbhQ,6
-olive_ai-0.5.0.dist-info/RECORD,,
+olive_ai-0.5.1.dist-info/LICENSE,sha256=ws_MuBL-SCEBqPBFl9_FqZkaaydIJmxHrJG2parhU4M,1141
+olive_ai-0.5.1.dist-info/METADATA,sha256=pyYmSPw0apWCIMgkJJuoqnX4OJb1iJNCEALraV0Js2E,3438
+olive_ai-0.5.1.dist-info/NOTICE.txt,sha256=h7RtTa8A8uYhoKJ-nO_kScaD8qHLYj0en8UYEqpy5_E,764423
+olive_ai-0.5.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+olive_ai-0.5.1.dist-info/entry_points.txt,sha256=XVTpnzIsU7uElE03r2zsuqXrvUKWbMw-ndKgwSBYAoM,101
+olive_ai-0.5.1.dist-info/top_level.txt,sha256=V9j-sv9fsgSHbZLD-a-cxoocQvfzNaWA8w_3snFQbhQ,6
+olive_ai-0.5.1.dist-info/RECORD,,
```

