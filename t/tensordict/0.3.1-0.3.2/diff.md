# Comparing `tmp/tensordict-0.3.1-cp39-cp39-win_amd64.whl.zip` & `tmp/tensordict-0.3.2-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,40 +1,40 @@
-Zip file size: 267892 bytes, number of entries: 38
--rw-rw-rw-  2.0 fat     1613 b- defN 24-Feb-26 23:47 tensordict/__init__.py
--rw-rw-rw-  2.0 fat     6156 b- defN 24-Feb-26 23:47 tensordict/_contextlib.py
--rw-rw-rw-  2.0 fat   126860 b- defN 24-Feb-26 23:47 tensordict/_lazy.py
--rw-rw-rw-  2.0 fat     3368 b- defN 24-Feb-26 23:47 tensordict/_pytree.py
--rw-rw-rw-  2.0 fat   118127 b- defN 24-Feb-26 23:47 tensordict/_td.py
--rw-rw-rw-  2.0 fat   113152 b- defN 24-Feb-26 23:49 tensordict/_tensordict.pyd
--rw-rw-rw-  2.0 fat    19584 b- defN 24-Feb-26 23:47 tensordict/_torch_func.py
--rw-rw-rw-  2.0 fat   207885 b- defN 24-Feb-26 23:47 tensordict/base.py
--rw-rw-rw-  2.0 fat    16915 b- defN 24-Feb-26 23:47 tensordict/functional.py
--rw-rw-rw-  2.0 fat    27011 b- defN 24-Feb-26 23:47 tensordict/memmap.py
--rw-rw-rw-  2.0 fat    33049 b- defN 24-Feb-26 23:47 tensordict/memmap_deprec.py
--rw-rw-rw-  2.0 fat    42659 b- defN 24-Feb-26 23:47 tensordict/persistent.py
--rw-rw-rw-  2.0 fat    50547 b- defN 24-Feb-26 23:47 tensordict/tensorclass.py
--rw-rw-rw-  2.0 fat     1093 b- defN 24-Feb-26 23:47 tensordict/tensordict.py
--rw-rw-rw-  2.0 fat    66595 b- defN 24-Feb-26 23:47 tensordict/utils.py
--rw-rw-rw-  2.0 fat       81 b- defN 24-Feb-26 23:48 tensordict/version.py
--rw-rw-rw-  2.0 fat     1634 b- defN 24-Feb-26 23:47 tensordict/nn/__init__.py
--rw-rw-rw-  2.0 fat    54557 b- defN 24-Feb-26 23:47 tensordict/nn/common.py
--rw-rw-rw-  2.0 fat     6020 b- defN 24-Feb-26 23:47 tensordict/nn/ensemble.py
--rw-rw-rw-  2.0 fat    25890 b- defN 24-Feb-26 23:47 tensordict/nn/functional_modules.py
--rw-rw-rw-  2.0 fat    36547 b- defN 24-Feb-26 23:47 tensordict/nn/params.py
--rw-rw-rw-  2.0 fat    25553 b- defN 24-Feb-26 23:47 tensordict/nn/probabilistic.py
--rw-rw-rw-  2.0 fat    19947 b- defN 24-Feb-26 23:47 tensordict/nn/sequence.py
--rw-rw-rw-  2.0 fat    13230 b- defN 24-Feb-26 23:47 tensordict/nn/utils.py
--rw-rw-rw-  2.0 fat      795 b- defN 24-Feb-26 23:47 tensordict/nn/distributions/__init__.py
--rw-rw-rw-  2.0 fat     6629 b- defN 24-Feb-26 23:47 tensordict/nn/distributions/composite.py
--rw-rw-rw-  2.0 fat     9924 b- defN 24-Feb-26 23:47 tensordict/nn/distributions/continuous.py
--rw-rw-rw-  2.0 fat     2667 b- defN 24-Feb-26 23:47 tensordict/nn/distributions/discrete.py
--rw-rw-rw-  2.0 fat     6694 b- defN 24-Feb-26 23:47 tensordict/nn/distributions/truncated_normal.py
--rw-rw-rw-  2.0 fat     1266 b- defN 24-Feb-26 23:47 tensordict/nn/distributions/utils.py
--rw-rw-rw-  2.0 fat      393 b- defN 24-Feb-26 23:47 tensordict/prototype/__init__.py
--rw-rw-rw-  2.0 fat     7869 b- defN 24-Feb-26 23:47 tensordict/prototype/fx.py
--rw-rw-rw-  2.0 fat      796 b- defN 24-Feb-26 23:47 tensordict/prototype/tensorclass.py
--rw-rw-rw-  2.0 fat     1119 b- defN 24-Feb-26 23:49 tensordict-0.3.1.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    19303 b- defN 24-Feb-26 23:49 tensordict-0.3.1.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 24-Feb-26 23:49 tensordict-0.3.1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       11 b- defN 24-Feb-26 23:49 tensordict-0.3.1.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     3200 b- defN 24-Feb-26 23:49 tensordict-0.3.1.dist-info/RECORD
-38 files, 1078839 bytes uncompressed, 262834 bytes compressed:  75.6%
+Zip file size: 280807 bytes, number of entries: 38
+-rw-rw-rw-  2.0 fat     1652 b- defN 24-Apr-03 06:08 tensordict/__init__.py
+-rw-rw-rw-  2.0 fat     6156 b- defN 24-Apr-03 06:08 tensordict/_contextlib.py
+-rw-rw-rw-  2.0 fat   133618 b- defN 24-Apr-03 06:08 tensordict/_lazy.py
+-rw-rw-rw-  2.0 fat     3368 b- defN 24-Apr-03 06:08 tensordict/_pytree.py
+-rw-rw-rw-  2.0 fat   127020 b- defN 24-Apr-03 06:08 tensordict/_td.py
+-rw-rw-rw-  2.0 fat   113152 b- defN 24-Apr-03 06:10 tensordict/_tensordict.pyd
+-rw-rw-rw-  2.0 fat    21200 b- defN 24-Apr-03 06:08 tensordict/_torch_func.py
+-rw-rw-rw-  2.0 fat   217075 b- defN 24-Apr-03 06:08 tensordict/base.py
+-rw-rw-rw-  2.0 fat    16915 b- defN 24-Apr-03 06:08 tensordict/functional.py
+-rw-rw-rw-  2.0 fat    27011 b- defN 24-Apr-03 06:08 tensordict/memmap.py
+-rw-rw-rw-  2.0 fat    33049 b- defN 24-Apr-03 06:08 tensordict/memmap_deprec.py
+-rw-rw-rw-  2.0 fat    43972 b- defN 24-Apr-03 06:08 tensordict/persistent.py
+-rw-rw-rw-  2.0 fat    93206 b- defN 24-Apr-03 06:08 tensordict/tensorclass.py
+-rw-rw-rw-  2.0 fat     1093 b- defN 24-Apr-03 06:08 tensordict/tensordict.py
+-rw-rw-rw-  2.0 fat    68097 b- defN 24-Apr-03 06:08 tensordict/utils.py
+-rw-rw-rw-  2.0 fat       81 b- defN 24-Apr-03 06:09 tensordict/version.py
+-rw-rw-rw-  2.0 fat     1634 b- defN 24-Apr-03 06:08 tensordict/nn/__init__.py
+-rw-rw-rw-  2.0 fat    54882 b- defN 24-Apr-03 06:08 tensordict/nn/common.py
+-rw-rw-rw-  2.0 fat     6020 b- defN 24-Apr-03 06:08 tensordict/nn/ensemble.py
+-rw-rw-rw-  2.0 fat    25890 b- defN 24-Apr-03 06:08 tensordict/nn/functional_modules.py
+-rw-rw-rw-  2.0 fat    36707 b- defN 24-Apr-03 06:08 tensordict/nn/params.py
+-rw-rw-rw-  2.0 fat    25553 b- defN 24-Apr-03 06:08 tensordict/nn/probabilistic.py
+-rw-rw-rw-  2.0 fat    19947 b- defN 24-Apr-03 06:08 tensordict/nn/sequence.py
+-rw-rw-rw-  2.0 fat    13231 b- defN 24-Apr-03 06:08 tensordict/nn/utils.py
+-rw-rw-rw-  2.0 fat      795 b- defN 24-Apr-03 06:08 tensordict/nn/distributions/__init__.py
+-rw-rw-rw-  2.0 fat     6629 b- defN 24-Apr-03 06:08 tensordict/nn/distributions/composite.py
+-rw-rw-rw-  2.0 fat     9924 b- defN 24-Apr-03 06:08 tensordict/nn/distributions/continuous.py
+-rw-rw-rw-  2.0 fat     2667 b- defN 24-Apr-03 06:08 tensordict/nn/distributions/discrete.py
+-rw-rw-rw-  2.0 fat     6694 b- defN 24-Apr-03 06:08 tensordict/nn/distributions/truncated_normal.py
+-rw-rw-rw-  2.0 fat     1266 b- defN 24-Apr-03 06:08 tensordict/nn/distributions/utils.py
+-rw-rw-rw-  2.0 fat      393 b- defN 24-Apr-03 06:08 tensordict/prototype/__init__.py
+-rw-rw-rw-  2.0 fat     7889 b- defN 24-Apr-03 06:08 tensordict/prototype/fx.py
+-rw-rw-rw-  2.0 fat      796 b- defN 24-Apr-03 06:08 tensordict/prototype/tensorclass.py
+-rw-rw-rw-  2.0 fat     1119 b- defN 24-Apr-03 06:10 tensordict-0.3.2.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    19303 b- defN 24-Apr-03 06:10 tensordict-0.3.2.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 24-Apr-03 06:10 tensordict-0.3.2.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       11 b- defN 24-Apr-03 06:10 tensordict-0.3.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     3200 b- defN 24-Apr-03 06:10 tensordict-0.3.2.dist-info/RECORD
+38 files, 1151315 bytes uncompressed, 275749 bytes compressed:  76.0%
```

## zipnote {}

```diff
@@ -93,23 +93,23 @@
 
 Filename: tensordict/prototype/fx.py
 Comment: 
 
 Filename: tensordict/prototype/tensorclass.py
 Comment: 
 
-Filename: tensordict-0.3.1.dist-info/LICENSE
+Filename: tensordict-0.3.2.dist-info/LICENSE
 Comment: 
 
-Filename: tensordict-0.3.1.dist-info/METADATA
+Filename: tensordict-0.3.2.dist-info/METADATA
 Comment: 
 
-Filename: tensordict-0.3.1.dist-info/WHEEL
+Filename: tensordict-0.3.2.dist-info/WHEEL
 Comment: 
 
-Filename: tensordict-0.3.1.dist-info/top_level.txt
+Filename: tensordict-0.3.2.dist-info/top_level.txt
 Comment: 
 
-Filename: tensordict-0.3.1.dist-info/RECORD
+Filename: tensordict-0.3.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tensordict/__init__.py

```diff
@@ -12,15 +12,15 @@
     merge_tensordicts,
     pad,
     pad_sequence,
 )
 from tensordict.memmap import MemoryMappedTensor
 from tensordict.memmap_deprec import is_memmap, MemmapTensor, set_transfer_ownership
 from tensordict.persistent import PersistentTensorDict
-from tensordict.tensorclass import NonTensorData, tensorclass
+from tensordict.tensorclass import NonTensorData, NonTensorStack, tensorclass
 from tensordict.utils import (
     assert_allclose_td,
     is_batchedtensor,
     is_tensorclass,
     lazy_legacy,
     NestedKey,
     set_lazy_legacy,
@@ -39,14 +39,15 @@
     "MemmapTensor",
     "SubTensorDict",
     "make_tensordict",
     "assert_allclose_td",
     "TensorDict",
     "TensorDictBase",
     "merge_tensordicts",
+    "NonTensorStack",
     "set_transfer_ownership",
     "pad_sequence",
     "is_memmap",
     "is_batchedtensor",
     "is_tensor_collection",
     "pad",
     "NestedKey",
```

## tensordict/_lazy.py

```diff
@@ -49,14 +49,15 @@
     cache,
     convert_ellipsis_to_idx,
     DeviceType,
     erase_cache,
     expand_right,
     IndexType,
     infer_size_impl,
+    is_non_tensor,
     is_tensorclass,
     KeyedJaggedTensor,
     lock_blocked,
     NestedKey,
 )
 from torch import Tensor
 
@@ -110,15 +111,18 @@
 def _fails_exclusive_keys(func):
     @wraps(func)
     def newfunc(self, *args, **kwargs):
         if self._has_exclusive_keys:
             raise RuntimeError(
                 f"the method {func.__name__} cannot complete when there are exclusive keys."
             )
-        return getattr(TensorDictBase, func.__name__)(self, *args, **kwargs)
+        parent_func = getattr(TensorDictBase, func.__name__, None)
+        if parent_func is None:
+            parent_func = getattr(TensorDict, func.__name__)
+        return parent_func(self, *args, **kwargs)
 
     return newfunc
 
 
 class LazyStackedTensorDict(TensorDictBase):
     """A Lazy stack of TensorDicts.
 
@@ -130,14 +134,16 @@
     Args:
          *tensordicts (TensorDict instances): a list of tensordict with
             same batch size.
          stack_dim (int): a dimension (between `-td.ndimension()` and
             `td.ndimension()-1` along which the stack should be performed.
          hook_out (callable, optional): a callable to execute after :meth:`~.get`.
          hook_in (callable, optional): a callable to execute before :meth:`~.set`.
+         stack_dim_name (str, optional): the name of the stack dimension.
+            Defaults to ``None``.
 
     Examples:
         >>> from tensordict import TensorDict
         >>> import torch
         >>> tds = [TensorDict({'a': torch.randn(3, 4)}, batch_size=[3])
         ...     for _ in range(10)]
         >>> td_stack = torch.stack(tds, -1)
@@ -180,14 +186,15 @@
     def __init__(
         self,
         *tensordicts: T,
         stack_dim: int = 0,
         hook_out: callable | None = None,
         hook_in: callable | None = None,
         batch_size: Sequence[int] | None = None,  # TODO: remove
+        stack_dim_name: str | None = None,
     ) -> None:
         self._is_locked = None
 
         # sanity check
         N = len(tensordicts)
         if not N:
             raise RuntimeError(
@@ -196,14 +203,18 @@
             )
         if stack_dim < 0:
             raise RuntimeError(
                 f"stack_dim must be non negative, got stack_dim={stack_dim}"
             )
         _batch_size = tensordicts[0].batch_size
         device = tensordicts[0].device
+        if stack_dim > len(_batch_size):
+            raise RuntimeError(
+                f"Stack dim {stack_dim} is too big for batch size {_batch_size}."
+            )
 
         for td in tensordicts[1:]:
             if not is_tensor_collection(td):
                 raise TypeError(
                     "Expected all inputs to be TensorDictBase instances but got "
                     f"{type(td)} instead."
                 )
@@ -220,14 +231,16 @@
         self.tensordicts: list[TensorDictBase] = list(tensordicts)
         self.stack_dim = stack_dim
         self._batch_size = self._compute_batch_size(_batch_size, stack_dim, N)
         self.hook_out = hook_out
         self.hook_in = hook_in
         if batch_size is not None and batch_size != self.batch_size:
             raise RuntimeError("batch_size does not match self.batch_size.")
+        if stack_dim_name is not None:
+            self._td_dim_name = stack_dim_name
 
     # These attributes should never be set
     @property
     def _is_shared(self):
         return all(td._is_shared for td in self.tensordicts)
 
     @property
@@ -411,14 +424,16 @@
     def _set_str(
         self,
         key: NestedKey,
         value: dict[str, CompatibleType] | CompatibleType,
         *,
         inplace: bool,
         validated: bool,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
     ) -> T:
         try:
             inplace = self._convert_inplace(inplace, key)
         except KeyError as e:
             raise KeyError(
                 "setting a value in-place on a stack of TensorDict is only "
                 "permitted if all members of the stack have this key in "
@@ -427,27 +442,41 @@
         if not validated:
             value = self._validate_value(value)
             validated = True
         if self._is_vmapped:
             value = self.hook_in(value)
         values = value.unbind(self.stack_dim)
         for tensordict, item in zip(self.tensordicts, values):
-            tensordict._set_str(key, item, inplace=inplace, validated=validated)
+            tensordict._set_str(
+                key,
+                item,
+                inplace=inplace,
+                validated=validated,
+                ignore_lock=ignore_lock,
+                non_blocking=non_blocking,
+            )
         return self
 
     def _set_tuple(
         self,
         key: NestedKey,
         value: dict[str, CompatibleType] | CompatibleType,
         *,
         inplace: bool,
         validated: bool,
+        non_blocking: bool = False,
     ) -> T:
         if len(key) == 1:
-            return self._set_str(key[0], value, inplace=inplace, validated=validated)
+            return self._set_str(
+                key[0],
+                value,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
+            )
         # if inplace is not False:  # inplace could be None
         #     # we don't want to end up in the situation where one tensordict has
         #     # inplace=True and another one inplace=False because inplace was loose.
         #     # Worse could be writing with inplace=True up until some level then to
         #     # realize the key is missing in one td, raising an exception and having
         #     # messed up the data. Hence we must start by checking if the key
         #     # is present.
@@ -462,15 +491,21 @@
         if not validated:
             value = self._validate_value(value)
             validated = True
         if self._is_vmapped:
             value = self.hook_in(value)
         values = value.unbind(self.stack_dim)
         for tensordict, item in zip(self.tensordicts, values):
-            tensordict._set_tuple(key, item, inplace=inplace, validated=validated)
+            tensordict._set_tuple(
+                key,
+                item,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
+            )
         return self
 
     def _split_index(self, index):
         """Given a tuple index, split it in as many indices as the number of tensordicts.
 
         Returns:
             a dictionary with {index-of-td: index-within-td}
@@ -483,17 +518,18 @@
         index = _broadcast_tensors(index)
         out = []
         num_single = 0
         num_none = 0
         isinteger = False
         is_nd_tensor = False
         cursor = 0  # the dimension cursor
-        selected_td_idx = range(len(self.tensordicts))
+        selected_td_idx = torch.arange(len(self.tensordicts))
         has_bool = False
         num_squash = 0
+        encountered_tensor = False
         for i, idx in enumerate(index):  # noqa: B007
             cursor_incr = 1
             if idx is None:
                 out.append(None)
                 num_none += cursor <= self.stack_dim
                 continue
             if cursor == self.stack_dim:
@@ -505,49 +541,48 @@
                         "using `tensordict.contiguous()`."
                     )
                 elif isinstance(idx, slice) or _is_number(idx):
                     selected_td_idx = range(len(self.tensordicts))[idx]
                     if not isinstance(selected_td_idx, range):
                         isinteger = True
                         selected_td_idx = [selected_td_idx]
-                elif isinstance(idx, (list, range)):
-                    selected_td_idx = idx
-                elif isinstance(idx, (torch.Tensor, np.ndarray)):
-                    if idx.dtype in (np.dtype("bool"), torch.bool):
+                elif isinstance(idx, torch.Tensor):
+                    if idx.dtype == torch.bool:
                         # we mark that we need to dispatch the indices across stack idx
                         has_bool = True
                         # split mask along dim
                         individual_masks = idx = idx.unbind(0)
                         selected_td_idx = range(len(self.tensordicts))
                         out.append(idx)
                         split_dim = self.stack_dim - num_single
                         mask_loc = i
                     else:
-                        if isinstance(idx, np.ndarray):
-                            idx = torch.tensor(idx)
                         is_nd_tensor = True
-                        selected_td_idx = range(len(idx))
-                        out.append(idx.unbind(0))
+                        if not encountered_tensor:
+                            # num_single -= idx.ndim - 1
+                            encountered_tensor = True
+                        else:
+                            num_single += 1
+                        selected_td_idx = idx
+                        # out.append(idx.unbind(0))
                 else:
                     raise TypeError(f"Invalid index type: {type(idx)}.")
             else:
                 if _is_number(idx) and cursor < self.stack_dim:
                     num_single += 1
                 if _is_number(idx) or isinstance(
                     idx,
                     (
                         ftdim.Dim,
                         slice,
-                        list,
-                        range,
                     ),
                 ):
                     out.append(idx)
-                elif isinstance(idx, (np.ndarray, torch.Tensor)):
-                    if idx.dtype in (np.dtype("bool"), torch.bool):
+                elif isinstance(idx, torch.Tensor):
+                    if idx.dtype == torch.bool:
                         cursor_incr = idx.ndim
                         if cursor < self.stack_dim:
                             num_squash += cursor_incr - 1
                         if (
                             cursor < self.stack_dim
                             and cursor + cursor_incr > self.stack_dim
                         ):
@@ -564,15 +599,19 @@
                         # a dimension. We play with num_single, reducing it
                         # by the number of dims of idx: if idx has 3 dims, our
                         # indexed tensor will have 2 more dimensions, going in
                         # the opposite direction of indexing with a single integer,
                         # smth[torch.tensor(1)].ndim = smth.ndim-1
                         # smth[torch.tensor([1])].ndim = smth.ndim
                         # smth[torch.tensor([[1]])].ndim = smth.ndim+1
-                        num_single -= idx.ndim - 1
+                        if not encountered_tensor:
+                            num_single -= idx.ndim - 1
+                            encountered_tensor = True
+                        else:
+                            num_single += 1
                     out.append(idx)
                 else:
                     raise TypeError(f"Invalid index type: {type(idx)}.")
             cursor += cursor_incr
         if has_bool:
             out = tuple(
                 tuple(idx if not isinstance(idx, tuple) else idx[i] for idx in out)
@@ -589,28 +628,53 @@
                 "is_nd_tensor": is_nd_tensor,
                 "num_none": num_none,
                 "num_squash": num_squash,
             }
         elif is_nd_tensor:
 
             def isindexable(idx):
-                if isinstance(idx, (torch.Tensor, np.ndarray)):
-                    if idx.dtype in (torch.bool, np.dtype("bool")):
+                if isinstance(idx, torch.Tensor):
+                    if idx.dtype == torch.bool:
                         return False
                     return True
                 if isinstance(idx, (tuple, list, range)):
                     return True
                 return False
 
-            out = tuple(
-                tuple(idx if not isindexable(idx) else idx[i] for idx in out)
-                for i in selected_td_idx
-            )
+            def outer_list(tensor_index, tuple_index):
+                """Converts a tensor and a tuple to a nested list where each leaf is a (int, index) tuple where the index only points to one element."""
+                if isinstance(tensor_index, torch.Tensor):
+                    list_index = tensor_index.tolist()
+                else:
+                    list_index = tensor_index
+                list_result = []
+
+                def index_tuple_index(i, convert=False):
+                    for idx in tuple_index:
+                        if isindexable(idx):
+                            if convert:
+                                yield int(idx[i])
+                            else:
+                                yield idx[i]
+                        else:
+                            yield idx
+
+                for i, idx in enumerate(list_index):
+                    if isinstance(idx, int):
+                        list_result.append(
+                            (idx, tuple(index_tuple_index(i, convert=True)))
+                        )
+                    elif isinstance(idx, list):
+                        list_result.append(outer_list(idx, tuple(index_tuple_index(i))))
+                    else:
+                        raise NotImplementedError
+                return list_result
+
             return {
-                "index_dict": dict(enumerate(out)),
+                "index_dict": outer_list(selected_td_idx, out),
                 "num_single": num_single,
                 "isinteger": isinteger,
                 "has_bool": has_bool,
                 "is_nd_tensor": is_nd_tensor,
                 "num_none": num_none,
                 "num_squash": num_squash,
             }
@@ -620,15 +684,15 @@
             "isinteger": isinteger,
             "has_bool": has_bool,
             "is_nd_tensor": is_nd_tensor,
             "num_none": num_none,
             "num_squash": num_squash,
         }
 
-    def _set_at_str(self, key, value, index, *, validated):
+    def _set_at_str(self, key, value, index, *, validated, non_blocking: bool):
         if not validated:
             value = self._validate_value(value, check_shape=False)
             validated = True
         if self._is_vmapped:
             value = self.hook_in(value)
         split_index = self._split_index(index)
         converted_idx = split_index["index_dict"]
@@ -637,30 +701,49 @@
         has_bool = split_index["has_bool"]
         num_squash = split_index.get("num_squash", 0)
         num_none = split_index.get("num_none", 0)
         is_nd_tensor = split_index.get("is_nd_tensor", False)
         if isinteger:
             # this will break if the index along the stack dim is [0] or :1 or smth
             for i, _idx in converted_idx.items():
-                self.tensordicts[i]._set_at_str(key, value, _idx, validated=validated)
+                self.tensordicts[i]._set_at_str(
+                    key, value, _idx, validated=validated, non_blocking=non_blocking
+                )
             return self
         if is_nd_tensor:
             unbind_dim = self.stack_dim - num_single + num_none - num_squash
             value_unbind = value.unbind(unbind_dim)
-            for idx, _value in zip(converted_idx.values(), value_unbind):
-                self._set_at_str(key, _value, idx, validated=validated)
+
+            def set_at_str(converted_idx):
+                for i, item in enumerate(converted_idx):
+                    if isinstance(item, list):
+                        set_at_str(item)
+                    else:
+                        _value = value_unbind[i]
+                        stack_idx, idx = item
+                        self.tensordicts[stack_idx]._set_at_str(
+                            key,
+                            _value,
+                            idx,
+                            validated=validated,
+                            non_blocking=non_blocking,
+                        )
+
+            set_at_str(converted_idx)
             return self
         elif not has_bool:
             unbind_dim = self.stack_dim - num_single + num_none - num_squash
             value_unbind = value.unbind(unbind_dim)
             for (i, _idx), _value in zip(
                 converted_idx.items(),
                 value_unbind,
             ):
-                self.tensordicts[i]._set_at_str(key, _value, _idx, validated=validated)
+                self.tensordicts[i]._set_at_str(
+                    key, _value, _idx, validated=validated, non_blocking=non_blocking
+                )
         else:
             # we must split, not unbind
             mask_unbind = split_index["individual_masks"]
             split_dim = split_index["split_dim"]
             splits = [_mask_unbind.sum().item() for _mask_unbind in mask_unbind]
             value_unbind = value.split(splits, split_dim)
             if mask_unbind[0].ndim == 0:
@@ -668,24 +751,36 @@
                 for (i, _idx), mask, _value in zip(
                     converted_idx.items(),
                     mask_unbind,
                     value_unbind,
                 ):
                     if mask.any():
                         self.tensordicts[i]._set_at_str(
-                            key, _value, _idx, validated=validated
+                            key,
+                            _value,
+                            _idx,
+                            validated=validated,
+                            non_blocking=non_blocking,
                         )
             else:
                 for (i, _idx), _value in zip(converted_idx.items(), value_unbind):
                     self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
-                    self[self_idx]._set_at_str(key, _value, _idx, validated=validated)
+                    self[self_idx]._set_at_str(
+                        key,
+                        _value,
+                        _idx,
+                        validated=validated,
+                        non_blocking=non_blocking,
+                    )
 
-    def _set_at_tuple(self, key, value, idx, *, validated):
+    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
         if len(key) == 1:
-            return self._set_at_str(key[0], value, idx, validated=validated)
+            return self._set_at_str(
+                key[0], value, idx, validated=validated, non_blocking=non_blocking
+            )
         # get the "last" tds
         tds = []
         for td in self.tensordicts:
             tds.append(td.get(key[:-1]))
         # build only a single lazy stack from it
         # (if the stack is a stack of stacks this won't be awesomely efficient
         # but then we'd need to splut the value (which we can do) and recompute
@@ -696,15 +791,15 @@
         if not validated:
             value = self._validate_value(value, check_shape=False)
             validated = True
         if self._is_vmapped:
             value = self.hook_in(value)
         item = td._get_str(key, NO_DEFAULT)
         item[idx] = value
-        td._set_str(key, item, inplace=True, validated=True)
+        td._set_str(key, item, inplace=True, validated=True, non_blocking=non_blocking)
         return self
 
     def _legacy_unsqueeze(self, dim: int) -> T:
         if dim < 0:
             dim = self.batch_dims + dim + 1
 
         if (dim > self.batch_dims) or (dim < 0):
@@ -714,15 +809,15 @@
                 f"dim={dim} with a batch size of {self.batch_size}."
             )
         if dim <= self.stack_dim:
             stack_dim = self.stack_dim + 1
         else:
             dim = dim - 1
             stack_dim = self.stack_dim
-        return LazyStackedTensorDict(
+        return type(self)(
             *(tensordict.unsqueeze(dim) for tensordict in self.tensordicts),
             stack_dim=stack_dim,
         )
 
     def _legacy_squeeze(self, dim: int | None = None) -> T:
         """Squeezes all tensors for a dimension comprised in between `-td.batch_dims+1` and `td.batch_dims-1` and returns them in a new tensordict.
 
@@ -752,15 +847,15 @@
         if dim == self.stack_dim:
             return self.tensordicts[0]
         elif dim < self.stack_dim:
             stack_dim = self.stack_dim - 1
         else:
             dim = dim - 1
             stack_dim = self.stack_dim
-        return LazyStackedTensorDict(
+        return type(self)(
             *(tensordict.squeeze(dim) for tensordict in self.tensordicts),
             stack_dim=stack_dim,
         )
 
     def _unbind(self, dim: int) -> tuple[TensorDictBase, ...]:
         if dim == self.stack_dim:
             return tuple(self.tensordicts)
@@ -769,15 +864,14 @@
             out = []
             new_dim = dim if dim < self.stack_dim else dim - 1
             new_stack_dim = (
                 self.stack_dim if dim > self.stack_dim else self.stack_dim - 1
             )
             for td in self.tensordicts:
                 out.append(td._unbind(new_dim))
-
             return tuple(self.lazy_stack(vals, new_stack_dim) for vals in zip(*out))
 
     def _stack_onto_(
         self,
         list_item: list[CompatibleType],
         dim: int,
     ) -> T:
@@ -806,40 +900,38 @@
                     default, (MemmapTensor, KeyedJaggedTensor, torch.Tensor)
                 )
                 and not is_tensor_collection(default)
             ):
                 # then we consider this default as non-stackable and return prematurly
                 return default
         try:
-            out = self.lazy_stack(tensors, self.stack_dim)
+            out = self.lazy_stack(
+                tensors, self.stack_dim, stack_dim_name=self._td_dim_name
+            )
             if _is_tensor_collection(out.__class__):
                 if isinstance(out, LazyStackedTensorDict):
                     # then it's a LazyStackedTD
                     out.hook_out = self.hook_out
                     out.hook_in = self.hook_in
                     out._is_vmapped = self._is_vmapped
                     incr = 0 if not self._is_vmapped else 1
                     out._batch_size = (
                         self._batch_size
                         + out.batch_size[(len(self._batch_size) + incr) :]
                     )
-                    if self._td_dim_name is not None:
-                        out._td_dim_name = self._td_dim_name
                 elif is_tensorclass(out):
                     # then it's a tensorclass
                     out._tensordict.hook_out = self.hook_out
                     out._tensordict.hook_in = self.hook_in
                     out._tensordict._is_vmapped = self._is_vmapped
                     incr = 0 if not self._is_vmapped else 1
                     out._tensordict._batch_size = (
                         self._batch_size
                         + out._tensordict.batch_size[(len(self._batch_size) + incr) :]
                     )
-                    if self._td_dim_name is not None:
-                        out._tensordict._td_dim_name = self._td_dim_name
                 else:
                     raise RuntimeError
             elif self.hook_out is not None:
                 out = self.hook_out(out)
             return out
         except RuntimeError as err:
             if "stack expects each tensor to be equal size" in str(err):
@@ -876,33 +968,41 @@
                 )
 
     @classmethod
     def lazy_stack(
         cls,
         items: Sequence[TensorDictBase],
         dim: int = 0,
+        *,
         device: DeviceType | None = None,
         out: T | None = None,
+        stack_dim_name: str | None = None,
     ) -> T:
         """Stacks tensordicts in a LazyStackedTensorDict."""
         if not items:
             raise RuntimeError("items cannot be empty")
 
-        from .tensorclass import NonTensorData
-
         if all(isinstance(item, torch.Tensor) for item in items):
             return torch.stack(items, dim=dim, out=out)
+        if all(is_non_tensor(tensordict) for tensordict in items):
+            # Non-tensor data (Data or Stack) are stacked using NonTensorStack
+            # If the content is identical (not equal but same id) this does not
+            # require additional memory.
+            from .tensorclass import NonTensorStack
+
+            return NonTensorStack(*items, stack_dim=dim)
         if all(
             is_tensorclass(item) and type(item) == type(items[0])  # noqa: E721
             for item in items
         ):
-            if all(isinstance(tensordict, NonTensorData) for tensordict in items):
-                return NonTensorData._stack_non_tensor(items, dim=dim)
             lazy_stack = cls.lazy_stack(
-                [item._tensordict for item in items], dim=dim, out=out
+                [item._tensordict for item in items],
+                dim=dim,
+                out=out,
+                stack_dim_name=stack_dim_name,
             )
             # we take the first non_tensordict by convention
             return type(items[0])._from_tensordict(
                 tensordict=lazy_stack, non_tensordict=items[0]._non_tensordict
             )
 
         batch_size = items[0].batch_size
@@ -919,15 +1019,17 @@
 
         if out is None:
             # We need to handle tensordicts with exclusive keys and tensordicts with
             # mismatching shapes.
             # The first case is handled within _check_keys which fails if keys
             # don't match exactly.
             # The second requires a check over the tensor shapes.
-            return LazyStackedTensorDict(*items, stack_dim=dim)
+            return LazyStackedTensorDict(
+                *items, stack_dim=dim, stack_dim_name=stack_dim_name
+            )
         else:
             batch_size = list(batch_size)
             batch_size.insert(dim, len(items))
             batch_size = torch.Size(batch_size)
 
             if out.batch_size != batch_size:
                 raise RuntimeError(
@@ -1232,34 +1334,34 @@
             device=device,
             names=self.names,
             _run_checks=False,
         )
         return out
 
     def empty(self, recurse=False) -> T:
-        return LazyStackedTensorDict(
+        return type(self)(
             *[td.empty(recurse=recurse) for td in self.tensordicts],
             stack_dim=self.stack_dim,
         )
 
     def _clone(self, recurse: bool = True) -> T:
         if recurse:
             # This could be optimized using copy but we must be careful with
             # metadata (_is_shared etc)
-            result = LazyStackedTensorDict(
+            result = type(self)(
                 *[td._clone() for td in self.tensordicts],
                 stack_dim=self.stack_dim,
+                stack_dim_name=self._td_dim_name,
             )
         else:
-            result = LazyStackedTensorDict(
+            result = type(self)(
                 *[td._clone(recurse=False) for td in self.tensordicts],
                 stack_dim=self.stack_dim,
+                stack_dim_name=self._td_dim_name,
             )
-        if self._td_dim_name is not None:
-            result._td_dim_name = self._td_dim_name
         return result
 
     def pin_memory(self) -> T:
         for td in self.tensordicts:
             td.pin_memory()
         return self
 
@@ -1270,15 +1372,15 @@
         if batch_size is not None:
             raise TypeError("Cannot pass batch-size to a LazyStackedTensorDict.")
         result = self
 
         if device is not None and dtype is None and device == self.device:
             return result
 
-        return LazyStackedTensorDict(
+        return type(self)(
             *[td.to(*args, **kwargs) for td in self.tensordicts],
             stack_dim=self.stack_dim,
             hook_out=self.hook_out,
             hook_in=self.hook_in,
         )
 
     def _check_new_batch_size(self, new_size: torch.Size) -> None:
@@ -1350,16 +1452,18 @@
         inplace: bool = False,
         checked: bool = False,
         call_on_nested: bool = False,
         default: Any = NO_DEFAULT,
         named: bool = False,
         nested_keys: bool = False,
         prefix: tuple = (),
+        filter_empty: bool | None = None,
+        is_leaf: Callable | None = None,
         **constructor_kwargs,
-    ) -> T:
+    ) -> T | None:
         if inplace and any(
             arg for arg in (batch_size, device, names, constructor_kwargs)
         ):
             raise ValueError(
                 "Cannot pass other arguments to LazyStackedTensorDict.apply when inplace=True."
             )
         if batch_size is not None:
@@ -1374,44 +1478,49 @@
                 checked=checked,
                 call_on_nested=call_on_nested,
                 default=default,
                 named=named,
                 nested_keys=nested_keys,
                 prefix=prefix,
                 inplace=inplace,
+                filter_empty=filter_empty,
+                is_leaf=is_leaf,
                 **constructor_kwargs,
             )
 
         others = (other.unbind(self.stack_dim) for other in others)
         results = [
             td._apply_nest(
                 fn,
                 *oth,
                 checked=checked,
                 device=device,
                 call_on_nested=call_on_nested,
                 default=default,
                 named=named,
                 nested_keys=nested_keys,
-                prefix=prefix + (i,),
+                prefix=prefix,  # + (i,),
                 inplace=inplace,
+                filter_empty=filter_empty,
+                is_leaf=is_leaf,
             )
             for i, (td, *oth) in enumerate(zip(self.tensordicts, *others))
         ]
+        if filter_empty and all(r is None for r in results):
+            return
         if not inplace:
-            out = LazyStackedTensorDict(
+            out = type(self)(
                 *results,
                 stack_dim=self.stack_dim,
+                stack_dim_name=self._td_dim_name,
             )
         else:
             out = self
         if names is not None:
             out.names = names
-        else:
-            out._td_dim_name = self._td_dim_name
         return out
 
     def _select(
         self,
         *keys: NestedKey,
         inplace: bool = False,
         strict: bool = False,
@@ -1420,59 +1529,62 @@
         # the following implementation keeps the hidden keys in the tensordicts
         tensordicts = [
             td._select(*keys, inplace=inplace, strict=strict, set_shared=set_shared)
             for td in self.tensordicts
         ]
         if inplace:
             return self
-        result = LazyStackedTensorDict(*tensordicts, stack_dim=self.stack_dim)
+        result = type(self)(*tensordicts, stack_dim=self.stack_dim)
         return result
 
     def _exclude(
         self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
     ) -> LazyStackedTensorDict:
         tensordicts = [
             tensordict._exclude(*keys, inplace=inplace, set_shared=set_shared)
             for tensordict in self.tensordicts
         ]
         if inplace:
             self.tensordicts = tensordicts
             return self
-        result = LazyStackedTensorDict(*tensordicts, stack_dim=self.stack_dim)
+        result = type(self)(*tensordicts, stack_dim=self.stack_dim)
         return result
 
     def __setitem__(self, index: IndexType, value: T) -> T:
         if isinstance(index, (tuple, str)):
             # try:
             index_unravel = _unravel_key_to_tuple(index)
             if index_unravel:
                 self._set_tuple(
                     index_unravel,
                     value,
                     inplace=BEST_ATTEMPT_INPLACE
                     if isinstance(self, _SubTensorDict)
                     else False,
                     validated=False,
+                    non_blocking=False,
                 )
                 return
 
-            if any(isinstance(sub_index, (list, range)) for sub_index in index):
+            if any(
+                isinstance(sub_index, (list, range, np.ndarray)) for sub_index in index
+            ):
                 index = tuple(
-                    torch.tensor(sub_index, device=self.device)
-                    if isinstance(sub_index, (list, range))
+                    torch.as_tensor(sub_index, device=self.device)
+                    if isinstance(sub_index, (list, range, np.ndarray))
                     else sub_index
                     for sub_index in index
                 )
 
         if index is Ellipsis or (isinstance(index, tuple) and Ellipsis in index):
             index = convert_ellipsis_to_idx(index, self.batch_size)
         elif isinstance(index, (list, range)):
-            index = torch.tensor(index, device=self.device)
+            index = torch.as_tensor(index, device=self.device)
 
-        if isinstance(value, (TensorDictBase, dict)):
+        if is_tensor_collection(value) or isinstance(value, dict):
             indexed_bs = _getitem_batch_size(self.batch_size, index)
             if isinstance(value, dict):
                 value = TensorDict(
                     value, batch_size=indexed_bs, device=self.device, _run_checks=False
                 )
             if value.batch_size != indexed_bs:
                 # try to expand
@@ -1491,29 +1603,45 @@
             has_bool = split_index["has_bool"]
             num_squash = split_index.get("num_squash", 0)
             num_none = split_index.get("num_none", 0)
             is_nd_tensor = split_index.get("is_nd_tensor", False)
             if isinteger:
                 # this will break if the index along the stack dim is [0] or :1 or smth
                 for i, _idx in converted_idx.items():
-                    self.tensordicts[i][_idx] = value
+                    if _idx == ():
+                        self.tensordicts[i].update(value, inplace=True)
+                    else:
+                        self.tensordicts[i][_idx] = value
                 return self
             if is_nd_tensor:
-                raise RuntimeError(
-                    "Indexing along stack dim with a non-boolean tensor is not supported yet. "
-                    "Use SubTensorDict instead."
-                )
+                unbind_dim = self.stack_dim - num_single + num_none - num_squash
+
+                # converted_idx is a nested list with (int, index) items
+                def assign(converted_idx, value=value):
+                    value = value.unbind(unbind_dim)
+                    for i, item in enumerate(converted_idx):
+                        if isinstance(item, list):
+                            assign(item)
+                        else:
+                            stack_item, idx = item
+                            self.tensordicts[stack_item][idx] = value[i]
+
+                assign(converted_idx)
+                return self
             if not has_bool:
                 unbind_dim = self.stack_dim - num_single + num_none - num_squash
                 value_unbind = value.unbind(unbind_dim)
                 for (i, _idx), _value in zip(
                     converted_idx.items(),
                     value_unbind,
                 ):
-                    self.tensordicts[i][_idx] = _value
+                    if _idx == ():
+                        self.tensordicts[i].update(_value, inplace=True)
+                    else:
+                        self.tensordicts[i][_idx] = _value
             else:
                 # we must split, not unbind
                 mask_unbind = split_index["individual_masks"]
                 split_dim = split_index["split_dim"]
                 splits = [_mask_unbind.sum().item() for _mask_unbind in mask_unbind]
                 value_unbind = value.split(splits, split_dim)
                 if mask_unbind[0].ndim == 0:
@@ -1538,20 +1666,21 @@
             return any(item is td for td in self.tensordicts)
         return super().__contains__(item)
 
     def __getitem__(self, index: IndexType) -> T:
         if isinstance(index, (tuple, str)):
             index_key = _unravel_key_to_tuple(index)
             if index_key:
-                result = self._get_tuple(index_key, NO_DEFAULT)
-                from .tensorclass import NonTensorData
-
-                if isinstance(result, NonTensorData):
-                    return result.data
-                return result
+                leaf = self._get_tuple(index_key, NO_DEFAULT)
+                if is_non_tensor(leaf):
+                    result = getattr(leaf, "data", NO_DEFAULT)
+                    if result is NO_DEFAULT:
+                        return leaf.tolist()
+                    return result
+                return leaf
         split_index = self._split_index(index)
         converted_idx = split_index["index_dict"]
         isinteger = split_index["isinteger"]
         has_bool = split_index["has_bool"]
         is_nd_tensor = split_index["is_nd_tensor"]
         num_single = split_index.get("num_single", 0)
         num_none = split_index.get("num_none", 0)
@@ -1573,19 +1702,34 @@
             else:
                 for i, _idx in converted_idx.items():
                     self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
                     result.append(self[self_idx][_idx])
                 return torch.cat(result, cat_dim)
         elif is_nd_tensor:
             new_stack_dim = self.stack_dim - num_single + num_none
-            out = LazyStackedTensorDict.lazy_stack(
-                [self[idx] for idx in converted_idx.values()], new_stack_dim
-            )
-            out._td_dim_name = self._td_dim_name
-            return out
+
+            def recompose(converted_idx, stack_dim=new_stack_dim):
+                stack = []
+                for item in converted_idx:
+                    if isinstance(item, list):
+                        stack.append(recompose(item, stack_dim=stack_dim))
+                    else:
+                        stack_elt, idx = item
+                        if idx != ():
+                            stack.append(self.tensordicts[stack_elt][idx])
+                        else:
+                            stack.append(self.tensordicts[stack_elt])
+
+                # TODO: this produces multiple dims with the same name
+                result = LazyStackedTensorDict.lazy_stack(
+                    stack, stack_dim, stack_dim_name=self._td_dim_name
+                )
+                return result
+
+            return recompose(converted_idx)
         else:
             if isinteger:
                 for (
                     i,
                     _idx,
                 ) in (
                     converted_idx.items()
@@ -1594,17 +1738,21 @@
                     if _idx is not None and _idx != ():
                         result = result[_idx]
                     return result
             else:
                 result = []
                 new_stack_dim = self.stack_dim - num_single + num_none - num_squash
                 for i, _idx in converted_idx.items():
-                    result.append(self.tensordicts[i][_idx])
-                result = LazyStackedTensorDict.lazy_stack(result, new_stack_dim)
-                result._td_dim_name = self._td_dim_name
+                    if _idx == ():
+                        result.append(self.tensordicts[i])
+                    else:
+                        result.append(self.tensordicts[i][_idx])
+                result = LazyStackedTensorDict.lazy_stack(
+                    result, new_stack_dim, stack_dim_name=self._td_dim_name
+                )
                 return result
 
     def __eq__(self, other):
         if is_tensorclass(other):
             return other == self
         if isinstance(other, (dict,)):
             # we may want to broadcast it instead
@@ -1902,20 +2050,22 @@
     def detach_(self) -> T:
         for td in self.tensordicts:
             td.detach_()
         return self
 
     def _memmap_(
         self,
+        *,
         prefix: str | None = None,
         copy_existing: bool = False,
         executor=None,
         futures=None,
         inplace=True,
         like=False,
+        share_non_tensor,
     ) -> T:
         if prefix is not None:
             prefix = Path(prefix)
 
             def save_metadata(prefix=prefix, self=self):
                 prefix = Path(prefix)
                 if not prefix.exists():
@@ -1936,14 +2086,15 @@
                 td._memmap_(
                     prefix=(prefix / str(i)) if prefix is not None else None,
                     copy_existing=copy_existing,
                     executor=executor,
                     futures=futures,
                     inplace=inplace,
                     like=like,
+                    share_non_tensor=share_non_tensor,
                 )
             )
         if not inplace:
             results = LazyStackedTensorDict.lazy_stack(results, dim=self.stack_dim)
         else:
             results = self
         results._device = torch.device("cpu")
@@ -1952,15 +2103,14 @@
     @classmethod
     def _load_memmap(cls, prefix: str, metadata: dict) -> LazyStackedTensorDict:
         tensordicts = []
         i = 0
         while (prefix / str(i)).exists():
             tensordicts.append(TensorDict.load_memmap(prefix / str(i)))
             i += 1
-
         return cls(*tensordicts, stack_dim=metadata["stack_dim"])
 
     def expand(self, *args: int, inplace: bool = False) -> T:
         if len(args) == 1 and isinstance(args[0], Sequence):
             shape = tuple(args[0])
         else:
             shape = args
@@ -1975,14 +2125,15 @@
 
     def update(
         self,
         input_dict_or_td: T,
         clone: bool = False,
         *,
         keys_to_update: Sequence[NestedKey] | None = None,
+        non_blocking: bool = False,
         **kwargs: Any,
     ) -> T:
         # This implementation of update is compatible with exclusive keys
         # as well as vmapped lazy stacks.
         # We iterate over the tensordicts rather than iterating over the keys,
         # which requires stacking and unbinding but is also not robust to missing keys.
         if input_dict_or_td is self:
@@ -2006,15 +2157,19 @@
                 raise ValueError(
                     "cannot update stacked tensordicts with different shapes."
                 )
             for td_dest, td_source in zip(
                 self.tensordicts, input_dict_or_td.tensordicts
             ):
                 td_dest.update(
-                    td_source, clone=clone, keys_to_update=keys_to_update, **kwargs
+                    td_source,
+                    clone=clone,
+                    keys_to_update=keys_to_update,
+                    non_blocking=non_blocking,
+                    **kwargs,
                 )
             return self
 
         if self.hook_in is not None:
             self_upd = self.hook_in(self)
             input_dict_or_td = self.hook_in(input_dict_or_td)
         else:
@@ -2047,14 +2202,16 @@
             self_upd = self
         return self_upd
 
     def update_(
         self,
         input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
         clone: bool = False,
+        *,
+        non_blocking: bool = False,
         **kwargs: Any,
     ) -> T:
         if input_dict_or_td is self:
             # no op
             return self
         if not is_tensor_collection(input_dict_or_td):
             input_dict_or_td = TensorDict.from_dict(
@@ -2065,47 +2222,51 @@
                     f"Built tensordict with ndim={input_dict_or_td.ndim} does not have enough dims."
                 )
         if input_dict_or_td.batch_size[self.stack_dim] != len(self.tensordicts):
             raise ValueError("cannot update stacked tensordicts with different shapes.")
         for td_dest, td_source in zip(
             self.tensordicts, input_dict_or_td.unbind(self.stack_dim)
         ):
-            td_dest.update_(td_source, clone=clone, **kwargs)
+            td_dest.update_(td_source, clone=clone, non_blocking=non_blocking, **kwargs)
         return self
 
     def update_at_(
         self,
         input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
         index: IndexType,
         clone: bool = False,
+        *,
+        non_blocking: bool = False,
     ) -> T:
-        if not isinstance(input_dict_or_td, TensorDictBase):
+        if not _is_tensor_collection(type(input_dict_or_td)):
             input_dict_or_td = TensorDict.from_dict(
                 input_dict_or_td, batch_size=self.batch_size
             )
         split_index = self._split_index(index)
         converted_idx = split_index["index_dict"]
         num_single = split_index["num_single"]
         isinteger = split_index["isinteger"]
         if isinteger:
             # this will break if the index along the stack dim is [0] or :1 or smth
             for i, _idx in converted_idx.items():
                 self.tensordicts[i].update_at_(
                     input_dict_or_td,
                     _idx,
+                    non_blocking=non_blocking,
                 )
             return self
         unbind_dim = self.stack_dim - num_single
         for (i, _idx), _value in zip(
             converted_idx.items(),
             input_dict_or_td.unbind(unbind_dim),
         ):
             self.tensordicts[i].update_at_(
                 _value,
                 _idx,
+                non_blocking=non_blocking,
             )
         return self
 
     def rename_key_(
         self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
     ) -> T:
         for td in self.tensordicts:
@@ -2320,38 +2481,38 @@
         if self._is_vmapped:
             raise RuntimeError("cannot call transpose within vmap.")
         if dim0 == self.stack_dim:
             # we know dim0 and dim1 are sorted so dim1 comes after dim0
             # example: shape = [5, 4, 3, 2, 1], stack_dim=1, dim0=1, dim1=4
             # resulting shape: [5, 1, 3, 2, 4]
             if dim1 == dim0 + 1:
-                result = LazyStackedTensorDict(*self.tensordicts, stack_dim=dim1)
+                result = type(self)(*self.tensordicts, stack_dim=dim1)
             else:
-                result = LazyStackedTensorDict(
+                result = type(self)(
                     *(td.transpose(dim0, dim1 - 1) for td in self.tensordicts),
                     stack_dim=dim1,
                 )
         elif dim1 == self.stack_dim:
             # example: shape = [5, 4, 3, 2, 1], stack_dim=3, dim0=1, dim1=3
             # resulting shape: [5, 2, 3, 4, 1]
             if dim0 + 1 == dim1:
-                result = LazyStackedTensorDict(*self.tensordicts, stack_dim=dim0)
+                result = type(self)(*self.tensordicts, stack_dim=dim0)
             else:
-                result = LazyStackedTensorDict(
+                result = type(self)(
                     *(td.transpose(dim0 + 1, dim1) for td in self.tensordicts),
                     stack_dim=dim0,
                 )
         else:
             dim0 = dim0 if dim0 < self.stack_dim else dim0 - 1
             dim1 = dim1 if dim1 < self.stack_dim else dim1 - 1
-            result = LazyStackedTensorDict(
+            result = type(self)(
                 *(td.transpose(dim0, dim1) for td in self.tensordicts),
                 stack_dim=self.stack_dim,
+                stack_dim_name=self._td_dim_name,
             )
-        result._td_dim_name = self._td_dim_name
         return result
 
     def _permute(
         self,
         *args,
         **kwargs,
     ):
@@ -2361,17 +2522,18 @@
         # find the new stack dim
         stack_dim = dims_list_sort[self.stack_dim]
         # remove that dim from the dims_list
         dims_list = [
             d if d < self.stack_dim else d - 1 for d in dims_list if d != self.stack_dim
         ]
         result = LazyStackedTensorDict.lazy_stack(
-            [td.permute(dims_list) for td in self.tensordicts], stack_dim
+            [td.permute(dims_list) for td in self.tensordicts],
+            stack_dim,
+            stack_dim_name=self._td_dim_name,
         )
-        result._td_dim_name = self._td_dim_name
         return result
 
     def _squeeze(self, dim=None):
         if dim is not None:
             new_dim = dim
             if new_dim < 0:
                 new_dim = self.batch_dims + new_dim
@@ -2386,17 +2548,18 @@
                 return self.tensordicts[0]
             if dim > self.stack_dim:
                 dim = dim - 1
                 stack_dim = self.stack_dim
             else:
                 stack_dim = self.stack_dim - 1
             result = LazyStackedTensorDict.lazy_stack(
-                [td.squeeze(dim) for td in self.tensordicts], stack_dim
+                [td.squeeze(dim) for td in self.tensordicts],
+                stack_dim,
+                stack_dim_name=self._td_dim_name,
             )
-            result._td_dim_name = result._td_dim_name
         else:
             result = self
             for dim in range(self.batch_dims - 1, -1, -1):
                 if self.batch_size[dim] == 1:
                     result = result.squeeze(dim)
         return result
 
@@ -2411,17 +2574,18 @@
         dim = new_dim
         if dim > self.stack_dim:
             dim = dim - 1
             stack_dim = self.stack_dim
         else:
             stack_dim = self.stack_dim + 1
         result = LazyStackedTensorDict.lazy_stack(
-            [td.unsqueeze(dim) for td in self.tensordicts], stack_dim
+            [td.unsqueeze(dim) for td in self.tensordicts],
+            stack_dim,
+            stack_dim_name=self._td_dim_name,
         )
-        result._td_dim_name = result._td_dim_name
         return result
 
     lock_ = TensorDictBase.lock_
     lock = _renamed_inplace_method(lock_)
 
     unlock_ = TensorDictBase.unlock_
     unlock = _renamed_inplace_method(unlock_)
@@ -2430,14 +2594,15 @@
     _check_is_shared = TensorDict._check_is_shared
     _convert_to_tensordict = TensorDict._convert_to_tensordict
     _index_tensordict = TensorDict._index_tensordict
     masked_select = TensorDict.masked_select
     reshape = TensorDict.reshape
     split = TensorDict.split
     _to_module = TensorDict._to_module
+    from_dict_instance = TensorDict.from_dict_instance
 
 
 class _CustomOpTensorDict(TensorDictBase):
     """Encodes lazy operations on tensors contained in a TensorDict."""
 
     _safe = False
     _lazy = True
@@ -2575,53 +2740,83 @@
         if tensor is default:
             return tensor
         return self._transform_value(tensor)
 
     def _transform_value(self, item):
         return getattr(item, self.custom_op)(**self._update_custom_op_kwargs(item))
 
-    def _set_str(self, key, value, *, inplace: bool, validated: bool):
+    def _set_str(
+        self,
+        key,
+        value,
+        *,
+        inplace: bool,
+        validated: bool,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
+    ):
         if not validated:
             value = self._validate_value(value, check_shape=True)
             validated = True
         value = getattr(value, self.inv_op)(**self._update_inv_op_kwargs(value))
-        self._source._set_str(key, value, inplace=inplace, validated=validated)
+        self._source._set_str(
+            key,
+            value,
+            inplace=inplace,
+            validated=validated,
+            ignore_lock=ignore_lock,
+            non_blocking=non_blocking,
+        )
         return self
 
-    def _set_tuple(self, key, value, *, inplace: bool, validated: bool):
+    def _set_tuple(
+        self, key, value, *, inplace: bool, validated: bool, non_blocking: bool
+    ):
         if len(key) == 1:
-            return self._set_str(key[0], value, inplace=inplace, validated=validated)
+            return self._set_str(
+                key[0],
+                value,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
+            )
         source = self._source._get_str(key[0], None)
         if source is None:
             source = self._source._create_nested_str(key[0])
         nested = type(self)(
             source,
             custom_op=self.custom_op,
             inv_op=self.inv_op,
             custom_op_kwargs=self._update_custom_op_kwargs(source),
             inv_op_kwargs=self._update_inv_op_kwargs(source),
         )
-        nested._set_tuple(key[1:], value, inplace=inplace, validated=validated)
+        nested._set_tuple(
+            key[1:],
+            value,
+            inplace=inplace,
+            validated=validated,
+            non_blocking=non_blocking,
+        )
         return self
 
-    def _set_at_str(self, key, value, idx, *, validated):
+    def _set_at_str(self, key, value, idx, *, validated, non_blocking: bool):
         transformed_tensor, original_tensor = self._get_str(
             key, NO_DEFAULT
         ), self._source._get_str(key, NO_DEFAULT)
         if transformed_tensor.data_ptr() != original_tensor.data_ptr():
             raise RuntimeError(
                 f"{self} original tensor and transformed_in do not point to the "
                 f"same storage. Setting values in place is not currently "
                 f"supported in this setting, consider calling "
                 f"`td.clone()` before `td.set_at_(...)`"
             )
         transformed_tensor[idx] = value
         return self
 
-    def _set_at_tuple(self, key, value, idx, *, validated):
+    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
         transformed_tensor, original_tensor = self._get_tuple(
             key, NO_DEFAULT
         ), self._source._get_tuple(key, NO_DEFAULT)
         if transformed_tensor.data_ptr() != original_tensor.data_ptr():
             raise RuntimeError(
                 f"{self} original tensor and transformed_in do not point to the "
                 f"same storage. Setting values in place is not currently "
@@ -2707,16 +2902,14 @@
             )
         return self.to_tensordict()
 
     def is_contiguous(self) -> bool:
         return all([value.is_contiguous() for _, value in self.items()])
 
     def contiguous(self) -> T:
-        if self.is_contiguous():
-            return self
         return self._fast_apply(lambda x: x.contiguous())
 
     def rename_key_(
         self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
     ) -> _CustomOpTensorDict:
         self._source.rename_key_(old_key, new_key, safe=safe)
         return self
@@ -2779,14 +2972,15 @@
         *,
         prefix: str | None,
         copy_existing: bool,
         executor,
         futures,
         inplace,
         like,
+        share_non_tensor,
     ) -> T:
         def save_metadata(data: TensorDictBase, filepath, metadata=None):
             if metadata is None:
                 metadata = {}
             metadata.update(
                 {
                     "shape": list(data.shape),
@@ -2810,14 +3004,15 @@
         dest_source = self._source._memmap_(
             prefix=None if prefix is None else prefix / "_source",
             copy_existing=copy_existing,
             executor=executor,
             futures=futures,
             inplace=inplace,
             like=like,
+            share_non_tensor=share_non_tensor,
         )
         if not inplace:
             dest = type(self)(
                 dest_source,
                 custom_op=self.custom_op,
                 inv_op=self.inv_op,
                 custom_op_kwargs=self.custom_op_kwargs,
@@ -2958,14 +3153,15 @@
     _apply_nest = TensorDict._apply_nest
     _remove_batch_dim = TensorDict._remove_batch_dim
     all = TensorDict.all
     any = TensorDict.any
     expand = TensorDict.expand
     _unbind = TensorDict._unbind
     _get_names_idx = TensorDict._get_names_idx
+    from_dict_instance = TensorDict.from_dict_instance
 
 
 class _UnsqueezedTensorDict(_CustomOpTensorDict):
     """A lazy view on an unsqueezed TensorDict.
 
     When calling `tensordict.unsqueeze(dim)`, a lazy view of this operation is
     returned such that the following code snippet works without raising an
```

## tensordict/_td.py

```diff
@@ -67,14 +67,15 @@
     as_decorator,
     Buffer,
     cache,
     convert_ellipsis_to_idx,
     DeviceType,
     expand_as_right,
     IndexType,
+    is_non_tensor,
     is_tensorclass,
     KeyedJaggedTensor,
     lock_blocked,
     NestedKey,
     unravel_key,
     unravel_key_list,
 )
@@ -187,21 +188,23 @@
         True
 
     """
 
     _td_dim_names = None
     _is_shared = False
     _is_memmap = False
+    _has_exclusive_keys = False
 
     def __init__(
         self,
         source: T | dict[str, CompatibleType],
         batch_size: Sequence[int] | torch.Size | int | None = None,
         device: DeviceType | None = None,
         names: Sequence[str] | None = None,
+        non_blocking: bool = False,
         _run_checks: bool = True,
     ) -> None:
         if device is not None and isinstance(device, (int, str)):
             device = torch.device(device)
         self._device = device
 
         self._tensordict = _tensordict = _StringOnlyDict()
@@ -211,29 +214,30 @@
                 for key, value in source.items():
                     if isinstance(value, dict):
                         value = TensorDict(
                             value,
                             batch_size=self._batch_size,
                             device=self._device,
                             _run_checks=_run_checks,
+                            non_blocking=non_blocking,
                         )
                     _tensordict[key] = value
             self._td_dim_names = names
         else:
             if not isinstance(source, (TensorDictBase, dict)):
                 raise ValueError(
                     "A TensorDict source is expected to be a TensorDictBase "
                     f"sub-type or a dictionary, found type(source)={type(source)}."
                 )
             self._batch_size = self._parse_batch_size(source, batch_size)
             self.names = names
 
             if source is not None:
                 for key, value in source.items():
-                    self.set(key, value)
+                    self.set(key, value, non_blocking=non_blocking)
 
     @classmethod
     def from_module(
         cls,
         module: torch.nn.Module,
         as_module: bool = False,
         lock: bool = False,
@@ -288,46 +292,48 @@
             if submodule is not None:
                 subtd = cls._from_module(
                     module=submodule,
                     as_module=False,
                     use_state_dict=use_state_dict,
                     prefix=prefix + name + ".",
                 )
-                destination._set_str(name, subtd, validated=True, inplace=False)
+                destination._set_str(
+                    name, subtd, validated=True, inplace=False, non_blocking=False
+                )
 
         if as_module:
             from tensordict.nn.params import TensorDictParams
 
             return TensorDictParams(destination, no_convert=True)
         return destination
 
     def is_empty(self):
 
         for item in self._tensordict.values():
             # we need to check if item is empty
             if _is_tensor_collection(type(item)):
                 if not item.is_empty():
                     return False
-                from tensordict.tensorclass import NonTensorData
 
-                if isinstance(item, NonTensorData):
+                if is_non_tensor(item):
                     return False
             else:
                 return False
         return True
 
     def _to_module(
         self,
         module,
         *,
         inplace: bool | None = None,
         return_swap: bool = True,
         swap_dest=None,
         memo=None,
         use_state_dict: bool = False,
+        non_blocking: bool = False,
     ):
 
         if not use_state_dict and isinstance(module, TensorDictBase):
             if return_swap:
                 swap = module.copy()
                 module._param_td = getattr(self, "_param_td", self)
                 return swap
@@ -393,15 +399,15 @@
                         # use specialized __setattr__ if needed
                         delattr(module, key)
                         setattr(module, key, value)
                     else:
                         new_val = local_out
                         if return_swap:
                             local_out = local_out.clone()
-                        new_val.data.copy_(value.data)
+                        new_val.data.copy_(value.data, non_blocking=non_blocking)
             else:
                 if value.is_empty():
                     # if there is at least one key, we must populate the module.
                     # Otherwise, we just go to the next key
                     continue
                 child = __dict__["_modules"][key]
                 local_out = memo.get(id(child), NO_DEFAULT)
@@ -419,39 +425,48 @@
                     local_out = value._to_module(
                         child,
                         inplace=inplace,
                         return_swap=return_swap,
                         swap_dest={},  # we'll be calling update later
                         memo=memo,
                         use_state_dict=use_state_dict,
+                        non_blocking=non_blocking,
                     )
 
             if return_swap:
                 _swap[key] = local_out
         if return_swap:
             if isinstance(swap_dest, dict):
                 return _swap
             elif swap_dest is not None:
 
                 def _quick_set(swap_dict, swap_td):
                     for key, val in swap_dict.items():
                         if isinstance(val, dict):
                             _quick_set(val, swap_td._get_str(key, default=NO_DEFAULT))
                         elif swap_td._get_str(key, None) is not val:
-                            swap_td._set_str(key, val, inplace=False, validated=True)
+                            swap_td._set_str(
+                                key,
+                                val,
+                                inplace=False,
+                                validated=True,
+                                non_blocking=non_blocking,
+                            )
 
                 _quick_set(_swap, swap_dest)
                 return swap_dest
             else:
                 return TensorDict(_swap, batch_size=[], _run_checks=False)
 
     def __ne__(self, other: object) -> T | bool:
         if _is_tensorclass(other):
             return other != self
-        if isinstance(other, (dict,)) or _is_tensor_collection(other.__class__):
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
             keys1 = set(self.keys())
             keys2 = set(other.keys())
             if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
                 raise KeyError(
                     f"keys in {self} and {other} mismatch, got {keys1} and {keys2}"
                 )
             d = {}
@@ -465,15 +480,17 @@
                 device=self.device,
             )
         return True
 
     def __xor__(self, other: object) -> T | bool:
         if _is_tensorclass(other):
             return other ^ self
-        if isinstance(other, (dict,)) or _is_tensor_collection(other.__class__):
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
             keys1 = set(self.keys())
             keys2 = set(other.keys())
             if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
                 raise KeyError(
                     f"keys in {self} and {other} mismatch, got {keys1} and {keys2}"
                 )
             d = {}
@@ -487,15 +504,17 @@
                 device=self.device,
             )
         return True
 
     def __or__(self, other: object) -> T | bool:
         if _is_tensorclass(other):
             return other | self
-        if isinstance(other, (dict,)) or _is_tensor_collection(other.__class__):
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
             keys1 = set(self.keys())
             keys2 = set(other.keys())
             if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
                 raise KeyError(
                     f"keys in {self} and {other} mismatch, got {keys1} and {keys2}"
                 )
             d = {}
@@ -510,19 +529,27 @@
             )
         return False
 
     def __eq__(self, other: object) -> T | bool:
         if is_tensorclass(other):
             return other == self
         if isinstance(other, (dict,)):
-            other = self.empty(recurse=True).update(other)
+            other = self.from_dict_instance(other)
         if _is_tensor_collection(other.__class__):
             keys1 = set(self.keys())
             keys2 = set(other.keys())
             if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                keys1 = sorted(
+                    keys1,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                keys2 = sorted(
+                    keys2,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
                 raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
             d = {}
             for key, item1 in self.items():
                 d[key] = item1 == other.get(key)
             return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
         if isinstance(other, (numbers.Number, Tensor)):
             return TensorDict(
@@ -545,27 +572,28 @@
                 self._set_tuple(
                     index_unravel,
                     value,
                     inplace=BEST_ATTEMPT_INPLACE
                     if isinstance(self, _SubTensorDict)
                     else False,
                     validated=False,
+                    non_blocking=False,
                 )
                 return
 
         # we must use any and because using Ellipsis in index can break with some indices
         if index is Ellipsis or (
             isinstance(index, tuple) and any(idx is Ellipsis for idx in index)
         ):
             index = convert_ellipsis_to_idx(index, self.batch_size)
 
         if isinstance(value, (TensorDictBase, dict)):
             indexed_bs = _getitem_batch_size(self.batch_size, index)
             if isinstance(value, dict):
-                value = TensorDict.from_dict(value, batch_size=indexed_bs)
+                value = self.from_dict_instance(value, batch_size=indexed_bs)
                 # value = self.empty(recurse=True)[index].update(value)
             if value.batch_size != indexed_bs:
                 if value.shape == indexed_bs[-len(value.shape) :]:
                     # try to expand on the left (broadcasting)
                     value = value.expand(indexed_bs)
                 else:
                     try:
@@ -580,17 +608,19 @@
                         ) from err
 
             keys = set(self.keys())
             if any(key not in keys for key in value.keys()):
                 subtd = self._get_sub_tensordict(index)
             for key, item in value.items():
                 if key in keys:
-                    self._set_at_str(key, item, index, validated=False)
+                    self._set_at_str(
+                        key, item, index, validated=False, non_blocking=False
+                    )
                 else:
-                    subtd.set(key, item, inplace=True)
+                    subtd.set(key, item, inplace=True, non_blocking=False)
         else:
             for key in self.keys():
                 self.set_at_(key, value, index)
 
     def all(self, dim: int = None) -> bool | TensorDictBase:
         if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
             raise RuntimeError(
@@ -647,43 +677,60 @@
         inplace: bool = False,
         checked: bool = False,
         call_on_nested: bool = False,
         default: Any = NO_DEFAULT,
         named: bool = False,
         nested_keys: bool = False,
         prefix: tuple = (),
+        filter_empty: bool | None = None,
+        is_leaf: Callable = None,
         **constructor_kwargs,
-    ) -> T:
+    ) -> T | None:
         if inplace:
-            out = self
+            result = self
+            is_locked = result.is_locked
         elif batch_size is not None:
-            out = TensorDict(
-                {},
-                batch_size=torch.Size(batch_size),
-                names=names,
-                device=self.device if not device else device,
-                _run_checks=False,
-                **constructor_kwargs,
-            )
+
+            def make_result():
+                return TensorDict(
+                    {},
+                    batch_size=torch.Size(batch_size),
+                    names=names,
+                    device=self.device if not device else device,
+                    _run_checks=False,
+                    **constructor_kwargs,
+                )
+
+            result = None
+            is_locked = False
         else:
-            out = TensorDict(
-                {},
-                batch_size=self.batch_size,
-                device=self.device if not device else device,
-                names=self.names if self._has_names() else None,
-                _run_checks=False,
-                **constructor_kwargs,
-            )
 
-        is_locked = out.is_locked
-        if not inplace and is_locked:
-            out.unlock_()
+            def make_result():
+                return TensorDict(
+                    {},
+                    batch_size=self.batch_size,
+                    device=self.device if not device else device,
+                    names=self.names if self._has_names() else None,
+                    _run_checks=False,
+                    **constructor_kwargs,
+                )
+
+            result = None
+            is_locked = False
+
+        any_set = False
+        if is_leaf is None:
+            is_leaf = _default_is_leaf
 
         for key, item in self.items():
-            if not call_on_nested and _is_tensor_collection(item.__class__):
+            if (
+                not call_on_nested
+                and not is_leaf(item.__class__)
+                # and not is_non_tensor(item)
+            ):
                 if default is not NO_DEFAULT:
                     _others = [_other._get_str(key, default=None) for _other in others]
                     _others = [
                         self.empty(recurse=True) if _other is None else _other
                         for _other in _others
                     ]
                 else:
@@ -698,39 +745,59 @@
                     batch_size=batch_size,
                     device=device,
                     checked=checked,
                     named=named,
                     nested_keys=nested_keys,
                     default=default,
                     prefix=prefix + (key,),
+                    filter_empty=filter_empty,
+                    is_leaf=is_leaf,
                     **constructor_kwargs,
                 )
             else:
                 _others = [_other._get_str(key, default=default) for _other in others]
                 if named:
                     if nested_keys:
                         item_trsf = fn(unravel_key(prefix + (key,)), item, *_others)
                     else:
                         item_trsf = fn(key, item, *_others)
                 else:
                     item_trsf = fn(item, *_others)
             if item_trsf is not None:
+                if not any_set:
+                    if result is None:
+                        result = make_result()
+                    any_set = True
                 if isinstance(self, _SubTensorDict):
-                    out.set(key, item_trsf, inplace=inplace)
+                    result.set(key, item_trsf, inplace=inplace)
                 else:
-                    out._set_str(
+                    result._set_str(
                         key,
                         item_trsf,
                         inplace=BEST_ATTEMPT_INPLACE if inplace else False,
                         validated=checked,
+                        non_blocking=False,
                     )
 
+        if filter_empty and not any_set:
+            return
+        elif filter_empty is None and not any_set:
+            warn(
+                "Your resulting tensordict has no leaves but you did not specify filter_empty=False. "
+                "Currently, this returns an empty tree (filter_empty=True), but from v0.5 it will return "
+                "a None unless filter_empty=False. "
+                "To silcence this warning, set filter_empty to the desired value in your call to `apply`.",
+                category=DeprecationWarning,
+            )
+        if result is None:
+            result = make_result()
+
         if not inplace and is_locked:
-            out.lock_()
-        return out
+            result.lock_()
+        return result
 
     # Functorch compatibility
     @cache  # noqa: B019
     def _add_batch_dim(self, *, in_dim, vmap_level):
         td = self
 
         def _add_batch_dim_wrapper(key, value):
@@ -858,15 +925,18 @@
                 new_shape = (*shape, *tensor_shape[-last_n_dims:])
             else:
                 new_shape = shape
             return tensor.expand(new_shape)
 
         names = [None] * (len(shape) - tensordict_dims) + self.names
         return self._fast_apply(
-            _expand, batch_size=shape, call_on_nested=True, names=names
+            _expand,
+            batch_size=shape,
+            call_on_nested=True,
+            names=names,
         )
 
     def _unbind(self, dim: int):
         batch_size = torch.Size([s for i, s in enumerate(self.batch_size) if i != dim])
         names = None
         if self._has_names():
             names = copy(self.names)
@@ -890,15 +960,17 @@
             unbound = (
                 val.unbind(dim)
                 if not isinstance(val, TensorDictBase)
                 # tensorclass is also unbound using plain unbind
                 else val._unbind(dim)
             )
             for td, _val in zip(tds, unbound):
-                td._set_str(key, _val, validated=True, inplace=False)
+                td._set_str(
+                    key, _val, validated=True, inplace=False, non_blocking=False
+                )
 
         for key, val in self.items():
             unbind(key, val)
         return tds
 
     def split(self, split_size: int | list[int], dim: int = 0) -> list[TensorDictBase]:
         # we must use slices to keep the storage of the tensors
@@ -1244,14 +1316,15 @@
         """
         if batch_dims is not None and batch_size is not None:
             raise ValueError(
                 "Cannot pass both batch_size and batch_dims to `from_dict`."
             )
 
         batch_size_set = torch.Size(()) if batch_size is None else batch_size
+        input_dict = copy(input_dict)
         for key, value in list(input_dict.items()):
             if isinstance(value, (dict,)):
                 # we don't know if another tensor of smaller size is coming
                 # so we can't be sure that the batch-size will still be valid later
                 input_dict[key] = TensorDict.from_dict(
                     value, batch_size=[], device=device, batch_dims=None
                 )
@@ -1263,14 +1336,49 @@
         )
         if batch_size is None:
             _set_max_batch_size(out, batch_dims)
         else:
             out.batch_size = batch_size
         return out
 
+    def from_dict_instance(
+        self, input_dict, batch_size=None, device=None, batch_dims=None
+    ):
+        if batch_dims is not None and batch_size is not None:
+            raise ValueError(
+                "Cannot pass both batch_size and batch_dims to `from_dict`."
+            )
+        from tensordict import TensorDict
+
+        batch_size_set = torch.Size(()) if batch_size is None else batch_size
+        input_dict = copy(input_dict)
+        for key, value in list(input_dict.items()):
+            if isinstance(value, (dict,)):
+                cur_value = self.get(key, None)
+                if cur_value is not None:
+                    input_dict[key] = cur_value.from_dict_instance(
+                        value, batch_size=[], device=device, batch_dims=None
+                    )
+                    continue
+                # we don't know if another tensor of smaller size is coming
+                # so we can't be sure that the batch-size will still be valid later
+                input_dict[key] = TensorDict.from_dict(
+                    value, batch_size=[], device=device, batch_dims=None
+                )
+        out = TensorDict.from_dict(
+            input_dict,
+            batch_size=batch_size_set,
+            device=device,
+        )
+        if batch_size is None:
+            _set_max_batch_size(out, batch_dims)
+        else:
+            out.batch_size = batch_size
+        return out
+
     @staticmethod
     def _parse_batch_size(
         source: T | dict,
         batch_size: Sequence[int] | torch.Size | int | None = None,
     ) -> torch.Size:
         try:
             return torch.Size(batch_size)
@@ -1455,32 +1563,34 @@
     def _set_str(
         self,
         key: NestedKey,
         value: dict[str, CompatibleType] | CompatibleType,
         *,
         inplace: bool,
         validated: bool,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
     ) -> T:
         if inplace is not False:
             best_attempt = inplace is BEST_ATTEMPT_INPLACE
             inplace = self._convert_inplace(inplace, key)
         if not validated:
             value = self._validate_value(value, check_shape=True)
         if not inplace:
-            if self._is_locked:
+            if self._is_locked and not ignore_lock:
                 raise RuntimeError(_LOCK_ERROR)
             self._tensordict[key] = value
         else:
             try:
                 dest = self._get_str(key, default=NO_DEFAULT)
                 if best_attempt and _is_tensor_collection(dest.__class__):
-                    dest.update(value, inplace=True)
+                    dest.update(value, inplace=True, non_blocking=non_blocking)
                 else:
                     if dest is not value:
-                        dest.copy_(value, non_blocking=True)
+                        dest.copy_(value, non_blocking=non_blocking)
             except KeyError as err:
                 raise err
             except Exception as err:
                 raise ValueError(
                     f"Failed to update '{key}' in tensordict {self}"
                 ) from err
         return self
@@ -1488,56 +1598,95 @@
     def _set_tuple(
         self,
         key: NestedKey,
         value: dict[str, CompatibleType] | CompatibleType,
         *,
         inplace: bool,
         validated: bool,
+        non_blocking: bool = False,
     ) -> T:
         if len(key) == 1:
-            return self._set_str(key[0], value, inplace=inplace, validated=validated)
+            return self._set_str(
+                key[0],
+                value,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
+            )
         td = self._get_str(key[0], None)
         if td is None:
             td = self._create_nested_str(key[0])
             inplace = False
         elif not _is_tensor_collection(td.__class__):
             raise KeyError(
                 f"The entry {key[0]} is already present in tensordict {self}."
             )
-        td._set_tuple(key[1:], value, inplace=inplace, validated=validated)
+        td._set_tuple(
+            key[1:],
+            value,
+            inplace=inplace,
+            validated=validated,
+            non_blocking=non_blocking,
+        )
         return self
 
-    def _set_at_str(self, key, value, idx, *, validated):
+    def _set_at_str(self, key, value, idx, *, validated, non_blocking: bool):
         if not validated:
             value = self._validate_value(value, check_shape=False)
             validated = True
         tensor_in = self._get_str(key, NO_DEFAULT)
 
         if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
             warn(
                 "Multiple indexing can lead to unexpected behaviours when "
                 "setting items, for instance `td[idx1][idx2] = other` may "
                 "not write to the desired location if idx1 is a list/tensor."
             )
             tensor_in = _sub_index(tensor_in, idx)
-            tensor_in.copy_(value)
+            tensor_in.copy_(value, non_blocking=non_blocking)
         else:
-            _set_item(tensor_in, idx, value, validated=validated)
+            tensor_out = _set_item(
+                tensor_in, idx, value, validated=validated, non_blocking=non_blocking
+            )
+            if tensor_in is not tensor_out:
+                if self._is_shared or self._is_memmap:
+                    raise RuntimeError(
+                        "You're attempting to update a leaf in-place with a shared "
+                        "tensordict, but the new value does not match the previous. "
+                        "If you're using NonTensorData, see the class documentation "
+                        "to see how to properly pre-allocate memory in shared contexts."
+                    )
+                # this happens only when a NonTensorData becomes a NonTensorStack
+                # so it is legitimate (there is no in-place modification of a tensor
+                # that was expected to happen but didn't).
+                # For this reason we can ignore the locked attribute of the td.
+                self._set_str(
+                    key,
+                    tensor_out,
+                    validated=True,
+                    inplace=False,
+                    ignore_lock=True,
+                    non_blocking=non_blocking,
+                )
 
         return self
 
-    def _set_at_tuple(self, key, value, idx, *, validated):
+    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
         if len(key) == 1:
-            return self._set_at_str(key[0], value, idx, validated=validated)
+            return self._set_at_str(
+                key[0], value, idx, validated=validated, non_blocking=non_blocking
+            )
         if key[0] not in self.keys():
             # this won't work
             raise KeyError(f"key {key} not found in set_at_ with tensordict {self}.")
         else:
             td = self._get_str(key[0], NO_DEFAULT)
-        td._set_at_tuple(key[1:], value, idx, validated=validated)
+        td._set_at_tuple(
+            key[1:], value, idx, validated=validated, non_blocking=non_blocking
+        )
         return self
 
     @lock_blocked
     def del_(self, key: NestedKey) -> T:
         key = _unravel_key_to_tuple(key)
         if len(key) > 1:
             td, subkey = _get_leaf_tensordict(self, key)
@@ -1564,17 +1713,29 @@
             raise TypeError(
                 f"Expected new_name to be a string or a tuple of strings but found {type(new_key)}"
             )
         if safe and (new_key in self.keys(include_nested=True)):
             raise KeyError(f"key {new_key} already present in TensorDict.")
 
         if isinstance(new_key, str):
-            self._set_str(new_key, self.get(old_key), inplace=False, validated=True)
+            self._set_str(
+                new_key,
+                self.get(old_key),
+                inplace=False,
+                validated=True,
+                non_blocking=False,
+            )
         else:
-            self._set_tuple(new_key, self.get(old_key), inplace=False, validated=True)
+            self._set_tuple(
+                new_key,
+                self.get(old_key),
+                inplace=False,
+                validated=True,
+                non_blocking=False,
+            )
         self.del_(old_key)
         return self
 
     def _stack_onto_(self, list_item: list[CompatibleType], dim: int) -> TensorDict:
         # if not isinstance(key, str):
         #     raise ValueError("_stack_onto_ expects string keys.")
         for key in self.keys():
@@ -1660,20 +1821,22 @@
     def detach_(self) -> T:
         for value in self.values():
             value.detach_()
         return self
 
     def _memmap_(
         self,
+        *,
         prefix: str | None,
         copy_existing: bool,
         executor,
         futures,
         inplace,
         like,
+        share_non_tensor,
     ) -> T:
         def save_metadata(data: TensorDictBase, filepath, metadata=None):
             if metadata is None:
                 metadata = {}
             metadata.update(
                 {
                     "shape": list(data.shape),
@@ -1699,23 +1862,35 @@
             else TensorDict(
                 {},
                 batch_size=self.batch_size,
                 names=self.names if self._has_names() else None,
                 device=torch.device("cpu"),
             )
         )
+
+        # We must set these attributes before memmapping because we need the metadata
+        # to match the tensordict content.
+        if inplace:
+            self._is_memmap = True
+            self._is_shared = False  # since they are mutually exclusive
+            self._device = torch.device("cpu")
+        else:
+            dest._is_memmap = True
+            dest._is_shared = False  # since they are mutually exclusive
+
         for key, value in self.items():
             if _is_tensor_collection(value.__class__):
                 dest._tensordict[key] = value._memmap_(
                     prefix=prefix / key if prefix is not None else None,
                     copy_existing=copy_existing,
                     executor=executor,
                     futures=futures,
                     inplace=inplace,
                     like=like,
+                    share_non_tensor=share_non_tensor,
                 )
                 continue
             else:
                 # user did specify location and memmap is in wrong place, so we copy
                 def _populate(
                     dest=dest, value=value, key=key, copy_existing=copy_existing
                 ):
@@ -1746,23 +1921,14 @@
                     prefix / "meta.json",
                     metadata=metadata,
                 )
             else:
                 futures.append(
                     executor.submit(save_metadata, dest, prefix / "meta.json", metadata)
                 )
-        if inplace:
-            self._is_memmap = True
-            self._is_shared = False  # since they are mutually exclusive
-            self._device = torch.device("cpu")
-        else:
-            dest._is_memmap = True
-            dest._is_shared = False  # since they are mutually exclusive
-            dest._device = torch.device("cpu")
-
         dest._is_locked = True
         return dest
 
     @classmethod
     def _load_memmap(cls, prefix: str, metadata: dict) -> T:
         if metadata["device"] == "None":
             metadata["device"] = None
@@ -1790,14 +1956,15 @@
                 MemoryMappedTensor.from_filename(
                     dtype=_STRDTYPE2DTYPE[dtype],
                     shape=torch.Size(entry_metadata["shape"]),
                     filename=str(prefix / f"{key}.memmap"),
                 ),
                 validated=True,
                 inplace=False,
+                non_blocking=False,
             )
         # iterate over folders and load them
         for path in prefix.iterdir():
             if path.is_dir():
                 key = path.parts[len(prefix.parts) :]
                 out.set(key, TensorDict.load_memmap(path))
         return out
@@ -1870,15 +2037,17 @@
                     if _other is None:
                         _other = tensor.empty()
                     val = tensor.where(
                         condition=condition, other=_other, out=_out, pad=pad
                     )
                 else:
                     val = func(tensor, _other, key)
-                result._set_str(key, val, inplace=False, validated=True)
+                result._set_str(
+                    key, val, inplace=False, validated=True, non_blocking=False
+                )
                 other_keys.discard(key)
             for key in other_keys:
                 tensor = None
                 _other = other._get_str(key, default=NO_DEFAULT)
                 if _is_tensor_collection(type(_other)):
                     try:
                         tensor = _other.empty()
@@ -1886,15 +2055,17 @@
                         # H5 tensordicts do not support select()
                         tensor = _other.to_tensordict().empty()
                     val = _other.where(
                         condition=~condition, other=tensor, out=None, pad=pad
                     )
                 else:
                     val = func(tensor, _other, key)
-                result._set_str(key, val, inplace=False, validated=True)
+                result._set_str(
+                    key, val, inplace=False, validated=True, non_blocking=False
+                )
             return result
         else:
             if out is None:
 
                 def func(tensor):
                     return torch.where(
                         condition=expand_as_right(condition, tensor),
@@ -1940,17 +2111,25 @@
         # This may be undesirable, not sure if this should be the default behaviour
         # (one usually does a copy to modify it).
         # if not recurse:
         #     self._maybe_set_shared_attributes(result)
         return result
 
     def contiguous(self) -> T:
-        if not self.is_contiguous():
-            return self.clone()
-        return self
+        source = {key: value.contiguous() for key, value in self.items()}
+        batch_size = self.batch_size
+        device = self.device
+        out = TensorDict(
+            source=source,
+            batch_size=batch_size,
+            device=device,
+            names=self.names,
+            _run_checks=False,
+        )
+        return out
 
     def empty(self, recurse=False) -> T:
         if not recurse:
             return TensorDict(
                 device=self._device,
                 batch_size=self._batch_size,
                 source={},
@@ -2264,21 +2443,25 @@
                 "prohibited for existing tensors. Consider calling "
                 "_SubTensorDict.set_(...) or cloning your tensordict first."
             )
         elif not inplace and self.is_locked:
             raise RuntimeError(_LOCK_ERROR)
         return inplace
 
+    from_dict_instance = TensorDict.from_dict_instance
+
     def _set_str(
         self,
         key: NestedKey,
         value: dict[str, CompatibleType] | CompatibleType,
         *,
         inplace: bool,
         validated: bool,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
     ) -> T:
         inplace = self._convert_inplace(inplace, key)
         # it is assumed that if inplace=False then the key doesn't exist. This is
         # checked in set method, but not here. responsibility lies with the caller
         # so that this method can have minimal overhead from runtime checks
         parent = self._source
         if not validated:
@@ -2293,81 +2476,114 @@
                     value_expand._set_str(
                         _key,
                         _expand_to_match_shape(
                             parent.batch_size, _tensor, self.batch_dims, self.device
                         ),
                         inplace=inplace,
                         validated=validated,
+                        ignore_lock=ignore_lock,
+                        non_blocking=non_blocking,
                     )
             else:
                 value_expand = torch.zeros(
                     (
                         *parent.batch_size,
                         *_shape(value)[self.batch_dims :],
                     ),
                     dtype=value.dtype,
                     device=self.device,
                 )
                 if self._is_shared:
                     value_expand.share_memory_()
                 elif self._is_memmap:
                     value_expand = MemoryMappedTensor.from_tensor(value_expand)
-            parent._set_str(key, value_expand, inplace=False, validated=validated)
+            parent._set_str(
+                key,
+                value_expand,
+                inplace=False,
+                validated=validated,
+                ignore_lock=ignore_lock,
+                non_blocking=non_blocking,
+            )
 
-        parent._set_at_str(key, value, self.idx, validated=validated)
+        parent._set_at_str(
+            key, value, self.idx, validated=validated, non_blocking=non_blocking
+        )
         return self
 
     def _set_tuple(
         self,
         key: NestedKey,
         value: dict[str, CompatibleType] | CompatibleType,
         *,
         inplace: bool,
         validated: bool,
+        non_blocking: bool = False,
     ) -> T:
         if len(key) == 1:
-            return self._set_str(key[0], value, inplace=inplace, validated=validated)
+            return self._set_str(
+                key[0],
+                value,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
+            )
         parent = self._source
         td = parent._get_str(key[0], None)
         if td is None:
             td = parent.select()
-            parent._set_str(key[0], td, inplace=False, validated=True)
+            parent._set_str(
+                key[0], td, inplace=False, validated=True, non_blocking=non_blocking
+            )
         _SubTensorDict(td, self.idx)._set_tuple(
-            key[1:], value, inplace=inplace, validated=validated
+            key[1:],
+            value,
+            inplace=inplace,
+            validated=validated,
+            non_blocking=non_blocking,
         )
         return self
 
-    def _set_at_str(self, key, value, idx, *, validated):
+    def _set_at_str(self, key, value, idx, *, validated, non_blocking: bool):
         tensor_in = self._get_str(key, NO_DEFAULT)
         if not validated:
             value = self._validate_value(value, check_shape=False)
             validated = True
         if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
             warn(
                 "Multiple indexing can lead to unexpected behaviours when "
                 "setting items, for instance `td[idx1][idx2] = other` may "
                 "not write to the desired location if idx1 is a list/tensor."
             )
             tensor_in = _sub_index(tensor_in, idx)
             tensor_in.copy_(value)
+            tensor_out = tensor_in
         else:
-            _set_item(tensor_in, idx, value, validated=validated)
+            tensor_out = _set_item(
+                tensor_in, idx, value, validated=validated, non_blocking=non_blocking
+            )
         # make sure that the value is updated
-        self._source._set_at_str(key, tensor_in, self.idx, validated=validated)
+        self._source._set_at_str(
+            key, tensor_out, self.idx, validated=validated, non_blocking=non_blocking
+        )
         return self
 
-    def _set_at_tuple(self, key, value, idx, *, validated):
+    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
         if len(key) == 1:
-            return self._set_at_str(key[0], value, idx, validated=validated)
+            return self._set_at_str(
+                key[0], value, idx, validated=validated, non_blocking=non_blocking
+            )
         if key[0] not in self.keys():
             # this won't work
             raise KeyError(f"key {key} not found in set_at_ with tensordict {self}.")
         else:
             td = self._get_str(key[0], NO_DEFAULT)
-        td._set_at_tuple(key[1:], value, idx, validated=validated)
+        td._set_at_tuple(
+            key[1:], value, idx, validated=validated, non_blocking=non_blocking
+        )
         return self
 
     # @cache  # noqa: B019
     def keys(
         self,
         include_nested: bool = False,
         leaves_only: bool = False,
@@ -2409,47 +2625,48 @@
         key: NestedKey,
         default: Tensor | str | None = NO_DEFAULT,
     ) -> CompatibleType:
         return self._source.get_at(key, self.idx, default=default)
 
     def _get_non_tensor(self, key: NestedKey, default=NO_DEFAULT):
         out = super()._get_non_tensor(key, default=default)
-        from tensordict.tensorclass import NonTensorData
 
-        if isinstance(out, _SubTensorDict) and isinstance(out._source, NonTensorData):
-            return out._source.data
+        if isinstance(out, _SubTensorDict) and is_non_tensor(out._source):
+            return out._source
         return out
 
     def _get_str(self, key, default):
         if key in self.keys() and _is_tensor_collection(self.entry_class(key)):
-            return _SubTensorDict(self._source._get_str(key, NO_DEFAULT), self.idx)
+            data = self._source._get_str(key, NO_DEFAULT)
+            if is_non_tensor(data):
+                return data[self.idx]
+            return _SubTensorDict(data, self.idx)
         return self._source._get_at_str(key, self.idx, default=default)
 
     def _get_tuple(self, key, default):
         return self._source._get_at_tuple(key, self.idx, default=default)
 
     def update(
         self,
         input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
         clone: bool = False,
         inplace: bool = False,
         *,
+        non_blocking: bool = False,
         keys_to_update: Sequence[NestedKey] | None = None,
         **kwargs,
     ) -> _SubTensorDict:
         if input_dict_or_td is self:
             # no op
             return self
-        from ._lazy import LazyStackedTensorDict
 
-        if isinstance(self._source, LazyStackedTensorDict):
-            if self._source._has_exclusive_keys:
-                raise RuntimeError(
-                    "Cannot use _SubTensorDict.update with a LazyStackedTensorDict that has exclusive keys."
-                )
+        if getattr(self._source, "_has_exclusive_keys", False):
+            raise RuntimeError(
+                "Cannot use _SubTensorDict.update with a LazyStackedTensorDict that has exclusive keys."
+            )
         if keys_to_update is not None:
             if len(keys_to_update) == 0:
                 return self
             keys_to_update = unravel_key_list(keys_to_update)
         keys = set(self.keys(False))
         for key, value in input_dict_or_td.items():
             key = _unravel_key_to_tuple(key)
@@ -2472,60 +2689,75 @@
                         sub_keys_to_update = _prune_selected_keys(
                             keys_to_update, firstkey
                         )
                         target.update(
                             {subkey: value},
                             inplace=False,
                             keys_to_update=sub_keys_to_update,
+                            non_blocking=non_blocking,
                         )
                         continue
                     elif isinstance(value, dict) or _is_tensor_collection(
                         value.__class__
                     ):
                         sub_keys_to_update = _prune_selected_keys(
                             keys_to_update, firstkey
                         )
-                        target.update(value, keys_to_update=sub_keys_to_update)
+                        target.update(
+                            value,
+                            keys_to_update=sub_keys_to_update,
+                            non_blocking=non_blocking,
+                        )
                         continue
                     raise ValueError(
                         f"Tried to replace a tensordict with an incompatible object of type {type(value)}"
                     )
                 else:
-                    self._set_tuple(key, value, inplace=True, validated=False)
+                    self._set_tuple(
+                        key,
+                        value,
+                        inplace=True,
+                        validated=False,
+                        non_blocking=non_blocking,
+                    )
             else:
                 self._set_tuple(
                     key,
                     value,
                     inplace=BEST_ATTEMPT_INPLACE if inplace else False,
                     validated=False,
+                    non_blocking=non_blocking,
                 )
         return self
 
     def update_(
         self,
         input_dict: dict[str, CompatibleType] | TensorDictBase,
         clone: bool = False,
         *,
+        non_blocking: bool = False,
         keys_to_update: Sequence[NestedKey] | None = None,
     ) -> _SubTensorDict:
         return self.update_at_(
             input_dict,
             idx=self.idx,
             discard_idx_attr=True,
             clone=clone,
             keys_to_update=keys_to_update,
+            non_blocking=non_blocking,
         )
 
     def update_at_(
         self,
         input_dict: dict[str, CompatibleType] | TensorDictBase,
         idx: IndexType,
         *,
         discard_idx_attr: bool = False,
         clone: bool = False,
+        non_blocking: bool = False,
         keys_to_update: Sequence[NestedKey] | None = None,
     ) -> _SubTensorDict:
         if keys_to_update is not None:
             if len(keys_to_update) == 0:
                 return self
             keys_to_update = unravel_key_list(keys_to_update)
         for key, value in input_dict.items():
@@ -2544,18 +2776,21 @@
             if clone:
                 value = value.clone()
             if discard_idx_attr:
                 self._source._set_at_tuple(
                     key,
                     value,
                     idx,
+                    non_blocking=non_blocking,
                     validated=False,
                 )
             else:
-                self._set_at_tuple(key, value, idx, validated=False)
+                self._set_at_tuple(
+                    key, value, idx, validated=False, non_blocking=non_blocking
+                )
         return self
 
     def get_parent_tensordict(self) -> T:
         if not isinstance(self._source, TensorDictBase):
             raise TypeError(
                 f"_SubTensorDict was initialized with a source of type"
                 f" {self._source.__class__.__name__}, "
@@ -2617,19 +2852,17 @@
             )
         return self.to_tensordict()
 
     def is_contiguous(self) -> bool:
         return all(value.is_contiguous() for value in self.values())
 
     def contiguous(self) -> T:
-        if self.is_contiguous():
-            return self
         return TensorDict(
             batch_size=self.batch_size,
-            source={key: value for key, value in self.items()},
+            source={key: value.contiguous() for key, value in self.items()},
             device=self.device,
             names=self.names,
             _run_checks=False,
         )
 
     def _select(
         self,
@@ -2710,14 +2943,15 @@
         *,
         prefix: str | None,
         copy_existing: bool,
         executor,
         futures,
         inplace,
         like,
+        share_non_tensor,
     ) -> T:
         if prefix is not None:
 
             def save_metadata(prefix=prefix, self=self):
                 prefix = Path(prefix)
                 if not prefix.exists():
                     os.makedirs(prefix, exist_ok=True)
@@ -2738,14 +2972,15 @@
         _source = self._source._memmap_(
             prefix=prefix / "_source" if prefix is not None else None,
             copy_existing=copy_existing,
             executor=executor,
             futures=futures,
             inplace=inplace,
             like=like,
+            share_non_tensor=share_non_tensor,
         )
         if not inplace:
             result = _SubTensorDict(_source, idx=self.idx)
         else:
             result = self
         return result
 
@@ -2802,15 +3037,17 @@
 
     def __del__(self):
         pass
 
     def _create_nested_str(self, key):
         # this may fail with a sub-sub tensordict
         out = self._source.empty()
-        self._source._set_str(key, out, inplace=False, validated=True)
+        self._source._set_str(
+            key, out, inplace=False, validated=True, non_blocking=False
+        )
         # the id of out changes
         return self._get_str(key, default=NO_DEFAULT)
 
     # TODO: check these implementations
     __eq__ = TensorDict.__eq__
     __ne__ = TensorDict.__ne__
     __setitem__ = TensorDict.__setitem__
@@ -2991,41 +3228,49 @@
         key = _unravel_key_to_tuple(key)
         if not key:
             raise TypeError(_NON_STR_KEY_ERR)
 
         if isinstance(key, str):
             if key in self._keys():
                 if self.leaves_only:
-                    return not _is_tensor_collection(self.tensordict.entry_class(key))
+                    # TODO: make this faster for LazyStacked without compromising regular
+                    return not _is_tensor_collection(
+                        type(self.tensordict._get_str(key))
+                    )
                 return True
             return False
         else:
             # thanks to _unravel_key_to_tuple we know the key is a tuple
             if len(key) == 1:
                 return key[0] in self._keys()
             elif self.include_nested:
-                if key[0] in self._keys():
-                    entry_type = self.tensordict.entry_class(key[0])
-                    if entry_type in (Tensor, _MemmapTensor):
+                item_root = self.tensordict._get_str(key[0], default=None)
+                if item_root is not None:
+                    entry_type = type(item_root)
+                    if issubclass(entry_type, (Tensor, _MemmapTensor)):
                         return False
-                    if entry_type is KeyedJaggedTensor:
+                    elif entry_type is KeyedJaggedTensor:
                         if len(key) > 2:
                             return False
-                        return key[1] in self.tensordict.get(key[0]).keys()
+                        return key[1] in item_root.keys()
+                    # TODO: make this faster for LazyStacked without compromising regular
                     _is_tensordict = _is_tensor_collection(entry_type)
                     if _is_tensordict:
                         # # this will call _unravel_key_to_tuple many times
                         # return key[1:] in self.tensordict._get_str(key[0], NO_DEFAULT).keys(include_nested=self.include_nested)
                         # this won't call _unravel_key_to_tuple but requires to get the default which can be suboptimal
-                        leaf_td = self.tensordict._get_tuple(key[:-1], None)
-                        if leaf_td is None or (
-                            not _is_tensor_collection(leaf_td.__class__)
-                            and not isinstance(leaf_td, KeyedJaggedTensor)
-                        ):
-                            return False
+                        if len(key) >= 3:
+                            leaf_td = item_root._get_tuple(key[1:-1], None)
+                            if leaf_td is None or (
+                                not _is_tensor_collection(leaf_td.__class__)
+                                and not isinstance(leaf_td, KeyedJaggedTensor)
+                            ):
+                                return False
+                        else:
+                            leaf_td = item_root
                         return key[-1] in leaf_td.keys()
                 return False
             # this is reached whenever there is more than one key but include_nested is False
             if all(isinstance(subkey, str) for subkey in key):
                 raise TypeError(_NON_STR_KEY_TUPLE_ERR)
 
     def __repr__(self):
```

## tensordict/_torch_func.py

```diff
@@ -15,14 +15,15 @@
 from tensordict._td import TensorDict
 from tensordict.base import _is_leaf_nontensor, NO_DEFAULT, TensorDictBase
 from tensordict.persistent import PersistentTensorDict
 from tensordict.utils import (
     _check_keys,
     _ErrorInteceptor,
     DeviceType,
+    is_non_tensor,
     lazy_legacy,
     set_lazy_legacy,
 )
 from torch import Tensor
 from torch.nn.parameter import (
     UninitializedBuffer,
     UninitializedParameter,
@@ -159,14 +160,40 @@
         raise RuntimeError(
             f"keyword arguments {list(kwargs.keys())} are not "
             f"supported with full_like with TensorDict"
         )
     return td_clone
 
 
+@implements_for_td(torch.rand_like)
+def _rand_like(td: T, **kwargs: Any) -> T:
+    td_clone = td._fast_apply(lambda x: torch.rand_like(x))
+    if "device" in kwargs:
+        td_clone = td_clone.to(kwargs.pop("device"))
+    if len(kwargs):
+        raise RuntimeError(
+            f"keyword arguments {list(kwargs.keys())} are not "
+            f"supported with full_like with TensorDict"
+        )
+    return td_clone
+
+
+@implements_for_td(torch.randn_like)
+def _randn_like(td: T, **kwargs: Any) -> T:
+    td_clone = td._fast_apply(lambda x: torch.randn_like(x))
+    if "device" in kwargs:
+        td_clone = td_clone.to(kwargs.pop("device"))
+    if len(kwargs):
+        raise RuntimeError(
+            f"keyword arguments {list(kwargs.keys())} are not "
+            f"supported with full_like with TensorDict"
+        )
+    return td_clone
+
+
 @implements_for_td(torch.empty_like)
 def _empty_like(td: T, *args, **kwargs) -> T:
     try:
         tdclone = td.clone()
     except Exception as err:
         raise RuntimeError(
             "The tensordict passed to torch.empty_like cannot be "
@@ -351,17 +378,17 @@
     out: T | None = None,
     strict: bool = False,
     contiguous: bool = False,
 ) -> T:
     if not list_of_tensordicts:
         raise RuntimeError("list_of_tensordicts cannot be empty")
 
-    from tensordict.tensorclass import NonTensorData
+    if all(is_non_tensor(td) for td in list_of_tensordicts):
+        from tensordict.tensorclass import NonTensorData
 
-    if all(isinstance(td, NonTensorData) for td in list_of_tensordicts):
         return NonTensorData._stack_non_tensor(list_of_tensordicts, dim=dim)
 
     batch_size = list_of_tensordicts[0].batch_size
     if dim < 0:
         dim = len(batch_size) + dim + 1
 
     for td in list_of_tensordicts[1:]:
@@ -405,14 +432,34 @@
                 keys = _check_keys(list_of_tensordicts, strict=True)
             except KeyError:
                 if not _lazy_legacy and not contiguous:
                     with set_lazy_legacy(True):
                         return _stack(list_of_tensordicts, dim=dim)
                 raise
 
+            if all(
+                isinstance(_tensordict, LazyStackedTensorDict)
+                for _tensordict in list_of_tensordicts
+            ):
+                lazy_stack_dim = list_of_tensordicts[0].stack_dim
+                if dim <= lazy_stack_dim:
+                    lazy_stack_dim += 1
+                else:
+                    dim = dim - 1
+
+                return LazyStackedTensorDict(
+                    *[
+                        torch.stack(list_of_td, dim)
+                        for list_of_td in zip(
+                            *[td.tensordicts for td in list_of_tensordicts]
+                        )
+                    ],
+                    stack_dim=lazy_stack_dim,
+                )
+
             out = {}
             for key in keys:
                 out[key] = []
                 is_lazy = False
                 tensor_shape = None
                 for _tensordict in list_of_tensordicts:
                     tensor = _tensordict._get_str(key, default=NO_DEFAULT)
```

## tensordict/base.py

```diff
@@ -54,15 +54,15 @@
     cache,
     convert_ellipsis_to_idx,
     DeviceType,
     erase_cache,
     IndexType,
     infer_size_impl,
     int_generator,
-    KeyedJaggedTensor,
+    is_non_tensor,
     lazy_legacy,
     lock_blocked,
     NestedKey,
     prod,
     set_lazy_legacy,
     TensorDictFuture,
     unravel_key,
@@ -100,20 +100,21 @@
 
 _TENSOR_COLLECTION_MEMO = {}
 
 
 class TensorDictBase(MutableMapping):
     """TensorDictBase is an abstract parent class for TensorDicts, a torch.Tensor data container."""
 
-    _safe = False
-    _lazy = False
-    _inplace_set = False
-    is_meta = False
-    _is_locked = False
-    _cache = None
+    _safe: bool = False
+    _lazy: bool = False
+    _inplace_set: bool = False
+    is_meta: bool = False
+    _is_locked: bool = False
+    _cache: bool = None
+    _is_non_tensor: bool = False
 
     def __bool__(self) -> bool:
         raise RuntimeError("Converting a tensordict to boolean value is not permitted")
 
     @abc.abstractmethod
     def __ne__(self, other: object) -> T:
         """NOT operation over two tensordicts, for evey key.
@@ -236,18 +237,19 @@
         """
         istuple = isinstance(index, tuple)
         if istuple or isinstance(index, str):
             # _unravel_key_to_tuple will return an empty tuple if the index isn't a NestedKey
             idx_unravel = _unravel_key_to_tuple(index)
             if idx_unravel:
                 result = self._get_tuple(idx_unravel, NO_DEFAULT)
-                from .tensorclass import NonTensorData
-
-                if isinstance(result, NonTensorData):
-                    return result.data
+                if is_non_tensor(result):
+                    result_data = getattr(result, "data", NO_DEFAULT)
+                    if result_data is NO_DEFAULT:
+                        return result.tolist()
+                    return result_data
                 return result
 
         if (istuple and not index) or (not istuple and index is Ellipsis):
             # empty tuple returns self
             return self
         if not istuple:
             if isinstance(index, int):
@@ -361,14 +363,65 @@
             >>> print(td.batch_size)
             torch.Size([3])
 
         """
         _set_max_batch_size(self, batch_dims)
         return self
 
+    @abc.abstractmethod
+    def from_dict_instance(
+        self, input_dict, batch_size=None, device=None, batch_dims=None
+    ):
+        """Instance method version of :meth:`~tensordict.TensorDict.from_dict`.
+
+        Unlike :meth:`~tensordict.TensorDict.from_dict`, this method will
+        attempt to keep the tensordict types within the existing tree (for
+        any existing leaf).
+
+        Examples:
+            >>> from tensordict import TensorDict, tensorclass
+            >>> import torch
+            >>>
+            >>> @tensorclass
+            >>> class MyClass:
+            ...     x: torch.Tensor
+            ...     y: int
+            >>>
+            >>> td = TensorDict({"a": torch.randn(()), "b": MyClass(x=torch.zeros(()), y=1)})
+            >>> print(td.from_dict_instance(td.to_dict()))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: MyClass(
+                        x=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                        y=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> print(td.from_dict(td.to_dict()))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                            y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        """
+        ...
+
     # Module interaction
     @classmethod
     def from_module(
         cls,
         module,
         as_module: bool = False,
         lock: bool = True,
@@ -551,14 +604,15 @@
         self,
         module: nn.Module,
         *,
         inplace: bool | None = None,
         return_swap: bool = True,
         swap_dest=None,
         use_state_dict: bool = False,
+        non_blocking: bool = False,
         memo=None,  # deprecated
     ):
         """Writes the content of a TensorDictBase instance onto a given nn.Module attributes, recursively.
 
         Args:
             module (nn.Module): a module to write the parameters into.
 
@@ -568,14 +622,17 @@
             return_swap (bool, optional): if ``True``, the old parameter configuration
                 will be returned. Defaults to ``False``.
             swap_dest (TensorDictBase, optional): if ``return_swap`` is ``True``,
                 the tensordict where the swap should be written.
             use_state_dict (bool, optional): if ``True``, state-dict API will be
                 used to load the parameters (including the state-dict hooks).
                 Defaults to ``False``.
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
 
         Examples:
             >>> from torch import nn
             >>> module = nn.TransformerDecoder(
             ...     decoder_layer=nn.TransformerDecoderLayer(nhead=4, d_model=4),
             ...     num_layers=1)
             >>> params = TensorDict.from_module(module)
@@ -592,26 +649,28 @@
         return self._to_module(
             module=module,
             inplace=inplace,
             return_swap=return_swap,
             swap_dest=swap_dest,
             memo=memo,
             use_state_dict=use_state_dict,
+            non_blocking=non_blocking,
         )
 
     @abc.abstractmethod
     def _to_module(
         self,
         module,
         *,
         inplace: bool | None = None,
         return_swap: bool = True,
         swap_dest=None,
         memo=None,
         use_state_dict: bool = False,
+        non_blocking: bool = False,
     ):
         ...
 
     # Shape functionality
     @property
     def shape(self) -> torch.Size:
         """See :obj:`~tensordict.TensorDictBase.batch_size`."""
@@ -663,21 +722,22 @@
                 "modifying the batch size of a lazy representation of a "
                 "tensordict is not permitted. Consider instantiating the "
                 "tensordict first by calling `td = td.to_tensordict()` before "
                 "resetting the batch size."
             )
         if not isinstance(new_batch_size, torch.Size):
             new_batch_size = torch.Size(new_batch_size)
-        for key in self.keys():
-            if _is_tensor_collection(self.entry_class(key)):
-                tensordict = self.get(key)
-                if len(tensordict.batch_size) < len(new_batch_size):
+        for key, value in self.items():
+            if _is_tensor_collection(type(value)):
+                if len(value.batch_size) < len(new_batch_size):
                     # document as edge case
-                    tensordict.batch_size = new_batch_size
-                    self._set_str(key, tensordict, inplace=True, validated=True)
+                    value.batch_size = new_batch_size
+                    self._set_str(
+                        key, value, inplace=True, validated=True, non_blocking=False
+                    )
         self._check_new_batch_size(new_batch_size)
         self._change_batch_size(new_batch_size)
         if self._has_names():
             # if the tensordict has dim names and the new batch-size has more dims,
             # we can simply add empty names after the current ones.
             # Otherwise, we discard the extra existing names.
             names = self.names
@@ -1648,14 +1708,22 @@
                 tensor should be cast.
 
         """
         if device is None:
             return self.to(torch.device("cuda"))
         return self.to(f"cuda:{device}")
 
+    @property
+    def is_cuda(self):
+        return self.device is not None and self.device.type == "cuda"
+
+    @property
+    def is_cpu(self):
+        return self.device is not None and self.device.type == "cpu"
+
     # Serialization functionality
     def state_dict(
         self,
         destination=None,
         prefix="",
         keep_vars=False,
         flatten=False,
@@ -1892,24 +1960,26 @@
         *,
         prefix: str | None,
         copy_existing: bool,
         executor,
         futures,
         inplace,
         like,
+        share_non_tensor,
     ) -> T:
         ...
 
     def memmap_(
         self,
         prefix: str | None = None,
         copy_existing: bool = False,
         *,
         num_threads: int = 0,
         return_early: bool = False,
+        share_non_tensor: bool = False,
     ) -> T:
         """Writes all tensors onto a corresponding memory-mapped Tensor, in-place.
 
         Args:
             prefix (str): directory prefix where the memory-mapped tensors will
                 be stored. The directory tree structure will mimic the tensordict's.
             copy_existing (bool): If False (default), an exception will be raised if an
@@ -1919,28 +1989,35 @@
                 If ``True``, any existing Tensor will be copied to the new location.
 
         Keyword Args:
             num_threads (int, optional): the number of threads used to write the memmap
                 tensors. Defaults to `0`.
             return_early (bool, optional): if ``True`` and ``num_threads>0``,
                 the method will return a future of the tensordict.
+            share_non_tensor (bool, optional): if ``True``, the non-tensor data will be
+                shared between the processes and writing operation (such as inplace update
+                or set) on any of the workers within a single node will update the value
+                on all other workers. If the number of non-tensor leaves is high (e.g.,
+                sharing large stacks of non-tensor data) this may result in OOM or similar
+                errors. Defaults to ``False``.
 
         The TensorDict is then locked, meaning that any writing operations that
         isn't in-place will throw an exception (eg, rename, set or remove an
         entry).
         Once the tensordict is unlocked, the memory-mapped attribute is turned to ``False``,
         because cross-process identity is not guaranteed anymore.
 
         Returns:
             self if ``return_early=False``, otherwise a :class:`~tensordict.utils.TensorDictFuture` instance.
 
         Note:
             Serialising in this fashion might be slow with deeply nested tensordicts, so
             it is not recommended to call this method inside a training loop.
         """
+        prefix = Path(prefix) if prefix is not None else None
         if num_threads > 1:
             with (
                 ThreadPoolExecutor(max_workers=num_threads)
                 if not return_early
                 else contextlib.nullcontext()
             ) as executor:
                 if return_early:
@@ -1949,36 +2026,39 @@
                 result = self._memmap_(
                     prefix=prefix,
                     copy_existing=copy_existing,
                     executor=executor,
                     futures=futures,
                     inplace=True,
                     like=False,
+                    share_non_tensor=share_non_tensor,
                 )
                 if not return_early:
                     concurrent.futures.wait(futures)
                     return result
                 else:
                     return TensorDictFuture(futures, result)
         return self._memmap_(
             prefix=prefix,
             copy_existing=copy_existing,
             inplace=True,
             futures=None,
             executor=None,
             like=False,
+            share_non_tensor=share_non_tensor,
         ).lock_()
 
     def memmap(
         self,
         prefix: str | None = None,
         copy_existing: bool = False,
         *,
         num_threads: int = 0,
         return_early: bool = False,
+        share_non_tensor: bool = False,
     ) -> T:
         """Writes all tensors onto a corresponding memory-mapped Tensor in a new tensordict.
 
         Args:
             prefix (str): directory prefix where the memory-mapped tensors will
                 be stored. The directory tree structure will mimic the tensordict's.
             copy_existing (bool): If False (default), an exception will be raised if an
@@ -1988,14 +2068,20 @@
                 If ``True``, any existing Tensor will be copied to the new location.
 
         Keyword Args:
             num_threads (int, optional): the number of threads used to write the memmap
                 tensors. Defaults to `0`.
             return_early (bool, optional): if ``True`` and ``num_threads>0``,
                 the method will return a future of the tensordict.
+            share_non_tensor (bool, optional): if ``True``, the non-tensor data will be
+                shared between the processes and writing operation (such as inplace update
+                or set) on any of the workers within a single node will update the value
+                on all other workers. If the number of non-tensor leaves is high (e.g.,
+                sharing large stacks of non-tensor data) this may result in OOM or similar
+                errors. Defaults to ``False``.
 
         The TensorDict is then locked, meaning that any writing operations that
         isn't in-place will throw an exception (eg, rename, set or remove an
         entry).
         Once the tensordict is unlocked, the memory-mapped attribute is turned to ``False``,
         because cross-process identity is not guaranteed anymore.
 
@@ -2003,14 +2089,16 @@
             A new tensordict with the tensors stored on disk if ``return_early=False``,
             otherwise a :class:`~tensordict.utils.TensorDictFuture` instance.
 
         Note:
             Serialising in this fashion might be slow with deeply nested tensordicts, so
             it is not recommended to call this method inside a training loop.
         """
+        prefix = Path(prefix) if prefix is not None else None
+
         if num_threads > 1:
             with (
                 ThreadPoolExecutor(max_workers=num_threads)
                 if not return_early
                 else contextlib.nullcontext()
             ) as executor:
                 if return_early:
@@ -2019,36 +2107,40 @@
                 result = self._memmap_(
                     prefix=prefix,
                     copy_existing=copy_existing,
                     executor=executor,
                     futures=futures,
                     inplace=False,
                     like=False,
+                    share_non_tensor=share_non_tensor,
                 )
                 if not return_early:
                     concurrent.futures.wait(futures)
                     return result
                 else:
                     return TensorDictFuture(futures, result)
+
         return self._memmap_(
             prefix=prefix,
             copy_existing=copy_existing,
             inplace=False,
             executor=None,
             like=False,
             futures=None,
+            share_non_tensor=share_non_tensor,
         ).lock_()
 
     def memmap_like(
         self,
         prefix: str | None = None,
         copy_existing: bool = False,
         *,
         num_threads: int = 0,
         return_early: bool = False,
+        share_non_tensor: bool = False,
     ) -> T:
         """Creates a contentless Memory-mapped tensordict with the same shapes as the original one.
 
         Args:
             prefix (str): directory prefix where the memory-mapped tensors will
                 be stored. The directory tree structure will mimic the tensordict's.
             copy_existing (bool): If False (default), an exception will be raised if an
@@ -2058,14 +2150,20 @@
                 If ``True``, any existing Tensor will be copied to the new location.
 
         Keyword Args:
             num_threads (int, optional): the number of threads used to write the memmap
                 tensors. Defaults to `0`.
             return_early (bool, optional): if ``True`` and ``num_threads>0``,
                 the method will return a future of the tensordict.
+            share_non_tensor (bool, optional): if ``True``, the non-tensor data will be
+                shared between the processes and writing operation (such as inplace update
+                or set) on any of the workers within a single node will update the value
+                on all other workers. If the number of non-tensor leaves is high (e.g.,
+                sharing large stacks of non-tensor data) this may result in OOM or similar
+                errors. Defaults to ``False``.
 
         The TensorDict is then locked, meaning that any writing operations that
         isn't in-place will throw an exception (eg, rename, set or remove an
         entry).
         Once the tensordict is unlocked, the memory-mapped attribute is turned to ``False``,
         because cross-process identity is not guaranteed anymore.
 
@@ -2081,14 +2179,15 @@
             >>> td = TensorDict({
             ...     "a": torch.zeros((3, 64, 64), dtype=torch.uint8),
             ...     "b": torch.zeros(1, dtype=torch.int64),
             ... }, batch_size=[]).expand(1_000_000)  # expand does not allocate new memory
             >>> buffer = td.memmap_like("/path/to/dataset")
 
         """
+        prefix = Path(prefix) if prefix is not None else None
         if num_threads > 1:
             with (
                 ThreadPoolExecutor(max_workers=num_threads)
                 if not return_early
                 else contextlib.nullcontext()
             ) as executor:
                 if return_early:
@@ -2105,14 +2204,15 @@
                 result = input._memmap_(
                     prefix=prefix,
                     copy_existing=copy_existing,
                     executor=executor,
                     futures=futures,
                     inplace=False,
                     like=True,
+                    share_non_tensor=share_non_tensor,
                 )
                 if not return_early:
                     concurrent.futures.wait(futures)
                     return result
                 else:
                     return TensorDictFuture(futures, result)
         input = self.apply(
@@ -2121,14 +2221,15 @@
         return input._memmap_(
             prefix=prefix,
             copy_existing=copy_existing,
             inplace=False,
             like=True,
             executor=None,
             futures=None,
+            share_non_tensor=share_non_tensor,
         ).lock_()
 
     @classmethod
     def load_memmap(cls, prefix: str | Path) -> T:
         prefix = Path(prefix)
 
         def load_metadata(filepath):
@@ -2142,15 +2243,16 @@
             import tensordict
 
             for other_cls in tensordict.base._ACCEPTED_CLASSES:
                 if str(other_cls) == type_name:
                     return other_cls._load_memmap(prefix, metadata)
             else:
                 raise RuntimeError(
-                    f"Could not find name {type_name} in {tensordict.base._ACCEPTED_CLASSES}. Did you call _register_tensor_class(cls) on {type_name}?"
+                    f"Could not find name {type_name} in {tensordict.base._ACCEPTED_CLASSES}. "
+                    f"Did you call _register_tensor_class(cls) on {type_name}?"
                 )
         return cls._load_memmap(prefix, metadata)
 
     @classmethod
     @abc.abstractmethod
     def _load_memmap(cls, prefix: Path, metadata: dict):
         ...
@@ -2163,29 +2265,40 @@
         This method should be preferred to ``tensordict.get(key).shape`` whenever
         :meth:`.get` can be expensive to execute.
 
         """
         ...
 
     def set(
-        self, key: NestedKey, item: CompatibleType, inplace: bool = False, **kwargs: Any
+        self,
+        key: NestedKey,
+        item: CompatibleType,
+        inplace: bool = False,
+        *,
+        non_blocking: bool = False,
+        **kwargs: Any,
     ) -> T:
         """Sets a new key-value pair.
 
         Args:
             key (str, tuple of str): name of the key to be set.
             item (torch.Tensor or equivalent, TensorDictBase instance): value
                 to be stored in the tensordict.
             inplace (bool, optional): if ``True`` and if a key matches an existing
                 key in the tensordict, then the update will occur in-place
                 for that key-value pair. If inplace is ``True`` and
                 the entry cannot be found, it will be added. For a more restrictive
                 in-place operation, use :meth:`~.set_` instead.
                 Defaults to ``False``.
 
+        Keyword Args:
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
+
         Returns:
             self
 
         Examples:
             >>> td = TensorDict({}, batch_size[3, 4])
             >>> td.set("x", torch.randn(3, 4))
             >>> y = torch.randn(3, 4, 5)
@@ -2195,22 +2308,33 @@
             >>> td.set("y", torch.ones(5), inplace=True) # raises an exception as shapes mismatch
 
         """
         key = _unravel_key_to_tuple(key)
         # inplace is loose here, but for set_ it is constraining. We translate it
         # to None to tell _set_str and others to drop it if the key isn't found
         inplace = BEST_ATTEMPT_INPLACE if inplace else False
-        return self._set_tuple(key, item, inplace=inplace, validated=False)
+        return self._set_tuple(
+            key, item, inplace=inplace, validated=False, non_blocking=non_blocking
+        )
 
     @abc.abstractmethod
-    def _set_str(self, key, value, *, inplace, validated):
+    def _set_str(
+        self,
+        key: str,
+        value: Any,
+        *,
+        inplace: bool,
+        validated: bool,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
+    ):
         ...
 
     @abc.abstractmethod
-    def _set_tuple(self, key, value, *, inplace, validated):
+    def _set_tuple(self, key, value, *, inplace, validated, non_blocking: bool):
         ...
 
     @lock_blocked
     def set_non_tensor(self, key: NestedKey, value: Any):
         """Registers a non-tensor value in the tensordict using :class:`tensordict.tensorclass.NonTensorData`.
 
         The value can be retrieved using :meth:`TensorDictBase.get_non_tensor`
@@ -2252,14 +2376,15 @@
                 value,
                 batch_size=self.batch_size,
                 device=self.device,
                 names=self.names if self._has_names() else None,
             ),
             validated=True,
             inplace=False,
+            non_blocking=False,
         )
         return self
 
     def get_non_tensor(self, key: NestedKey, default=NO_DEFAULT):
         """Gets a non-tensor value, if it exists, or `default` if the non-tensor value is not found.
 
         This method is robust to tensor/TensorDict values, meaning that if the
@@ -2301,26 +2426,27 @@
             if len(key) == 1:
                 return self._get_non_tensor(key[0], default=default)
             subtd = self._get_str(key[0], default=default)
             if subtd is default:
                 return subtd
             return subtd._get_non_tensor(key[1:], default=default)
         value = self._get_str(key, default=default)
-        from tensordict.tensorclass import NonTensorData
 
-        if isinstance(value, NonTensorData):
-            return value.data
+        if is_non_tensor(value):
+            data = getattr(value, "data", None)
+            if data is None:
+                return value.tolist()
+            return data
         return value
 
     def filter_non_tensor_data(self) -> T:
         """Filters out all non-tensor-data."""
-        from tensordict.tensorclass import NonTensorData
 
         def _filter(x):
-            if not isinstance(x, NonTensorData):
+            if not is_non_tensor(x):
                 if is_tensor_collection(x):
                     return x.filter_non_tensor_data()
                 return x
 
         return self._apply_nest(_filter, call_on_nested=True)
 
     def _convert_inplace(self, inplace, key):
@@ -2329,68 +2455,91 @@
             if inplace is True and not has_key:  # inplace could be None
                 raise KeyError(
                     _KEY_ERROR.format(key, self.__class__.__name__, sorted(self.keys()))
                 )
             inplace = has_key
         return inplace
 
-    def set_at_(self, key: NestedKey, value: CompatibleType, index: IndexType) -> T:
+    def set_at_(
+        self,
+        key: NestedKey,
+        value: CompatibleType,
+        index: IndexType,
+        *,
+        non_blocking: bool = False,
+    ) -> T:
         """Sets the values in-place at the index indicated by ``index``.
 
         Args:
             key (str, tuple of str): key to be modified.
             value (torch.Tensor): value to be set at the index `index`
             index (int, tensor or tuple): index where to write the values.
 
+        Keyword Args:
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
+
         Returns:
             self
 
         Examples:
             >>> td = TensorDict({}, batch_size[3, 4])
             >>> x = torch.randn(3, 4)
             >>> td.set("x", x)
             >>> td.set_at_("x", value=torch.ones(1, 4), index=slice(1))
             >>> assert (x[0] == 1).all()
         """
         key = _unravel_key_to_tuple(key)
-        return self._set_at_tuple(key, value, index, validated=False)
+        return self._set_at_tuple(
+            key, value, index, validated=False, non_blocking=non_blocking
+        )
 
     @abc.abstractmethod
-    def _set_at_str(self, key, value, idx, *, validated):
+    def _set_at_str(self, key, value, idx, *, validated, non_blocking: bool):
         ...
 
     @abc.abstractmethod
-    def _set_at_tuple(self, key, value, idx, *, validated):
+    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
         ...
 
     def set_(
         self,
         key: NestedKey,
         item: CompatibleType,
+        *,
+        non_blocking: bool = False,
     ) -> T:
         """Sets a value to an existing key while keeping the original storage.
 
         Args:
             key (str): name of the value
             item (torch.Tensor or compatible type, TensorDictBase): value to
                 be stored in the tensordict
 
+        Keyword Args:
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
+
         Returns:
             self
 
         Examples:
             >>> td = TensorDict({}, batch_size[3, 4])
             >>> x = torch.randn(3, 4)
             >>> td.set("x", x)
             >>> td.set_("x", torch.zeros_like(x))
             >>> assert (x == 0).all()
 
         """
         key = _unravel_key_to_tuple(key)
-        return self._set_tuple(key, item, inplace=True, validated=False)
+        return self._set_tuple(
+            key, item, inplace=True, validated=False, non_blocking=non_blocking
+        )
 
     # Stack functionality
     @abc.abstractmethod
     def _stack_onto_(
         self,
         list_item: list[CompatibleType],
         dim: int,
@@ -2504,14 +2653,15 @@
 
     def update(
         self,
         input_dict_or_td: dict[str, CompatibleType] | T,
         clone: bool = False,
         inplace: bool = False,
         *,
+        non_blocking: bool = False,
         keys_to_update: Sequence[NestedKey] | None = None,
     ) -> T:
         """Updates the TensorDict with values from either a dictionary or another TensorDict.
 
         Args:
             input_dict_or_td (TensorDictBase or dict): input data to be written
                 in self.
@@ -2524,14 +2674,17 @@
                 added. Defaults to ``False``.
 
         Keyword Args:
             keys_to_update (sequence of NestedKeys, optional): if provided, only
                 the list of keys in ``key_to_update`` will be updated.
                 This is aimed at avoiding calls to
                 ``data_dest.update(data_src.select(*keys_to_update))``.
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
 
         Returns:
             self
 
         Examples:
             >>> td = TensorDict({}, batch_size=[3])
             >>> a = torch.randn(3)
@@ -2540,16 +2693,14 @@
             >>> td.update(other_td, inplace=True) # writes "a" and "b" even though they can't be found
             >>> assert td['a'] is other_td['a']
             >>> other_td = other_td.clone().zero_()
             >>> td.update(other_td)
             >>> assert td['a'] is not other_td['a']
 
         """
-        from tensordict._lazy import LazyStackedTensorDict
-
         if input_dict_or_td is self:
             # no op
             return self
         if keys_to_update is not None:
             if len(keys_to_update) == 0:
                 return self
             keys_to_update = unravel_key_list(keys_to_update)
@@ -2574,19 +2725,22 @@
                             keys_to_update, firstkey
                         )
                         target.update(
                             {subkey: value},
                             inplace=inplace,
                             clone=clone,
                             keys_to_update=sub_keys_to_update,
+                            non_blocking=non_blocking,
                         )
                         continue
                     elif isinstance(value, (dict,)) or _is_tensor_collection(
                         value.__class__
                     ):
+                        from tensordict._lazy import LazyStackedTensorDict
+
                         if isinstance(value, LazyStackedTensorDict) and not isinstance(
                             target, LazyStackedTensorDict
                         ):
                             sub_keys_to_update = _prune_selected_keys(
                                 keys_to_update, firstkey
                             )
                             self._set_tuple(
@@ -2595,42 +2749,47 @@
                                     *target.unbind(value.stack_dim),
                                     stack_dim=value.stack_dim,
                                 ).update(
                                     value,
                                     inplace=inplace,
                                     clone=clone,
                                     keys_to_update=sub_keys_to_update,
+                                    non_blocking=non_blocking,
                                 ),
                                 validated=True,
                                 inplace=False,
+                                non_blocking=non_blocking,
                             )
                         else:
                             sub_keys_to_update = _prune_selected_keys(
                                 keys_to_update, firstkey
                             )
                             target.update(
                                 value,
                                 inplace=inplace,
                                 clone=clone,
+                                non_blocking=non_blocking,
                                 keys_to_update=sub_keys_to_update,
                             )
                         continue
             self._set_tuple(
                 key,
                 value,
                 inplace=BEST_ATTEMPT_INPLACE if inplace else False,
                 validated=False,
+                non_blocking=non_blocking,
             )
         return self
 
     def update_(
         self,
         input_dict_or_td: dict[str, CompatibleType] | T,
         clone: bool = False,
         *,
+        non_blocking: bool = False,
         keys_to_update: Sequence[NestedKey] | None = None,
     ) -> T:
         """Updates the TensorDict in-place with values from either a dictionary or another TensorDict.
 
         Unlike :meth:`~.update`, this function will throw an error if the key is unknown to ``self``.
 
         Args:
@@ -2640,14 +2799,17 @@
                 tensor) dict should be cloned before being set. Defaults to ``False``.
 
         Keyword Args:
             keys_to_update (sequence of NestedKeys, optional): if provided, only
                 the list of keys in ``key_to_update`` will be updated.
                 This is aimed at avoiding calls to
                 ``data_dest.update_(data_src.select(*keys_to_update))``.
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
 
         Returns:
             self
 
         Examples:
             >>> a = torch.randn(3)
             >>> b = torch.randn(3, 4)
@@ -2662,56 +2824,57 @@
         if input_dict_or_td is self:
             # no op
             return self
         if keys_to_update is not None:
             if len(keys_to_update) == 0:
                 return self
             keys_to_update = [_unravel_key_to_tuple(key) for key in keys_to_update]
-        if keys_to_update:
+
             named = True
 
             def inplace_update(name, dest, source):
                 if source is None:
-                    return dest
+                    return None
                 name = _unravel_key_to_tuple(name)
                 for key in keys_to_update:
                     if key == name[: len(key)]:
-                        return dest.copy_(source, non_blocking=True)
-                else:
-                    return dest
+                        dest.copy_(source, non_blocking=non_blocking)
 
         else:
             named = False
 
             def inplace_update(dest, source):
                 if source is None:
-                    return dest
-                return dest.copy_(source, non_blocking=True)
+                    return None
+                dest.copy_(source, non_blocking=non_blocking)
 
-        if not is_tensor_collection(input_dict_or_td):
+        if not _is_tensor_collection(type(input_dict_or_td)):
             from tensordict import TensorDict
 
             input_dict_or_td = TensorDict.from_dict(
                 input_dict_or_td, batch_dims=self.batch_dims
             )
-        return self._apply_nest(
+        self._apply_nest(
             inplace_update,
             input_dict_or_td,
             nested_keys=True,
             default=None,
-            inplace=True,
+            filter_empty=True,
             named=named,
+            is_leaf=_is_leaf_nontensor,
         )
+        return self
 
     def update_at_(
         self,
         input_dict_or_td: dict[str, CompatibleType] | T,
         idx: IndexType,
         clone: bool = False,
         *,
+        non_blocking: bool = False,
         keys_to_update: Sequence[NestedKey] | None = None,
     ) -> T:
         """Updates the TensorDict in-place at the specified index with values from either a dictionary or another TensorDict.
 
         Unlike  TensorDict.update, this function will throw an error if the key is unknown to the TensorDict.
 
         Args:
@@ -2722,14 +2885,17 @@
             clone (bool, optional): whether the tensors in the input (
                 tensor) dict should be cloned before being set. Default is
                 `False`.
 
         Keyword Args:
             keys_to_update (sequence of NestedKeys, optional): if provided, only
                 the list of keys in ``key_to_update`` will be updated.
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
 
         Returns:
             self
 
         Examples:
             >>> td = TensorDict({
             ...     'a': torch.zeros(3, 4, 5),
@@ -2745,14 +2911,21 @@
                     b: Tensor(torch.Size([3, 4, 10]), dtype=torch.float32)},
                 batch_size=torch.Size([3, 4]),
                 device=None,
                 is_shared=False)
             >>> assert (td[1] == 1).all()
 
         """
+        if idx == ():
+            return self.update_(
+                input_dict_or_td=input_dict_or_td,
+                keys_to_update=keys_to_update,
+                clone=clone,
+                non_blocking=non_blocking,
+            )
         if keys_to_update is not None:
             if len(keys_to_update) == 0:
                 return self
             keys_to_update = unravel_key_list(keys_to_update)
         for key, value in input_dict_or_td.items():
             firstkey, *nextkeys = _unravel_key_to_tuple(key)
             if keys_to_update and not any(
@@ -2763,15 +2936,15 @@
             if not isinstance(value, _ACCEPTED_CLASSES):
                 raise TypeError(
                     f"Expected value to be one of types {_ACCEPTED_CLASSES} "
                     f"but got {type(value)}"
                 )
             if clone:
                 value = value.clone()
-            self.set_at_((firstkey, *nextkeys), value, idx)
+            self.set_at_((firstkey, *nextkeys), value, idx, non_blocking=non_blocking)
         return self
 
     @lock_blocked
     def create_nested(self, key):
         """Creates a nested tensordict of the same shape, device and dim names as the current tensordict.
 
         If the value already exists, it will be overwritten by this operation.
@@ -2812,35 +2985,33 @@
         """
         key = _unravel_key_to_tuple(key)
         self._create_nested_tuple(key)
         return self
 
     def _create_nested_str(self, key):
         out = self.empty()
-        self._set_str(key, out, inplace=False, validated=True)
+        self._set_str(key, out, inplace=False, validated=True, non_blocking=False)
         return out
 
     def _create_nested_tuple(self, key):
         td = self._create_nested_str(key[0])
         if len(key) > 1:
             td._create_nested_tuple(key[1:])
 
-    def copy_(self, tensordict: T, non_blocking: bool = None) -> T:
+    def copy_(self, tensordict: T, non_blocking: bool = False) -> T:
         """See :obj:`TensorDictBase.update_`.
 
         The non-blocking argument will be ignored and is just present for
         compatibility with :func:`torch.Tensor.copy_`.
         """
-        if non_blocking is False:
-            raise ValueError("non_blocking=False isn't supported in TensorDict.")
-        return self.update_(tensordict)
+        return self.update_(tensordict, non_blocking=non_blocking)
 
-    def copy_at_(self, tensordict: T, idx: IndexType) -> T:
+    def copy_at_(self, tensordict: T, idx: IndexType, non_blocking: bool = False) -> T:
         """See :obj:`TensorDictBase.update_at_`."""
-        return self.update_at_(tensordict, idx)
+        return self.update_at_(tensordict, idx, non_blocking=non_blocking)
 
     def is_empty(self) -> bool:
         """Checks if the tensordict contains any leaf."""
         for _ in self.keys(True, True):
             return False
         return True
 
@@ -3419,14 +3590,15 @@
 
     def _recv(
         self,
         src: int,
         _tag: int = -1,
         pseudo_rand: bool = False,
         group: "dist.ProcessGroup" | None = None,
+        non_blocking: bool = False,
     ) -> int:
         for key in self.sorted_keys:
             value = self._get_str(key, NO_DEFAULT)
             if isinstance(value, Tensor):
                 pass
             elif _is_tensor_collection(value.__class__):
                 _tag = value._recv(src, _tag=_tag, pseudo_rand=pseudo_rand, group=group)
@@ -3434,15 +3606,17 @@
             else:
                 raise NotImplementedError(f"Type {type(value)} is not supported.")
             if not pseudo_rand:
                 _tag += 1
             else:
                 _tag = int_generator(_tag + 1)
             dist.recv(value, src=src, tag=_tag, group=group)
-            self._set_str(key, value, inplace=True, validated=True)
+            self._set_str(
+                key, value, inplace=True, validated=True, non_blocking=non_blocking
+            )
 
         return _tag
 
     def isend(
         self,
         dst: int,
         *,
@@ -3725,14 +3899,16 @@
 
         Args:
             fn (Callable): function to be applied to the tensors in the
                 tensordict.
             *others (sequence of TensorDictBase, optional): the other
                 tensordicts to be used.
 
+        Keyword Args: See :meth:`~.apply`.
+
         Returns:
             self or a copy of self with the function applied
 
         """
         return self.apply(fn, *others, inplace=True, **kwargs)
 
     def apply(
@@ -3740,16 +3916,17 @@
         fn: Callable,
         *others: T,
         batch_size: Sequence[int] | None = None,
         device: torch.device | None = None,
         names: Sequence[str] | None = None,
         inplace: bool = False,
         default: Any = NO_DEFAULT,
+        filter_empty: bool | None = None,
         **constructor_kwargs,
-    ) -> T:
+    ) -> T | None:
         """Applies a callable to all values stored in the tensordict and sets them in a new tensordict.
 
         The callable signature must be ``Callable[Tuple[Tensor, ...], Optional[Union[Tensor, TensorDictBase]]]``.
 
         Args:
             fn (Callable): function to be applied to the tensors in the
                 tensordict.
@@ -3769,14 +3946,20 @@
             names (list of str, optional): the new dimension names, in case the
                 batch_size is modified.
             inplace (bool, optional): if True, changes are made in-place.
                 Default is False. This is a keyword only argument.
             default (Any, optional): default value for missing entries in the
                 other tensordicts. If not provided, missing entries will
                 raise a `KeyError`.
+            filter_empty (bool, optional): if ``True``, empty tensordicts will be
+                filtered out. This also comes with a lower computational cost as
+                empty data structures won't be created and destroyed. Non-tensor data
+                is considered as a leaf and thereby will be kept in the tensordict even
+                if left untouched by the function.
+                Defaults to ``False`` for backward compatibility.
             **constructor_kwargs: additional keyword arguments to be passed to the
                 TensorDict constructor.
 
         Returns:
             a new tensordict with transformed_in tensors.
 
         Example:
@@ -3826,29 +4009,31 @@
             *others,
             batch_size=batch_size,
             device=device,
             names=names,
             inplace=inplace,
             checked=False,
             default=default,
+            filter_empty=filter_empty,
             **constructor_kwargs,
         )
 
     def named_apply(
         self,
         fn: Callable,
         *others: T,
         nested_keys: bool = False,
         batch_size: Sequence[int] | None = None,
         device: torch.device | None = None,
         names: Sequence[str] | None = None,
         inplace: bool = False,
         default: Any = NO_DEFAULT,
+        filter_empty: bool | None = None,
         **constructor_kwargs,
-    ) -> T:
+    ) -> T | None:
         """Applies a key-conditioned callable to all values stored in the tensordict and sets them in a new atensordict.
 
         The callable signature must be ``Callable[Tuple[str, Tensor, ...], Optional[Union[Tensor, TensorDictBase]]]``.
 
         Args:
             fn (Callable): function to be applied to the (name, tensor) pairs in the
                 tensordict. For each leaf, only its leaf name will be used (not
@@ -3870,14 +4055,18 @@
             names (list of str, optional): the new dimension names, in case the
                 batch_size is modified.
             inplace (bool, optional): if True, changes are made in-place.
                 Default is False. This is a keyword only argument.
             default (Any, optional): default value for missing entries in the
                 other tensordicts. If not provided, missing entries will
                 raise a `KeyError`.
+            filter_empty (bool, optional): if ``True``, empty tensordicts will be
+                filtered out. This also comes with a lower computational cost as
+                empty data structures won't be created and destroyed. Defaults to
+                ``False`` for backward compatibility.
             **constructor_kwargs: additional keyword arguments to be passed to the
                 TensorDict constructor.
 
         Returns:
             a new tensordict with transformed_in tensors.
 
         Example:
@@ -3954,14 +4143,15 @@
             device=device,
             names=names,
             inplace=inplace,
             checked=False,
             default=default,
             named=True,
             nested_keys=nested_keys,
+            filter_empty=filter_empty,
             **constructor_kwargs,
         )
 
     @abc.abstractmethod
     def _apply_nest(
         self,
         fn: Callable,
@@ -3972,32 +4162,38 @@
         inplace: bool = False,
         checked: bool = False,
         call_on_nested: bool = False,
         default: Any = NO_DEFAULT,
         named: bool = False,
         nested_keys: bool = False,
         prefix: tuple = (),
+        filter_empty: bool | None = None,
+        is_leaf: Callable = None,
         **constructor_kwargs,
-    ) -> T:
+    ) -> T | None:
         ...
 
     def _fast_apply(
         self,
         fn: Callable,
         *others: T,
         batch_size: Sequence[int] | None = None,
         device: torch.device | None = None,
         names: Sequence[str] | None = None,
         inplace: bool = False,
         call_on_nested: bool = False,
         default: Any = NO_DEFAULT,
         named: bool = False,
         nested_keys: bool = False,
+        # filter_empty must be False because we use _fast_apply for all sorts of ops like expand etc
+        # and non-tensor data will disappear if we use True by default.
+        filter_empty: bool | None = False,
+        is_leaf: Callable = None,
         **constructor_kwargs,
-    ) -> T:
+    ) -> T | None:
         """A faster apply method.
 
         This method does not run any check after performing the func. This
         means that one to make sure that the metadata of the resulting tensors
         (device, shape etc.) match the :meth:`~.apply` ones.
 
         """
@@ -4009,14 +4205,16 @@
             names=names,
             inplace=inplace,
             checked=True,
             call_on_nested=call_on_nested,
             named=named,
             default=default,
             nested_keys=nested_keys,
+            filter_empty=filter_empty,
+            is_leaf=is_leaf,
             **constructor_kwargs,
         )
 
     def map(
         self,
         fn: Callable,
         dim: int = 0,
@@ -4667,15 +4865,15 @@
         """
         key = _unravel_key_to_tuple(key)
         data = self._get_tuple(key, NO_DEFAULT)
         if _is_tensor_collection(type(data)):
             data._fast_apply(lambda x: x.fill_(value), inplace=True)
         else:
             data = data.fill_(value)
-            self._set_tuple(key, data, inplace=True, validated=True)
+            self._set_tuple(key, data, inplace=True, validated=True, non_blocking=False)
         return self
 
     # Masking
     @abc.abstractmethod
     def masked_fill_(self, mask: Tensor, value: float | bool) -> T:
         """Fills the values corresponding to the mask with the desired value.
 
@@ -4878,15 +5076,19 @@
                     root_keys.discard(leaf)
             self.exclude(*root_keys, inplace=True)
             return self
         else:
             result = self.empty()
             for leaf, leaf_flat in zip(all_leaves, all_leaves_flat):
                 result._set_str(
-                    leaf_flat, self.get(leaf), validated=True, inplace=False
+                    leaf_flat,
+                    self.get(leaf),
+                    validated=True,
+                    inplace=False,
+                    non_blocking=False,
                 )
             # Uncomment if you want key operations to propagate the shared status
             # self._maybe_set_shared_attributes(result)
             if result._is_shared or result._is_memmap:
                 result.lock_()
             return result
 
@@ -5240,16 +5442,17 @@
     global _ACCEPTED_CLASSES
     _ACCEPTED_CLASSES = set(_ACCEPTED_CLASSES)
     _ACCEPTED_CLASSES.add(cls)
     _ACCEPTED_CLASSES = tuple(_ACCEPTED_CLASSES)
 
 
 def _is_tensor_collection(datatype):
-    out = _TENSOR_COLLECTION_MEMO.get(datatype, None)
-    if out is None:
+    try:
+        out = _TENSOR_COLLECTION_MEMO[datatype]
+    except KeyError:
         if issubclass(datatype, TensorDictBase):
             out = True
         elif _is_tensorclass(datatype):
             out = True
         else:
             out = False
         _TENSOR_COLLECTION_MEMO[datatype] = out
@@ -5281,14 +5484,12 @@
 
 
 def _default_is_leaf(cls: Type) -> bool:
     return not _is_tensor_collection(cls)
 
 
 def _is_leaf_nontensor(cls: Type) -> bool:
-    from tensordict.tensorclass import NonTensorData
-
-    if issubclass(cls, KeyedJaggedTensor):
-        return False
     if _is_tensor_collection(cls):
-        return issubclass(cls, NonTensorData)
+        return cls._is_non_tensor
+    # if issubclass(cls, KeyedJaggedTensor):
+    #     return False
     return issubclass(cls, torch.Tensor)
```

## tensordict/persistent.py

```diff
@@ -511,15 +511,19 @@
 
     def empty(self, recurse=False) -> T:
         if recurse:
             out = self.empty(recurse=False)
             for key, val in self.items():
                 if is_tensor_collection(val):
                     out._set_str(
-                        key, val.empty(recurse=True), inplace=False, validated=True
+                        key,
+                        val.empty(recurse=True),
+                        inplace=False,
+                        validated=True,
+                        non_blocking=False,
                     )
             return out
         return TensorDict(
             {},
             device=self.device,
             batch_size=self.batch_size,
             names=self.names if self._has_names() else None,
@@ -572,14 +576,15 @@
         *,
         prefix: str | None,
         copy_existing: bool,
         executor,
         futures,
         inplace,
         like,
+        share_non_tensor,
     ) -> T:
         if inplace:
             raise RuntimeError("Cannot call memmap inplace in a persistent tensordict.")
 
         # re-implements this to make it faster using the meta-data
         def save_metadata(data: TensorDictBase, filepath, metadata=None):
             if metadata is None:
@@ -619,17 +624,19 @@
                     value._memmap_(
                         prefix=prefix / key if prefix is not None else None,
                         executor=executor,
                         like=like,
                         copy_existing=copy_existing,
                         futures=futures,
                         inplace=inplace,
+                        share_non_tensor=share_non_tensor,
                     ),
                     inplace=False,
                     validated=True,
+                    non_blocking=False,
                 )
                 continue
             else:
                 value = self._get_str(key)
                 if prefix is not None:
                     metadata[key] = {
                         "dtype": str(value.dtype),
@@ -650,14 +657,15 @@
                         existsok=True,
                     )
                     tensordict._set_str(
                         key,
                         val,
                         inplace=False,
                         validated=True,
+                        non_blocking=False,
                     )
 
                 if executor is None:
                     _populate()
                 else:
                     futures.append(executor.submit(_populate))
 
@@ -843,25 +851,28 @@
             )
         return out
 
     def _set(
         self,
         key: NestedKey,
         value: Any,
+        *,
         inplace: bool = False,
         idx=None,
-        validated=False,
+        validated: bool = False,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
     ) -> PersistentTensorDict:
         if not validated:
             value = self._validate_value(value, check_shape=idx is None)
         value = self._to_numpy(value)
         if not inplace:
             if idx is not None:
                 raise RuntimeError("Cannot pass an index to _set when inplace=False.")
-            elif self.is_locked:
+            elif self.is_locked and not ignore_lock:
                 raise RuntimeError(_LOCK_ERROR)
         # shortcut set if we're placing a tensordict
         if is_tensor_collection(value):
             if isinstance(key, tuple):
                 key, subkey = key[0], key[1:]
             else:
                 key, subkey = key, []
@@ -945,33 +956,75 @@
         return inplace
 
     def _set_non_tensor(self, key: NestedKey, value: Any):
         raise NotImplementedError(
             f"set_non_tensor is not compatible with the tensordict type {type(self)}."
         )
 
-    def _set_str(self, key, value, *, inplace, validated):
+    def _set_str(
+        self,
+        key: str,
+        value: Any,
+        *,
+        inplace: bool,
+        validated: bool,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
+    ):
         inplace = self._convert_inplace(inplace, key)
-        return self._set(key, value, inplace=inplace, validated=validated)
+        return self._set(
+            key,
+            value,
+            inplace=inplace,
+            validated=validated,
+            ignore_lock=ignore_lock,
+            non_blocking=non_blocking,
+        )
 
-    def _set_tuple(self, key, value, *, inplace, validated):
+    def _set_tuple(self, key, value, *, inplace, validated, non_blocking):
         if len(key) == 1:
-            return self._set_str(key[0], value, inplace=inplace, validated=validated)
+            return self._set_str(
+                key[0],
+                value,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
+            )
         elif key[0] in self.keys():
             return self._get_str(key[0])._set_tuple(
-                key[1:], value, inplace=inplace, validated=validated
+                key[1:],
+                value,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
             )
         inplace = self._convert_inplace(inplace, key)
-        return self._set(key, value, inplace=inplace, validated=validated)
-
-    def _set_at_str(self, key, value, idx, *, validated):
-        return self._set(key, value, inplace=True, idx=idx, validated=validated)
+        return self._set(
+            key, value, inplace=inplace, validated=validated, non_blocking=non_blocking
+        )
 
-    def _set_at_tuple(self, key, value, idx, *, validated):
-        return self._set(key, value, inplace=True, idx=idx, validated=validated)
+    def _set_at_str(self, key, value, idx, *, validated, non_blocking):
+        return self._set(
+            key,
+            value,
+            inplace=True,
+            idx=idx,
+            validated=validated,
+            non_blocking=non_blocking,
+        )
+
+    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking):
+        return self._set(
+            key,
+            value,
+            inplace=True,
+            idx=idx,
+            validated=validated,
+            non_blocking=non_blocking,
+        )
 
     def _set_metadata(self, orig_metadata_container: PersistentTensorDict):
         for key, td in orig_metadata_container._nested_tensordicts.items():
             array = self._get_array(key)
             self._nested_tensordicts[key] = PersistentTensorDict(
                 group=array,
                 batch_size=td.batch_size,
@@ -1109,14 +1162,15 @@
     expand = TensorDict.expand
     masked_select = TensorDict.masked_select
     reshape = TensorDict.reshape
     split = TensorDict.split
     _to_module = TensorDict._to_module
     _unbind = TensorDict._unbind
     _get_names_idx = TensorDict._get_names_idx
+    from_dict_instance = TensorDict.from_dict_instance
 
 
 def _set_max_batch_size(source: PersistentTensorDict):
     """Updates a tensordict with its maximium batch size."""
     tensor_data = list(source._items_metadata())
     for key, val in tensor_data:
         if not val["array"]:
```

## tensordict/tensorclass.py

```diff
@@ -1,65 +1,73 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
 from __future__ import annotations
 
+import ctypes
+
 import dataclasses
 import functools
 import inspect
 import json
+import multiprocessing.managers
+import multiprocessing.sharedctypes
 import numbers
 import os
 import pickle
-import re
-import sys
+import shutil
 import warnings
-from copy import copy
+from copy import copy, deepcopy
 from dataclasses import dataclass
 from pathlib import Path
 from textwrap import indent
-from typing import Any, Callable, List, Sequence, TypeVar
+from typing import Any, Callable, get_type_hints, List, Sequence, TypeVar
 
 import tensordict as tensordict_lib
 
 import torch
+from tensordict import LazyStackedTensorDict
 from tensordict._td import is_tensor_collection, NO_DEFAULT, TensorDict, TensorDictBase
 from tensordict._tensordict import _unravel_key_to_tuple
 from tensordict._torch_func import TD_HANDLED_FUNCTIONS
-from tensordict.base import _ACCEPTED_CLASSES, _register_tensor_class
+from tensordict.base import (
+    _ACCEPTED_CLASSES,
+    _is_tensor_collection,
+    _register_tensor_class,
+    CompatibleType,
+)
 from tensordict.memmap_deprec import MemmapTensor as _MemmapTensor
-
 from tensordict.utils import (
     _get_repr,
     _is_json_serializable,
     _LOCK_ERROR,
     DeviceType,
     IndexType,
+    is_non_tensor,
     is_tensorclass,
     NestedKey,
 )
-from torch import Tensor
+from torch import multiprocessing as mp, Tensor
+from torch.multiprocessing import Manager
 
 T = TypeVar("T", bound=TensorDictBase)
-PY37 = sys.version_info < (3, 8)
-
-# Regex precompiled patterns
-OPTIONAL_PATTERN = re.compile(r"Optional\[(.*?)\]")
-UNION_PATTERN = re.compile(r"Union\[(.*?)\]")
 
 # methods where non_tensordict data should be cleared in the return value
 _CLEAR_METADATA = {"all", "any"}
 # torch functions where we can wrap the corresponding TensorDict version
 _TD_PASS_THROUGH = {
     torch.unbind,
     torch.full_like,
     torch.zeros_like,
     torch.ones_like,
+    torch.rand_like,
+    torch.empty_like,
+    torch.randn_like,
     torch.clone,
     torch.squeeze,
     torch.unsqueeze,
     torch.split,
     torch.permute,
     torch.split,
     torch.stack,
@@ -154,19 +162,21 @@
         if isinstance(result, (list, tuple)):
             return result.__class__(
                 _from_tensordict_with_copy(tensorclass_instance, tensordict_result)
                 for tensordict_result in result
             )
         return _from_tensordict_with_copy(tensorclass_instance, result)
 
+    _is_non_tensor = getattr(cls, "_is_non_tensor", False)
+
     cls = dataclass(cls)
     expected_keys = set(cls.__dataclass_fields__)
 
     for attr in cls.__dataclass_fields__:
-        if attr in dir(TensorDict):
+        if attr in dir(TensorDict) and attr != "_is_non_tensor":
             raise AttributeError(
                 f"Attribute name {attr} can't be used with @tensorclass"
             )
 
     cls.__init__ = _init_wrapper(cls.__init__)
     cls._from_tensordict = classmethod(_from_tensordict_wrapper(expected_keys))
     cls.from_tensordict = cls._from_tensordict
@@ -182,48 +192,94 @@
     cls.__setitem__ = _setitem
     cls.__repr__ = _repr
     cls.__len__ = _len
     cls.__eq__ = _eq
     cls.__ne__ = _ne
     cls.__or__ = _or
     cls.__xor__ = _xor
-    cls.set = _set
-    cls.set_at_ = _set_at_
-    cls.del_ = _del_
-    cls.get = _get
-    cls.get_at = _get_at
-    cls.unbind = _unbind
-    cls.state_dict = _state_dict
-    cls.load_state_dict = _load_state_dict
-    cls._memmap_ = _memmap_
+    cls.__bool__ = _bool
+    # if not hasattr(cls, "keys"):
+    #     cls.keys = _keys
+    # if not hasattr(cls, "values"):
+    #     cls.values = _values
+    # if not hasattr(cls, "items"):
+    #     cls.items = _items
+    if not hasattr(cls, "set"):
+        cls.set = _set
+    if not hasattr(cls, "set_at_"):
+        cls.set_at_ = _set_at_
+    if not hasattr(cls, "del_"):
+        cls.del_ = _del_
+    if not hasattr(cls, "get"):
+        cls.get = _get
+    if not hasattr(cls, "get_at"):
+        cls.get_at = _get_at
+    if not hasattr(cls, "unbind"):
+        cls.unbind = _unbind
+    cls._unbind = _unbind
+    if not hasattr(cls, "state_dict"):
+        cls.state_dict = _state_dict
+    if not hasattr(cls, "load_state_dict"):
+        cls.load_state_dict = _load_state_dict
+    if not hasattr(cls, "_memmap_"):
+        cls._memmap_ = _memmap_
+    if not hasattr(cls, "share_memory_"):
+        cls.share_memory_ = _share_memory_
+    if not hasattr(cls, "update"):
+        cls.update = _update
+    if not hasattr(cls, "update_"):
+        cls.update_ = _update_
+    if not hasattr(cls, "update_at_"):
+        cls.update_at_ = _update_at_
 
     cls.__enter__ = __enter__
     cls.__exit__ = __exit__
 
     # Memmap
-    cls.memmap_like = TensorDictBase.memmap_like
-    cls.memmap_ = TensorDictBase.memmap_
-    cls.memmap = TensorDictBase.memmap
-    cls.load_memmap = TensorDictBase.load_memmap
-    cls._load_memmap = classmethod(_load_memmap)
+    if not hasattr(cls, "memmap_like"):
+        cls.memmap_like = TensorDictBase.memmap_like
+    if not hasattr(cls, "memmap_"):
+        cls.memmap_ = TensorDictBase.memmap_
+    if not hasattr(cls, "memmap"):
+        cls.memmap = TensorDictBase.memmap
+    if not hasattr(cls, "load_memmap"):
+        cls.load_memmap = TensorDictBase.load_memmap
+    if not hasattr(cls, "_load_memmap"):
+        cls._load_memmap = classmethod(_load_memmap)
+    if not hasattr(cls, "from_dict"):
+        cls.from_dict = classmethod(_from_dict)
+    if not hasattr(cls, "from_dict_instance"):
+        cls.from_dict_instance = _from_dict_instance
 
     for attr in TensorDict.__dict__.keys():
         func = getattr(TensorDict, attr)
         if inspect.ismethod(func) and attr not in cls.__dict__:
             tdcls = func.__self__
             if issubclass(tdcls, TensorDictBase):  # detects classmethods
                 setattr(cls, attr, _wrap_classmethod(tdcls, cls, func))
 
-    cls.to_tensordict = _to_tensordict
-    cls.device = property(_device, _device_setter)
-    cls.batch_size = property(_batch_size, _batch_size_setter)
+    if not hasattr(cls, "to_tensordict"):
+        cls.to_tensordict = _to_tensordict
+    if not hasattr(cls, "device"):
+        cls.device = property(_device, _device_setter)
+    if not hasattr(cls, "batch_size"):
+        cls.batch_size = property(_batch_size, _batch_size_setter)
+    if not hasattr(cls, "names"):
+        cls.names = property(_names, _names_setter)
+    if not hasattr(cls, "to_dict"):
+        cls.to_dict = _to_dict
 
     cls.__doc__ = f"{cls.__name__}{inspect.signature(cls)}"
 
     _register_tensor_class(cls)
+
+    # faster than doing instance checks
+    cls._is_non_tensor = _is_non_tensor
+    cls._is_tensorclass = True
+
     return cls
 
 
 def _arg_to_tensordict(arg):
     # if arg is a tensorclass or sequence of tensorclasses, extract the underlying
     # tensordicts and return those instead
     if is_tensorclass(arg):
@@ -256,24 +312,27 @@
     # drop first entry of params which corresponds to self and isn't passed by the user
     required_params = [p.name for p in params[1:] if p.default is inspect._empty]
 
     @functools.wraps(init)
     def wrapper(
         self,
         *args: Any,
-        batch_size: Sequence[int] | torch.Size | int,
+        batch_size: Sequence[int] | torch.Size | int = None,
         device: DeviceType | None = None,
         names: List[str] | None = None,
         **kwargs,
     ):
+        _get_type_hints(type(self))
+
         for value, key in zip(args, self.__dataclass_fields__):
             if key in kwargs:
                 raise ValueError(f"The key {key} is already set in kwargs")
             kwargs[key] = value
-
+        if batch_size is None:
+            batch_size = torch.Size([])
         for key, field in self.__dataclass_fields__.items():
             if field.default_factory is not dataclasses.MISSING:
                 default = field.default_factory()
             else:
                 default = field.default
             if default not in (None, dataclasses.MISSING):
                 kwargs.setdefault(key, default)
@@ -290,28 +349,114 @@
         self._tensordict = TensorDict(
             {},
             batch_size=torch.Size(batch_size),
             device=device,
             names=names,
             _run_checks=False,
         )
-        # To save non tensor data (Nested tensor classes also go here)
         self._non_tensordict = {}
+
         init(self, **kwargs)
 
     new_params = [
         inspect.Parameter("batch_size", inspect.Parameter.KEYWORD_ONLY),
         inspect.Parameter("device", inspect.Parameter.KEYWORD_ONLY, default=None),
         inspect.Parameter("names", inspect.Parameter.KEYWORD_ONLY, default=None),
     ]
     wrapper.__signature__ = init_sig.replace(parameters=params + new_params)
 
     return wrapper
 
 
+def _get_type_hints(cls, with_locals=False):
+    #######
+    # Set proper type annotations for autocasting to tensordict/tensorclass
+    #
+    # by updating locals, we can allow this to be used within a function
+    # local-cross referencing will not work though
+    # def foo():
+    #     @tensorclass
+    #     class MyOtherClass:
+    #         x: torch.Tensor
+    #     @tensorclass
+    #     class MyClass:
+    #         x: MyClass # works
+    #         y: MyOtherClass # fails
+    #
+    # In this case, we will use the get_parent_local function to get the locals
+    # from the parent frame and so recursively until we can find the class.
+
+    if with_locals:
+        # This function gets the parent frame recursively until we can find the current class.
+        # Any exception leads to this to be None and auto-casting will be disabled
+        localns = locals()
+        localns = copy(localns)
+
+        def get_parent_locals(cls, localns=localns):
+            # Get the current frame
+            frame = inspect.currentframe()
+            try:
+                parent_locs = localns
+                while cls.__name__ not in parent_locs:
+                    # Get the parent frame
+                    parent_frame = frame.f_back
+                    # Get the locals dictionary of the parent frame
+                    parent_locs = parent_frame.f_locals
+                    frame = parent_frame
+            except Exception:
+                localns.setdefault(cls.__name__, cls)
+                return localns
+            finally:
+                # Clean up the frame reference
+                del frame
+            return copy(parent_locs)
+
+        localns = get_parent_locals(cls)
+    else:
+        localns = None
+
+    globalns = None
+
+    try:
+        cls._type_hints = get_type_hints(
+            cls,
+            localns=localns,
+            # globalns=globals(),
+        )
+    except NameError:
+        if not with_locals:
+            return _get_type_hints(cls, with_locals=True)
+        cls._set_dict_warn_msg = (
+            "A NameError occurred while trying to retrieve a type annotation. "
+            "This can occur when a tensorclass references another locally defined "
+            "tensorclass. "
+            f"As a result type hints cannot be read and {cls}.from_dict(...) "
+            f"or `{cls}.set` will not attempt to map dictionaries to "
+            "the relevant tensorclass. To resolve this issue, consider defining "
+            "your tensorclass globally."
+        )
+        cls._type_hints = None
+    except TypeError:
+        # This is a rather common case where type annotation is like
+        # class MyClass:
+        #     x: int | str
+        # in which case get_type_hints doesn't work (it does work
+        # however with old-school Optional or Union...)
+        # We simply differ the warning till _set() is called
+        cls._set_dict_warn_msg = (
+            "A TypeError occurred when trying to retrieve a type annotation. "
+            "This may be caused by annotations that use plain `|` instead of typing.Union "
+            "or typing.Optional which are supported. If you wish to use the feature "
+            "of setting dict as attributes with automapping to tensordict/tensorclass "
+            "(`my_obj.attr = dict(...)`), consider re-writing the tensorclass with "
+            "traditional type annotations."
+        )
+        cls._type_hints = None
+
+
 def _from_tensordict_wrapper(expected_keys):
     def wrapper(cls, tensordict, non_tensordict=None):  # noqa: D417
         """Tensor class wrapper to instantiate a new tensor class object.
 
         Args:
             tensordict (TensorDict): Dictionary of tensor types
             non_tensordict (dict): Dictionary with non-tensor and nested tensor class objects
@@ -358,34 +503,38 @@
         return tc
 
     return wrapper
 
 
 def _memmap_(
     self,
+    *,
     prefix: str | None = None,
     copy_existing: bool = False,
     executor=None,
     futures=None,
     inplace=True,
     like=False,
+    memmaped: bool = False,
+    share_non_tensor: bool = False,
 ):
-    _non_tensordict = self._non_tensordict
+    _non_tensordict = copy(self._non_tensordict)
     cls = self.__class__
 
-    if prefix is not None:
+    if not memmaped and prefix is not None:
         prefix = Path(prefix)
         if not prefix.exists():
             os.makedirs(prefix, exist_ok=True)
 
         def save_metadata(cls=cls, _non_tensordict=_non_tensordict, prefix=prefix):
             with open(prefix / "meta.json", "w") as f:
                 metadata = {"_type": str(cls)}
                 to_pickle = {}
                 for key, value in _non_tensordict.items():
+                    value = _from_shared_nontensor(value)
                     if _is_json_serializable(value):
                         metadata[key] = value
                     else:
                         to_pickle[key] = value
                 json.dump(metadata, f)
                 if to_pickle:
                     with open(prefix / "other.pickle", "wb") as pickle_file:
@@ -401,23 +550,29 @@
     td = self._tensordict._memmap_(
         prefix=prefix,
         executor=executor,
         futures=futures,
         inplace=inplace,
         like=like,
         copy_existing=copy_existing,
+        share_non_tensor=share_non_tensor,
     )
-
+    td._device = torch.device("cpu")
     if not inplace:
         result = cls._from_tensordict(td, _non_tensordict)
     else:
         result = self
     return result
 
 
+def _share_memory_(self):
+    self._tensordict.share_memory_()
+    return self
+
+
 def _load_memmap(cls, prefix: Path, metadata: dict):
     non_tensordict = copy(metadata)
     del non_tensordict["_type"]
     if os.path.exists(prefix / "other.pickle"):
         with open(prefix / "other.pickle", "rb") as pickle_file:
             non_tensordict.update(pickle.load(pickle_file))
     td = TensorDict.load_memmap(prefix / "_tensordict")
@@ -473,21 +628,27 @@
                 out = self._tensordict.get(item)
                 return out
             elif (
                 "_non_tensordict" in self.__dict__
                 and item in self.__dict__["_non_tensordict"]
             ):
                 out = self._non_tensordict[item]
+                if (
+                    isinstance(self, NonTensorData)
+                    and item == "data"
+                    and (self._is_shared or self._is_memmap)
+                ):
+                    return _from_shared_nontensor(out)
                 return out
         return getattribute(self, item)
 
     return wrapper
 
 
-SET_ATTRIBUTES = ("batch_size", "device", "_locked_tensordicts")
+SET_ATTRIBUTES = ("batch_size", "device", "_locked_tensordicts", "names")
 
 
 def _setattr_wrapper(setattr_: Callable, expected_keys: set[str]) -> Callable:
     @functools.wraps(setattr_)
     def wrapper(self, key: str, value: Any) -> None:  # noqa: D417
         """Set the value of an attribute for the tensor class object.
 
@@ -533,14 +694,113 @@
             # create a new tensorclass from res and copy the metadata from self
             return self._from_tensordict(res, copy(self._non_tensordict))
         return res
 
     return wrapped_func
 
 
+def _update(
+    self,
+    input_dict_or_td: dict[str, CompatibleType] | T,
+    clone: bool = False,
+    inplace: bool = False,
+    *,
+    keys_to_update: Sequence[NestedKey] | None = None,
+    non_blocking: bool = False,
+):
+    if isinstance(input_dict_or_td, dict):
+        input_dict_or_td = self.from_dict(input_dict_or_td)
+
+    if is_tensorclass(input_dict_or_td):
+        self._tensordict.update(input_dict_or_td._tensordict)
+        self._non_tensordict.update(input_dict_or_td._non_tensordict)
+        return self
+
+    non_tensordict = {}
+    for key, value in input_dict_or_td.items():
+        if is_non_tensor(value):
+            non_tensordict[key] = value.data
+
+    self._tensordict.update(
+        input_dict_or_td.exclude(*non_tensordict.keys()),
+        clone=clone,
+        inplace=inplace,
+        keys_to_update=keys_to_update,
+        non_blocking=non_blocking,
+    )
+    self._non_tensordict.update(non_tensordict)
+    return self
+
+
+def _update_(
+    self,
+    input_dict_or_td: dict[str, CompatibleType] | T,
+    clone: bool = False,
+    inplace: bool = False,
+    *,
+    keys_to_update: Sequence[NestedKey] | None = None,
+    non_blocking: bool = False,
+):
+    if isinstance(input_dict_or_td, dict):
+        input_dict_or_td = self.from_dict(input_dict_or_td, batch_size=self.batch_size)
+
+    if is_tensorclass(input_dict_or_td):
+        self._tensordict.update(input_dict_or_td._tensordict)
+        self._non_tensordict.update(input_dict_or_td._non_tensordict)
+        return self
+
+    non_tensordict = {}
+    for key, value in input_dict_or_td.items():
+        if is_non_tensor(value):
+            non_tensordict[key] = value.data
+
+    self._tensordict.update_(
+        input_dict_or_td.exclude(*non_tensordict.keys()),
+        clone=clone,
+        inplace=inplace,
+        keys_to_update=keys_to_update,
+        non_blocking=non_blocking,
+    )
+    self._non_tensordict.update(non_tensordict)
+    return self
+
+
+def _update_at_(
+    self,
+    input_dict_or_td: dict[str, CompatibleType] | T,
+    index: IndexType,
+    clone: bool = False,
+    *,
+    keys_to_update: Sequence[NestedKey] | None = None,
+    non_blocking: bool = False,
+):
+    if isinstance(input_dict_or_td, dict):
+        input_dict_or_td = self.from_dict(input_dict_or_td, batch_size=self.batch_size)
+
+    if is_tensorclass(input_dict_or_td):
+        self._tensordict.update(input_dict_or_td._tensordict)
+        self._non_tensordict.update(input_dict_or_td._non_tensordict)
+        return self
+
+    non_tensordict = {}
+    for key, value in input_dict_or_td.items():
+        if is_non_tensor(value):
+            non_tensordict[key] = value.data
+
+    self._tensordict.update_at_(
+        input_dict_or_td.exclude(*non_tensordict.keys()),
+        index=index,
+        clone=clone,
+        keys_to_update=keys_to_update,
+        non_blocking=non_blocking,
+    )
+    self._non_tensordict.update(non_tensordict)
+    return self
+
+
 def _wrap_classmethod(td_cls, cls, func):
     @functools.wraps(func)
     def wrapped_func(*args, **kwargs):
         res = func.__get__(td_cls)(*args, **kwargs)
         # res = func(*args, **kwargs)
         if isinstance(res, TensorDictBase):
             # create a new tensorclass from res and copy the metadata from self
@@ -635,20 +895,26 @@
             # Making sure that the key-clashes won't happen, if the key is present
             # in tensor data in value we will honor that and remove the key-value
             # pair from non-tensor data
             if key in self._non_tensordict.keys():
                 del self._non_tensordict[key]
 
         self._tensordict[item] = value._tensordict
-    else:  # it is one of accepted "broadcast" types
+    elif isinstance(value, TensorDictBase):  # it is one of accepted "broadcast" types
         # attempt broadcast on all tensordata and nested tensorclasses
+        self._tensordict[item] = value.filter_non_tensor_data()
+        self._non_tensordict.update(
+            {
+                key: val.data
+                for key, val in value.items(is_leaf=is_non_tensor, leaves_only=True)
+            }
+        )
+    else:
+        # int, float etc.
         self._tensordict[item] = value
-        for key, val in self._non_tensordict.items():
-            if is_tensorclass(val):
-                _setitem(self._non_tensordict[key], item, value)
 
 
 def _repr(self) -> str:
     """Return a string representation of Tensor class object."""
     fields = _all_td_fields_as_str(self._tensordict)
     field_str = [fields] if fields else []
     non_tensor_fields = _all_non_td_fields_as_str(self._non_tensordict)
@@ -670,25 +936,94 @@
 
 
 def _len(self) -> int:
     """Returns the length of first dimension, if there is, otherwise 0."""
     return len(self._tensordict)
 
 
+def _to_dict(self) -> dict:
+    td_dict = self._tensordict.to_dict()
+    td_dict.update(self._non_tensordict)
+    return td_dict
+
+
+def _from_dict(cls, input_dict, batch_size=None, device=None, batch_dims=None):
+    # we pass through a tensordict because keys could be passed as NestedKeys
+    # We can't assume all keys are strings, otherwise calling cls(**kwargs)
+    # would work ok
+
+    td = TensorDict.from_dict(
+        input_dict, batch_size=batch_size, device=device, batch_dims=batch_dims
+    )
+    non_tensor = {}
+
+    for key, value in list(td.items()):
+        if is_non_tensor(value):
+            non_tensor[key] = value.data
+            del td[key]
+
+    return cls.from_tensordict(tensordict=td, non_tensordict=non_tensor)
+
+
+def _from_dict_instance(
+    self, input_dict, batch_size=None, device=None, batch_dims=None
+):
+    if batch_dims is not None and batch_size is not None:
+        raise ValueError("Cannot pass both batch_size and batch_dims to `from_dict`.")
+    from tensordict import TensorDict
+
+    batch_size_set = torch.Size(()) if batch_size is None else batch_size
+    # TODO: this is a bit slow and will be a bottleneck every time td[idx] = dict(subtd)
+    # is called when there are non tensor data in it
+    if not _is_tensor_collection(type(input_dict)):
+        input_tdict = TensorDict.from_dict(input_dict)
+    else:
+        input_tdict = input_dict
+    trsf_dict = {}
+    for key, value in list(input_tdict.items()):
+        # cur_value = getattr(self, key, None)
+        cur_value = self.get(key, None)
+        if _is_tensor_collection(type(cur_value)):
+            trsf_dict[key] = cur_value.from_dict_instance(
+                value, batch_size=[], device=device, batch_dims=None
+            )
+        elif not isinstance(cur_value, torch.Tensor) and is_non_tensor(value):
+            trsf_dict[key] = value.data
+        elif cur_value is not None and not isinstance(cur_value, torch.Tensor):
+            # This is slightly unsafe but will work with bool, float and int
+            try:
+                trsf_dict[key] = type(cur_value)(value)
+            except Exception:
+                trsf_dict[key] = input_dict[key]
+        else:
+            trsf_dict[key] = value
+    out = type(self)(
+        **trsf_dict,
+        batch_size=batch_size_set,
+        device=device,
+    )
+    # check that
+    if batch_size is None:
+        out._tensordict.auto_batch_size_()
+    return out
+
+
 def _to_tensordict(self) -> TensorDict:
     """Convert the tensorclass into a regular TensorDict.
 
     Makes a copy of all entries. Memmap and shared memory tensors are converted to
     regular tensors.
 
     Returns:
         A new TensorDict object containing the same values as the tensorclass.
 
     """
     td = self._tensordict.to_tensordict()
+    for key, val in self._non_tensordict.items():
+        td.set_non_tensor(key, val)
     return td
 
 
 def _device(self) -> torch.device:
     """Retrieves the device type of tensor class."""
     return self._tensordict.device
 
@@ -698,15 +1033,17 @@
         "device cannot be set using tensorclass.device = device, "
         "because device cannot be updated in-place. To update device, use "
         "tensorclass.to(new_device), which will return a new tensorclass "
         "on the new device."
     )
 
 
-def _set(self, key: NestedKey, value: Any, inplace: bool = False):
+def _set(
+    self, key: NestedKey, value: Any, inplace: bool = False, non_blocking: bool = False
+):
     """Sets a new key-value pair.
 
     Args:
         key (str, tuple of str): name of the key to be set.
            If tuple of str it is equivalent to chained calls of getattr
            followed by a final setattr.
         value (Any): value to be stored in the tensorclass
@@ -718,39 +1055,55 @@
         self
 
     """
     if isinstance(key, str):
         __dict__ = self.__dict__
         if __dict__["_tensordict"].is_locked:
             raise RuntimeError(_LOCK_ERROR)
+        if key in ("batch_size", "names", "device"):
+            # handled by setattr
+            return
         expected_keys = self.__dataclass_fields__
         if key not in expected_keys:
             raise AttributeError(
                 f"Cannot set the attribute '{key}', expected attributes are {expected_keys}."
             )
 
         if isinstance(value, tuple(tensordict_lib.base._ACCEPTED_CLASSES)):
             # Avoiding key clash, honoring the user input to assign tensor type data to the key
             if key in self._non_tensordict.keys():
                 if inplace:
                     raise RuntimeError(
                         f"Cannot update an existing entry of type {type(self._non_tensordict.get(key))} with a value of type {type(value)}."
                     )
                 del self._non_tensordict[key]
-            self._tensordict.set(key, value, inplace=inplace)
-        else:
-            # Avoiding key clash, honoring the user input to assign non-tensor data to the key
-            if key in self._tensordict.keys():
-                if inplace:
-                    raise RuntimeError(
-                        f"Cannot update an existing entry of type {type(self._tensordict.get(key))} with a value of type {type(value)}."
+            self._tensordict.set(key, value, inplace=inplace, non_blocking=non_blocking)
+            return self
+        if isinstance(value, dict):
+            type_hints = self._type_hints
+            if type_hints is not None:
+                target_cls = type_hints.get(key, None)
+                if isinstance(target_cls, type) and _is_tensor_collection(target_cls):
+                    value = target_cls.from_dict(value)
+                    self._tensordict.set(
+                        key, value, inplace=inplace, non_blocking=non_blocking
                     )
-                self._tensordict.del_(key)
-            # Saving all non-tensor attributes
-            self._non_tensordict[key] = value
+                    return self
+            else:
+                warnings.warn(self._set_dict_warn_msg)
+
+        # Avoiding key clash, honoring the user input to assign non-tensor data to the key
+        if key in self._tensordict.keys():
+            if inplace:
+                raise RuntimeError(
+                    f"Cannot update an existing entry of type {type(self._tensordict.get(key))} with a value of type {type(value)}."
+                )
+            self._tensordict.del_(key)
+        # Saving all non-tensor attributes
+        self._non_tensordict[key] = value
         return self
 
     if isinstance(key, tuple) and len(key):
         key = _unravel_key_to_tuple(key)
         if len(key) > 1:
             return self.set(key[0], getattr(self, key[0]).set(key[1:], value))
         out = self.set(key[0], value)
@@ -772,18 +1125,20 @@
     elif key[0] in self._non_tensordict.keys():
         self._non_tensordict[key[0]] = None
     else:
         raise KeyError(f"Key {key} could not be found in tensorclass {self}.")
     return
 
 
-def _set_at_(self, key: NestedKey, value: Any, idx: IndexType):
+def _set_at_(
+    self, key: NestedKey, value: Any, idx: IndexType, non_blocking: bool = False
+):
     if key in self._non_tensordict:
         del self._non_tensordict[key]
-    return self._tensordict.set_at_(key, value, idx)
+    return self._tensordict.set_at_(key, value, idx, non_blocking=non_blocking)
 
 
 def _get(self, key: NestedKey, default: Any = NO_DEFAULT):
     """Gets the value stored with the input key.
 
     Args:
         key (str, tuple of str): key to be queried. If tuple of str it is
@@ -834,14 +1189,34 @@
     Args:
         new_size (torch.Size): new_batch size to be set
 
     """
     self._tensordict._batch_size_setter(new_size)
 
 
+def _names(self) -> torch.Size:
+    """Retrieves the dim names for the tensor class.
+
+    Returns:
+        names (list of str)
+
+    """
+    return self._tensordict.names
+
+
+def _names_setter(self, names: str) -> None:  # noqa: D417
+    """Set the value of ``tensorclass.names``.
+
+    Args:
+        names (sequence of str)
+
+    """
+    self._tensordict.names = names
+
+
 def _state_dict(
     self, destination=None, prefix="", keep_vars=False, flatten=False
 ) -> dict[str, Any]:
     """Returns a state_dict dictionary that can be used to save and load data from a tensorclass."""
     state_dict = {
         "_tensordict": self._tensordict.state_dict(
             destination=destination, prefix=prefix, keep_vars=keep_vars, flatten=flatten
@@ -945,14 +1320,18 @@
     """
     if not is_tensor_collection(other) and not isinstance(
         other, (dict, numbers.Number, Tensor, _MemmapTensor)
     ):
         return False
     if is_tensorclass(other):
         tensor = self._tensordict == other._tensordict
+    elif _is_tensor_collection(type(other)):
+        # other can be a tensordict reconstruction of self, in which case we discard
+        # the non-tensor data
+        tensor = self._tensordict == other.exclude(*self._non_tensordict.keys())
     else:
         tensor = self._tensordict == other
     return _from_tensordict_with_none(self, tensor)
 
 
 def _ne(self, other: object) -> bool:
     """Compare the Tensor class object to another object for inequality. However, the equality check for non-tensor data is not performed.
@@ -1002,14 +1381,18 @@
     """
     if not is_tensor_collection(other) and not isinstance(
         other, (dict, numbers.Number, Tensor, _MemmapTensor)
     ):
         return True
     if is_tensorclass(other):
         tensor = self._tensordict != other._tensordict
+    elif _is_tensor_collection(type(other)):
+        # other can be a tensordict reconstruction of self, in which case we discard
+        # the non-tensor data
+        tensor = self._tensordict != other.exclude(*self._non_tensordict.keys())
     else:
         tensor = self._tensordict != other
     return _from_tensordict_with_none(self, tensor)
 
 
 def _or(self, other: object) -> bool:
     """Compares the Tensor class object to another object for logical OR. However, the logical OR check for non-tensor data is not performed.
@@ -1026,14 +1409,18 @@
     """
     if not is_tensor_collection(other) and not isinstance(
         other, (dict, numbers.Number, Tensor, _MemmapTensor)
     ):
         return False
     if is_tensorclass(other):
         tensor = self._tensordict | other._tensordict
+    elif _is_tensor_collection(type(other)):
+        # other can be a tensordict reconstruction of self, in which case we discard
+        # the non-tensor data
+        tensor = self._tensordict | other.exclude(*self._non_tensordict.keys())
     else:
         tensor = self._tensordict | other
     return _from_tensordict_with_none(self, tensor)
 
 
 def _xor(self, other: object) -> bool:
     """Compares the Tensor class object to another object for exclusive OR. However, the exclusive OR check for non-tensor data is not performed.
@@ -1050,19 +1437,27 @@
     """
     if not is_tensor_collection(other) and not isinstance(
         other, (dict, numbers.Number, Tensor, _MemmapTensor)
     ):
         return False
     if is_tensorclass(other):
         tensor = self._tensordict ^ other._tensordict
+    elif _is_tensor_collection(type(other)):
+        # other can be a tensordict reconstruction of self, in which case we discard
+        # the non-tensor data
+        tensor = self._tensordict ^ other.exclude(*self._non_tensordict.keys())
     else:
         tensor = self._tensordict ^ other
     return _from_tensordict_with_none(self, tensor)
 
 
+def _bool(self):
+    raise RuntimeError("Converting a tensorclass to boolean value is not permitted")
+
+
 def _single_td_field_as_str(key, item, tensordict):
     """Returns a string as a  key-value pair of tensordict.
 
     Args:
         key (str): key of tensor dict item
         item (tensor type): value to be returned for key
         tensordict (Tensordict): Tensordict object
@@ -1126,14 +1521,23 @@
 
 ################
 # Custom classes
 # --------------
 
 NONTENSOR_HANDLED_FUNCTIONS = []
 
+_MP_MANAGER = None
+
+
+def _mp_manager():
+    global _MP_MANAGER
+    if _MP_MANAGER is None:
+        _MP_MANAGER = Manager()
+    return _MP_MANAGER
+
 
 @tensorclass
 class NonTensorData:
     """A carrier for non-tensordict data.
 
     This class can be used whenever non-tensor data needs to be carrier at
     any level of a tensordict instance.
@@ -1245,25 +1649,139 @@
                     _tensordict/
                         meta.json
                     meta.json
                 meta.json
             meta.json
         >>> assert loaded.get_non_tensor("pickable").value == 10
 
+    .. note:: __Preallocation__ is also possible with ``NonTensorData``.
+      This class can handle conversion from ``NonTensorData`` to
+      ``NonTensorStack`` where appropriate, as the following example
+      demonstrates:
+
+        >>> td = TensorDict({"val": NonTensorData(data=0, batch_size=[10])}, [10])
+        >>> print(td)
+        TensorDict(
+            fields={
+                val: NonTensorData(
+                    data=0,
+                    _metadata=None,
+                    _is_non_tensor=True,
+                    batch_size=torch.Size([10]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([10]),
+            device=None,
+            is_shared=False)
+        >>> print(td["val"])
+        0
+        >>> newdata = TensorDict({"val": NonTensorData(data=1, batch_size=[5])}, [5])
+        >>> td[1::2] = newdata
+        >>> print(td)
+        TensorDict(
+            fields={
+                val: NonTensorStack(
+                    [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
+                    batch_size=torch.Size([10]),
+                    device=None)},
+            batch_size=torch.Size([10]),
+            device=None,
+            is_shared=False)
+        >>> print(td["val"])  # the stack is automatically converted to a list
+        [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
+
+      If the value is unique, the ``NonTensorData`` container is kept and
+      retrieving the value only returns this value. If a ``NonTensorStack``
+      is used, ``__getitem__`` will return the list of values instead.
+      This makes the two operations not exactly interchangeable. The reason
+      for this inconsistency is that a single ``NonTensorData`` with a non-empty
+      batch-size is intended to be used as a metadata carrier for bigger
+      tensordicts, whereas ``NonTensorStack`` usage is aimed at allocating
+      one metadata atom to each corresponding batch element.
+
+    .. note::
+      ``NonTensorData`` can be shared between processes. In fact, both
+      :meth:`~tensordict.TensorDict.memmap_` (and the likes) and
+      :meth:`~tensordict.TensorDict.share_memory_` will produce sharable
+      instances.
+
+      Valid methods to write data are :meth:`~tensordict.TensorDictBase.update`
+      with the `inplace=True` flag and :meth:`~tensordict.TensorDictBase.update_`
+      or :meth:`~tensordict.TensorDictBase.update_at_`.
+
+        >>> if __name__ == "__main__":
+        ...     td = TensorDict({"val": NonTensorData(data=0, batch_size=[])}, [])
+        ...     td.share_memory_()
+        ...     td.update_(TensorDict({"val": NonTensorData(data=1, batch_size=[])}, []))  # works
+        ...     td.update(TensorDict({"val": NonTensorData(data=1, batch_size=[])}, []), inplace=True)  # works
+        ...     td["val"] = 1  # breaks
+
+      A shared ``NonTensorData`` is writable whenever its content is a ``str``,
+      ``int``, ``float``, ``bool``, ``dict`` or ``list`` instance. Other types
+      (e.g., dataclasses) will not raise an exception during the call to
+      ``memmap_`` or ``share_memory_`` but they will cause the code to break
+      when the data is overwritten.
+
+        >>> @dataclass
+        ... class MyClass:
+        ...     string: str
+        ...
+        >>> if __name__ == "__main__":
+        ...     td = TensorDict({"val": MyClass("a string!")}, [])
+        ...     td.share_memory_()  # works and can be shared between processes
+        ...     td.update_(TensorDict({"val": MyClass("another string!")}, []))  # breaks!
+
+      :class:`~tensordict.tensorclass.TensorStack` instances are also sharable
+      in a similar way. Crucially, preallocation must be properly handled for
+      this to work.
+
+        >>> td = TensorDict({"val": NonTensorData(data=0, batch_size=[10])}, [10])
+        >>> newdata = TensorDict({"val": NonTensorData(data=1, batch_size=[5])}, [5])
+        >>> td[1::2] = newdata
+        >>> # If TD is properly preallocated, we can share it and change its content
+        >>> td.share_memory_()
+        >>> newdata = TensorDict({"val": NonTensorData(data=2, batch_size=[5])}, [5])
+        >>> td[1::2] = newdata  # Works!
+        >>> # In contrast, not preallocating the tensordict properly will break when assigning values
+        >>> td = TensorDict({"val": NonTensorData(data=0, batch_size=[10])}, [10])
+        >>> td.share_memory_()
+        >>> newdata = TensorDict({"val": NonTensorData(data=2, batch_size=[5])}, [5])
+        >>> td[1::2] = newdata  # breaks!
+
+      Writable memmapped-``NonTensorData`` instances will update the underlying
+      metadata if required. This involves writing in a JSON file, which can
+      introduce some overhead. We advise against this usage whenever one seeks
+      performance and long-lasting data sharing isn't required (``share_memory_``
+      should be preferred in these cases).
+
+        >>> if __name__ == "__main__":
+        ...     td = TensorDict({"val": NonTensorData(data=0, batch_size=[])}, [])
+        ...     td.memmap_(dest_folder)
+        ...     td.update_(TensorDict({"val": NonTensorData(data=1, batch_size=[])}, []))
+        ...     # The underlying metadata on disk is updated during calls to update_
+        ...     td_load = TensorDict.load_memmap(dest_folder)
+        ...     assert (td == td_load).all()
+
     """
 
     # Used to carry non-tensor data in a tensordict.
     # The advantage of storing this in a tensorclass is that we don't need
     # to patch tensordict with additional checks that will encur unwanted overhead
     # and all the overhead falls back on this class.
     data: Any
+    _metadata: dict | None = None
+
+    _is_non_tensor: bool = True
 
     def __post_init__(self):
-        if isinstance(self.data, NonTensorData):
-            self.data = self.data.data
+        if is_non_tensor(self.data):
+            data = getattr(self.data, "data", None)
+            if data is None:
+                data = self.data.tolist()
+            self.data = data
 
         old_eq = self.__class__.__eq__
         if old_eq is _eq:
             global NONTENSOR_HANDLED_FUNCTIONS
             NONTENSOR_HANDLED_FUNCTIONS.extend(TD_HANDLED_FUNCTIONS)
 
             # Patch only the first time a class is created
@@ -1310,53 +1828,190 @@
                     return torch.full(
                         self.batch_size, self.data | other.data, device=self.device
                     )
                 return _or(self, other)
 
             self.__class__.__or__ = __or__
 
+    def update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        inplace: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> T:
+        return self._update(
+            input_dict_or_td=input_dict_or_td,
+            clone=clone,
+            inplace=inplace,
+            keys_to_update=keys_to_update,
+        )
+
+    def _update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        inplace: bool = False,
+        *,
+        keys_to_update: Sequence[NestedKey] | None = None,
+        break_on_memmap: bool = None,
+    ) -> T:
+        if isinstance(input_dict_or_td, NonTensorData):
+            data = input_dict_or_td.data
+            if inplace and self._tensordict._is_shared:
+                _update_shared_nontensor(self._non_tensordict["data"], data)
+                return self
+            elif inplace and self._is_memmap:
+                _is_memmaped_from_above = self._is_memmaped_from_above()
+                if break_on_memmap is None:
+                    global _BREAK_ON_MEMMAP
+                    break_on_memmap = _BREAK_ON_MEMMAP
+                if _is_memmaped_from_above and break_on_memmap:
+                    raise RuntimeError(
+                        "Cannot update a leaf NonTensorData from a memmaped parent NonTensorStack. "
+                        "To update this leaf node, please update the NonTensorStack with the proper index."
+                    )
+                share_non_tensor = self._metadata["_share_non_tensor"]
+                if share_non_tensor:
+                    _update_shared_nontensor(self._non_tensordict["data"], data)
+                else:
+                    self._non_tensordict["data"] = data
+                # Force json update by setting is memmap to False
+                if not _is_memmaped_from_above and "memmap_prefix" in self._metadata:
+                    self._tensordict._is_memmap = False
+                    self._memmap_(
+                        prefix=self._metadata["memmap_prefix"],
+                        copy_existing=False,
+                        executor=None,
+                        futures=None,
+                        inplace=True,
+                        like=False,
+                        share_non_tensor=share_non_tensor,
+                    )
+                return self
+            elif not inplace and self.is_locked:
+                raise RuntimeError(_LOCK_ERROR)
+            if clone:
+                data = deepcopy(data)
+            self.data = data
+        elif isinstance(input_dict_or_td, NonTensorStack):
+            raise ValueError(
+                "Cannot update a NonTensorData object with a NonTensorStack. Call `non_tensor_data.maybe_to_stack()` "
+                "before calling update()."
+            )
+        elif not input_dict_or_td.is_empty():
+            raise RuntimeError(f"Unexpected type {type(input_dict_or_td)}")
+        return self
+
+    def maybe_to_stack(self):
+        """Converts the NonTensorData object to a NonTensorStack object if it has a non-empty batch-size."""
+        datalist = self.data
+        if not self.batch_size:
+            return self
+        for i in reversed(self.batch_size):
+            datalist = [datalist] * i
+        return NonTensorStack._from_list(datalist, device=self.device)
+
+    def update_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> T:
+        return self._update_(
+            input_dict_or_td=input_dict_or_td,
+            clone=clone,
+            keys_to_update=keys_to_update,
+        )
+
+    def _update_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        *,
+        keys_to_update: Sequence[NestedKey] | None = None,
+        break_on_memmap: bool = None,
+    ) -> T:
+
+        if isinstance(input_dict_or_td, NonTensorStack):
+            raise RuntimeError(
+                "Cannot update a NonTensorData with a NonTensorStack object."
+            )
+        if not isinstance(input_dict_or_td, NonTensorData):
+            raise RuntimeError(
+                "NonTensorData.copy_ / update_ requires the source to be a NonTensorData object."
+            )
+        return self._update(
+            input_dict_or_td,
+            inplace=True,
+            clone=clone,
+            keys_to_update=keys_to_update,
+            break_on_memmap=break_on_memmap,
+        )
+
+    def update_at_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        index: IndexType,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+    ) -> NonTensorData:
+        if index != () and index != slice(None):
+            raise RuntimeError("Cannot update a part of a NonTensorData.")
+        return self.update_(
+            input_dict_or_td=input_dict_or_td, clone=clone, non_blocking=non_blocking
+        )
+
     def empty(self, recurse=False):
         return NonTensorData(
             data=self.data,
             batch_size=self.batch_size,
             names=self.names if self._has_names() else None,
             device=self.device,
         )
 
     def to_dict(self):
         # override to_dict to return just the data
         return self.data
 
+    def to_tensordict(self):
+        return self
+
     @classmethod
     def _stack_non_tensor(cls, list_of_non_tensor, dim=0):
         # checks have been performed previously, so we're sure the list is non-empty
         first = list_of_non_tensor[0]
 
         def _check_equal(a, b):
             if isinstance(a, _ACCEPTED_CLASSES) or isinstance(b, _ACCEPTED_CLASSES):
                 return (a == b).all()
             try:
                 iseq = a == b
             except Exception:
                 iseq = False
             return iseq
 
-        if all(_check_equal(data.data, first.data) for data in list_of_non_tensor[1:]):
+        if all(isinstance(data, NonTensorData) for data in list_of_non_tensor) and all(
+            _check_equal(data.data, first.data) for data in list_of_non_tensor[1:]
+        ):
             batch_size = list(first.batch_size)
             batch_size.insert(dim, len(list_of_non_tensor))
             return NonTensorData(
                 data=first.data,
                 batch_size=batch_size,
                 names=first.names if first._has_names() else None,
                 device=first.device,
             )
 
-        from tensordict._lazy import LazyStackedTensorDict
-
-        return LazyStackedTensorDict(*list_of_non_tensor, stack_dim=dim)
+        return NonTensorStack(*list_of_non_tensor, stack_dim=dim)
 
     @classmethod
     def __torch_function__(
         cls,
         func: Callable,
         types: tuple[type, ...],
         args: tuple[Any, ...] = (),
@@ -1390,7 +2045,497 @@
             return result.__class__(
                 _from_tensordict_with_copy(tensorclass_instance, tensordict_result)
                 for tensordict_result in result
             )
         if not escape_conversion:
             return _from_tensordict_with_copy(tensorclass_instance, result)
         return result
+
+    def _apply_nest(self, *args, **kwargs):
+        kwargs["filter_empty"] = False
+        return _wrap_method(self, "_apply_nest", self._tensordict._apply_nest)(
+            *args, **kwargs
+        )
+
+    def _fast_apply(self, *args, **kwargs):
+        kwargs["filter_empty"] = False
+        return _wrap_method(self, "_fast_apply", self._tensordict._fast_apply)(
+            *args, **kwargs
+        )
+
+    def tolist(self):
+        """Converts the data in a list if the batch-size is non-empty.
+
+        If the batch-size is empty, returns the data.
+
+        """
+        if not self.batch_size:
+            return self.data
+        return [ntd.tolist() for ntd in self.unbind(0)]
+
+    def copy_(self, src: NonTensorData | NonTensorStack, non_blocking: bool = False):
+        return self.update_(src, non_blocking=non_blocking)
+
+    def clone(self, recurse: bool = True):
+        if recurse:
+            return type(self)(
+                data=deepcopy(self.data),
+                batch_size=self.batch_size,
+                device=self.device,
+                names=self.names,
+            )
+        return type(self)(
+            data=self.data,
+            batch_size=self.batch_size,
+            device=self.device,
+            names=self.names,
+        )
+
+    def share_memory_(self):
+        if self._tensordict._is_shared:
+            return self
+        with self.unlock_():
+            self._non_tensordict["data"] = _share_memory_nontensor(
+                self.data, manager=_mp_manager()
+            )
+        self._tensordict.share_memory_()
+        return self
+
+    def _memmap_(
+        self,
+        *,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+        executor=None,
+        futures=None,
+        inplace=True,
+        like=False,
+        memmaped: bool = False,
+        share_non_tensor: bool = False,
+    ):
+        if self._tensordict._is_memmap:
+            return self
+
+        _metadata = {}
+        if prefix is not None:
+            _metadata = copy(self._metadata)
+            if _metadata is None:
+                _metadata = {}
+            _metadata["memmap_prefix"] = prefix
+            _metadata["memmaped"] = memmaped
+
+        out = _memmap_(
+            self,
+            prefix=prefix,
+            copy_existing=copy_existing,
+            executor=executor,
+            futures=futures,
+            inplace=inplace,
+            like=like,
+            memmaped=memmaped,
+            share_non_tensor=share_non_tensor,
+        )
+        _metadata["_share_non_tensor"] = share_non_tensor
+        out._non_tensordict["_metadata"] = _metadata
+        if share_non_tensor:
+            out._non_tensordict["data"] = _share_memory_nontensor(
+                out.data, manager=_mp_manager()
+            )
+        return out
+
+    def _is_memmaped_from_above(self):
+        _metadata = self._metadata
+        if _metadata is None:
+            return False
+        return _metadata.get("memmaped", False)
+
+    def __repr__(self):
+        return f"{type(self).__name__}(data={self.data}, batch_size={self.batch_size}, device={self.device})"
+
+
+# For __setitem__ and _update_at_ we don't pass a kwarg but use a global variable instead
+_BREAK_ON_MEMMAP = True
+
+
+class NonTensorStack(LazyStackedTensorDict):
+    """A thin wrapper around LazyStackedTensorDict to make stack on non-tensor data easily recognizable.
+
+    A ``NonTensorStack`` is returned whenever :func:`~torch.stack` is called on
+    a list of :class:`~tensordict.NonTensorData` or ``NonTensorStack``.
+
+    Examples:
+        >>> from tensordict import NonTensorData
+        >>> import torch
+        >>> data = torch.stack([
+        ...     torch.stack([NonTensorData(data=(i, j), batch_size=[]) for i in range(2)])
+        ...    for j in range(3)])
+        >>> print(data)
+        NonTensorStack(
+            [[(0, 0), (1, 0)], [(0, 1), (1, 1)], [(0, 2), (1, ...,
+            batch_size=torch.Size([3, 2]),
+            device=None)
+
+    To obtain the values stored in a ``NonTensorStack``, call :class:`~.tolist`.
+
+    """
+
+    _is_non_tensor: bool = True
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        if not all(is_non_tensor(item) for item in self.tensordicts):
+            raise RuntimeError("All tensordicts must be non-tensors.")
+
+    def tolist(self):
+        """Extracts the content of a :class:`tensordict.tensorclass.NonTensorStack` in a nested list.
+
+        Examples:
+            >>> from tensordict import NonTensorData
+            >>> import torch
+            >>> data = torch.stack([
+            ...     torch.stack([NonTensorData(data=(i, j), batch_size=[]) for i in range(2)])
+            ...    for j in range(3)])
+            >>> data.tolist()
+            [[(0, 0), (1, 0)], [(0, 1), (1, 1)], [(0, 2), (1, 2)]]
+
+        """
+        iterator = self.tensordicts if self.stack_dim == 0 else self.unbind(0)
+        return [td.tolist() for td in iterator]
+
+    @classmethod
+    def from_nontensordata(cls, non_tensor: NonTensorData):
+        data = non_tensor.data
+        prev = NonTensorData(data, batch_size=[], device=non_tensor.device)
+        for dim in reversed(non_tensor.shape):
+            prev = cls(*[prev.clone(False) for _ in range(dim)], stack_dim=0)
+        return prev
+
+    def __repr__(self):
+        selfrepr = str(self.tolist())
+        if len(selfrepr) > 50:
+            selfrepr = f"{selfrepr[:50]}..."
+        selfrepr = indent(selfrepr, prefix=4 * " ")
+        batch_size = indent(f"batch_size={self.batch_size}", prefix=4 * " ")
+        device = indent(f"device={self.device}", prefix=4 * " ")
+        return f"NonTensorStack(\n{selfrepr}," f"\n{batch_size}," f"\n{device})"
+
+    @classmethod
+    def lazy_stack(
+        cls,
+        items: Sequence[TensorDictBase],
+        dim: int = 0,
+        *,
+        device: DeviceType | None = None,
+        out: T | None = None,
+        stack_dim_name: str | None = None,
+    ) -> T:
+        result = super().lazy_stack(
+            items=items, dim=dim, out=out, stack_dim_name=stack_dim_name, device=device
+        )
+        if not isinstance(result, cls):
+            raise RuntimeError(
+                f"Unexpected result type: {type(result)} - expected one of {cls}."
+            )
+        return result
+
+    def to_dict(self) -> dict[str, Any]:
+        return self.tolist()
+
+    def to_tensordict(self):
+        return self
+
+    def _memmap_(
+        self,
+        *,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+        executor=None,
+        futures=None,
+        inplace=True,
+        like=False,
+        memmaped: bool = False,
+        share_non_tensor: bool = False,
+    ) -> T:
+
+        memmaped_leaves = memmaped
+        if not memmaped and prefix is not None:
+            memmaped_leaves = True
+
+            def save_metadata(prefix=prefix, self=self):
+                data = self.tolist()
+                device = str(self.device) if self.device is not None else None
+                if not prefix.exists():
+                    os.makedirs(prefix, exist_ok=True)
+                jsondict = {
+                    "_type": str(self.__class__),
+                    "stack_dim": self.stack_dim,
+                    "device": device,
+                }
+                if _is_json_serializable(data):
+                    jsondict["data"] = data
+                else:
+                    jsondict["data"] = "pickle.pkl"
+                    with open(prefix / "pickle.pkl", "wb") as f:
+                        pickle.dump(data, f)
+                with open(prefix / "meta.json", "w") as f:
+                    json.dump(jsondict, f)
+
+            if executor is None:
+                save_metadata()
+            else:
+                futures.append(executor.submit(save_metadata))
+        # The leaves are all non-tensor or non-tensor stacks, and we already saved this on disk
+        # The only thing remaining to do is share the data between processes
+        results = []
+        for i, td in enumerate(self.tensordicts):
+            results.append(
+                td._memmap_(
+                    prefix=(prefix / str(i)) if prefix is not None else None,
+                    copy_existing=copy_existing,
+                    executor=executor,
+                    futures=futures,
+                    inplace=inplace,
+                    like=like,
+                    # tell the nested stack / nontensor that
+                    # no memmapping should be executed
+                    memmaped=memmaped_leaves,
+                    share_non_tensor=share_non_tensor,
+                )
+            )
+        if not inplace:
+            results = self.lazy_stack(results, dim=self.stack_dim)
+        else:
+            results = self
+        if not memmaped and prefix is not None:
+            results.__dict__["_path_to_memmap"] = prefix
+        return results
+
+    @classmethod
+    def _load_memmap(cls, prefix: str, metadata: dict) -> LazyStackedTensorDict:
+        data = metadata.get("data", None)
+        if data is not None:
+            if isinstance(data, str):
+                with open(prefix / data, "rb") as file:
+                    data = pickle.load(file)
+            device = metadata["device"]
+            if device is not None:
+                device = torch.device(device)
+            return cls._from_list(data, device=device)
+        return super()._load_memmap(prefix=prefix, metadata=metadata)
+
+    @classmethod
+    def _from_list(cls, datalist: List, device: torch.device):
+        if all(isinstance(item, list) for item in datalist) and all(
+            len(item) == len(datalist[0]) for item in datalist
+        ):
+            return NonTensorStack(
+                *(cls._from_list(item, device=device) for item in datalist), stack_dim=0
+            )
+        return NonTensorStack(
+            *(
+                NonTensorData(data=item, device=device, batch_size=torch.Size([]))
+                for item in datalist
+            ),
+            stack_dim=0,
+        )
+
+    def update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        inplace: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> T:
+        return self._update(
+            input_dict_or_td=input_dict_or_td,
+            clone=clone,
+            inplace=inplace,
+            keys_to_update=keys_to_update,
+        )
+
+    def update_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> T:
+        return self._update(
+            input_dict_or_td=input_dict_or_td,
+            clone=clone,
+            inplace=True,
+            keys_to_update=keys_to_update,
+        )
+
+    def _update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        inplace: bool = False,
+        *,
+        keys_to_update: Sequence[NestedKey] | None = None,
+        break_on_memmap: bool = None,
+        non_blocking: bool = False,
+    ) -> T:
+        if inplace and self.is_locked and not (self._is_shared or self._is_memmap):
+            raise RuntimeError(_LOCK_ERROR)
+
+        if isinstance(input_dict_or_td, NonTensorData):
+            datalist = input_dict_or_td.data
+            for d in reversed(self.batch_size):
+                datalist = [datalist] * d
+            reconstructed = self._from_list(datalist, device=self.device)
+            return self.update(
+                reconstructed,
+                clone=clone,
+                inplace=inplace,
+                keys_to_update=keys_to_update,
+            )
+
+        memmap = False
+        if self._is_memmap and hasattr(self, "_path_to_memmap"):
+            if break_on_memmap is None:
+                global _BREAK_ON_MEMMAP
+                break_on_memmap = _BREAK_ON_MEMMAP
+            if not break_on_memmap:
+                raise RuntimeError(
+                    "Calling _update with break_on_memmap=False is not permitted if the stack has a path."
+                )
+            # this is the only way break_on_memmap is False
+            break_on_memmap = False
+            # remove memmap
+            if self._path_to_memmap.exists():
+                shutil.rmtree(self._path_to_memmap)
+            memmap = True
+
+        # update content
+        if isinstance(input_dict_or_td, NonTensorStack):
+            for leaf_dest, leaf_src in zip(
+                self.tensordicts, input_dict_or_td.unbind(self.stack_dim)
+            ):
+                leaf_dest._update(
+                    leaf_src,
+                    clone=clone,
+                    inplace=inplace,
+                    keys_to_update=keys_to_update,
+                    break_on_memmap=break_on_memmap,
+                )
+            if memmap:
+                self._memmap_(prefix=self._path_to_memmap, inplace=True)
+        else:
+            raise NotImplementedError(
+                f"The data type {type(input_dict_or_td)} is not supported within {type(self).__name__}.update"
+            )
+        return self
+
+    def __setitem__(self, index, value):
+        memmap = False
+        if self._is_memmap and hasattr(self, "_path_to_memmap"):
+            global _BREAK_ON_MEMMAP
+            _BREAK_ON_MEMMAP = False
+            memmap = True
+        try:
+            super().__setitem__(index, value)
+            if memmap:
+                self._memmap_(prefix=self._path_to_memmap, inplace=True)
+        finally:
+            _BREAK_ON_MEMMAP = True
+
+    def update_at_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        index: IndexType,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+    ) -> T:
+        memmap = False
+        if self._is_memmap and hasattr(self, "_path_to_memmap"):
+            global _BREAK_ON_MEMMAP
+            _BREAK_ON_MEMMAP = False
+            memmap = True
+        try:
+            super().update_at_(
+                input_dict_or_td, index, clone=clone, non_blocking=non_blocking
+            )
+            if memmap:
+                self._memmap_(prefix=self._path_to_memmap, inplace=True)
+        finally:
+            _BREAK_ON_MEMMAP = True
+        return self
+
+
+_register_tensor_class(NonTensorStack)
+
+
+def _share_memory_nontensor(data, manager: Manager):
+    if isinstance(data, int):
+        return mp.Value(ctypes.c_int, data)
+    if isinstance(data, float):
+        return mp.Value(ctypes.c_double, data)
+    if isinstance(data, bool):
+        return mp.Value(ctypes.c_bool, data)
+    if isinstance(data, bytes):
+        return mp.Value(ctypes.c_byte, data)
+    if isinstance(data, dict):
+        result = manager.dict()
+        result.update(data)
+        return result
+    if isinstance(data, str):
+        result = mp.Array(ctypes.c_char, 100)
+        data = data.encode("utf-8")
+        result[: len(data)] = data
+        return result
+    if isinstance(data, list):
+        result = manager.list()
+        result.extend(data)
+        return result
+    # In all other cases, we just return the tensor. It's ok because the content
+    # will be passed to the remote process using regular serialization. We will
+    # lock the update in _update_shared_nontensor though.
+    return data
+
+
+def _from_shared_nontensor(nontensor):
+    if isinstance(nontensor, multiprocessing.managers.ListProxy):
+        return list(nontensor)
+    if isinstance(nontensor, multiprocessing.managers.DictProxy):
+        return dict(nontensor)
+    if isinstance(nontensor, multiprocessing.sharedctypes.Synchronized):
+        return nontensor.value
+    if isinstance(nontensor, multiprocessing.sharedctypes.SynchronizedArray):
+        byte_list = []
+        for byte in nontensor:
+            if byte == b"\x00":
+                break
+            byte_list.append(byte)
+        return b"".join(byte_list).decode("utf-8")
+    return nontensor
+
+
+def _update_shared_nontensor(nontensor, val):
+    if isinstance(nontensor, multiprocessing.managers.ListProxy):
+        nontensor[:] = []
+        nontensor.extend(val)
+    elif isinstance(nontensor, multiprocessing.managers.DictProxy):
+        nontensor.clear()
+        nontensor.update(val)
+    elif isinstance(nontensor, multiprocessing.sharedctypes.Synchronized):
+        nontensor.value = val
+    elif isinstance(nontensor, multiprocessing.sharedctypes.SynchronizedArray):
+        val = val.encode("utf-8")
+        for i, byte in enumerate(nontensor):
+            if i < len(val):
+                v = val[i]
+                nontensor[i] = v
+            elif byte == b"\x00":
+                break
+            else:
+                nontensor[i] = b"\x00"
+        # nontensor[0] = val.encode("utf-8")
+    else:
+        raise NotImplementedError(
+            f"Updating {type(nontensor).__name__} within a shared/memmaped structure is not supported."
+        )
```

## tensordict/utils.py

```diff
@@ -2,15 +2,14 @@
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 from __future__ import annotations
 
 import collections
 import concurrent.futures
-import dataclasses
 import inspect
 import logging
 
 import math
 import os
 
 import sys
@@ -45,25 +44,25 @@
 from tensordict._contextlib import _DecoratorContextManager
 from tensordict._tensordict import (  # noqa: F401
     _unravel_key_to_tuple,
     unravel_key,
     unravel_key_list,
     unravel_keys,
 )
+
 from torch import Tensor
 from torch._C import _disabled_torch_function_impl
 from torch.nn.parameter import (
     _ParameterMeta,
     UninitializedBuffer,
     UninitializedParameter,
     UninitializedTensorMixin,
 )
 from torch.utils.data._utils.worker import _generate_state
 
-
 if TYPE_CHECKING:
     from tensordict.memmap_deprec import MemmapTensor as _MemmapTensor
     from tensordict.tensordict import TensorDictBase
 
 try:
     try:
         from functorch._C import get_unwrapped, is_batchedtensor
@@ -117,14 +116,29 @@
 _KEY_ERROR = 'key "{}" not found in {} with ' "keys {}"
 _LOCK_ERROR = (
     "Cannot modify locked TensorDict. For in-place modification, consider "
     "using the `set_()` method and make sure the key is present."
 )
 
 
+LOGGING_LEVEL = os.environ.get("TD_LOGGING_LEVEL", "DEBUG")
+logger = logging.getLogger("tensordict")
+logger.setLevel(getattr(logging, LOGGING_LEVEL))
+# Disable propagation to the root logger
+logger.propagate = False
+# Remove all attached handlers
+while logger.hasHandlers():
+    logger.removeHandler(logger.handlers[0])
+console_handler = logging.StreamHandler()
+console_handler.setLevel(logging.INFO)
+formatter = logging.Formatter("%(asctime)s [%(name)s][%(levelname)s] %(message)s")
+console_handler.setFormatter(formatter)
+logger.addHandler(console_handler)
+
+
 def _sub_index(tensor: Tensor, idx: IndexType) -> Tensor:
     """Allows indexing of tensors with nested tuples.
 
      >>> sub_tensor1 = tensor[tuple1][tuple2]
      >>> sub_tensor2 = _sub_index(tensor, (tuple1, tuple2))
      >>> assert torch.allclose(sub_tensor1, sub_tensor2)
 
@@ -645,24 +659,41 @@
             raise err
     elif isinstance(tensor, KeyedJaggedTensor):
         return index_keyedjaggedtensor(tensor, index)
     else:
         return tensor[index]
 
 
-def _set_item(tensor: Tensor, index: IndexType, value: Tensor, *, validated) -> Tensor:
+def _set_item(
+    tensor: Tensor, index: IndexType, value: Tensor, *, validated, non_blocking
+) -> Tensor:
     # the tensor must be validated
     if not validated:
         raise RuntimeError
     if isinstance(tensor, Tensor):
         tensor[index] = value
         return tensor
     elif isinstance(tensor, KeyedJaggedTensor):
         tensor = setitem_keyedjaggedtensor(tensor, index, value)
         return tensor
+    from tensordict.tensorclass import NonTensorData, NonTensorStack
+
+    if is_non_tensor(tensor):
+        if (
+            isinstance(value, NonTensorData)
+            and isinstance(tensor, NonTensorData)
+            and tensor.data == value.data
+        ):
+            return tensor
+        elif isinstance(tensor, NonTensorData):
+            tensor = NonTensorStack.from_nontensordata(tensor)
+        if tensor.stack_dim != 0:
+            tensor = NonTensorStack(*tensor.unbind(0), stack_dim=0)
+        tensor[index] = value
+        return tensor
     else:
         tensor[index] = value
         return tensor
 
 
 def _requires_grad(tensor: Tensor) -> bool:
     if isinstance(tensor, Tensor):
@@ -710,15 +741,15 @@
         for name in keys:
             strings = []
             if prefix:
                 strings.append(prefix)
             strings.append(
                 f"{name} took {timeit._REG[name][0] * 1000:4.4} msec (total = {timeit._REG[name][1]} sec)"
             )
-            logging.info(" -- ".join(strings))
+            logger.info(" -- ".join(strings))
 
     @staticmethod
     def erase():
         for k in timeit._REG:
             timeit._REG[k] = [0.0, 0.0, 0]
 
 
@@ -765,20 +796,16 @@
 
 def is_tensorclass(obj: type | Any) -> bool:
     """Returns True if obj is either a tensorclass or an instance of a tensorclass."""
     cls = obj if isinstance(obj, type) else type(obj)
     return _is_tensorclass(cls)
 
 
-def _is_tensorclass(cls) -> bool:
-    return (
-        dataclasses.is_dataclass(cls)
-        and "to_tensordict" in cls.__dict__
-        and "_from_tensordict" in cls.__dict__
-    )
+def _is_tensorclass(cls: type) -> bool:
+    return getattr(cls, "_is_tensorclass", False)
 
 
 class implement_for:
     """A version decorator that checks the version in the environment and implements a function with the fitting one.
 
     If specified module is missing or there is no fitting implementation, call of the decorated function
     will lead to the explicit error.
@@ -1348,16 +1375,25 @@
     if isinstance(actual, LazyStackedTensorDict) and isinstance(
         expected, LazyStackedTensorDict
     ):
         for sub_actual, sub_expected in zip(actual.tensordicts, expected.tensordicts):
             assert_allclose_td(sub_actual, sub_expected, rtol=rtol, atol=atol)
         return True
 
-    set1 = set(actual.keys())
-    set2 = set(expected.keys())
+    try:
+        set1 = set(
+            actual.keys(is_leaf=lambda x: not is_non_tensor(x), leaves_only=True)
+        )
+        set2 = set(
+            expected.keys(is_leaf=lambda x: not is_non_tensor(x), leaves_only=True)
+        )
+    except ValueError:
+        # Persistent tensordicts do not work with is_leaf
+        set1 = set(actual.keys())
+        set2 = set(expected.keys())
     if not (len(set1.difference(set2)) == 0 and len(set2) == len(set1)):
         raise KeyError(
             "actual and expected tensordict keys mismatch, "
             f"keys {(set1 - set2).union(set2 - set1)} appear in one but not "
             f"the other."
         )
     keys = sorted(actual.keys(), key=str)
@@ -1502,17 +1538,15 @@
             [*parent_batch_size, *_shape(tensor)[self_batch_dims:]]
         )
         return out
 
 
 def _set_max_batch_size(source: T, batch_dims=None):
     """Updates a tensordict with its maximium batch size."""
-    from tensordict import NonTensorData
-
-    tensor_data = [val for val in source.values() if not isinstance(val, NonTensorData)]
+    tensor_data = [val for val in source.values() if not is_non_tensor(val)]
 
     for val in tensor_data:
         from tensordict.base import _is_tensor_collection
 
         if _is_tensor_collection(val.__class__):
             _set_max_batch_size(val, batch_dims=batch_dims)
 
@@ -1583,15 +1617,15 @@
 
     return wrapper
 
 
 def _broadcast_tensors(index):
     # tensors and range need to be broadcast
     tensors = {
-        i: tensor if isinstance(tensor, Tensor) else torch.tensor(tensor)
+        i: torch.as_tensor(tensor)
         for i, tensor in enumerate(index)
         if isinstance(tensor, (range, list, np.ndarray, Tensor))
     }
     if tensors:
         shape = torch.broadcast_shapes(*[tensor.shape for tensor in tensors.values()])
         tensors = {i: tensor.expand(shape) for i, tensor in tensors.items()}
         index = tuple(
@@ -1902,25 +1936,25 @@
             for unit in ["B", "KB", "MB", "GB", "TB"]:
                 if size < 1024.0:
                     return f"{size:.2f} {unit}"
                 size /= 1024.0
 
         total_size_bytes = get_directory_size(path)
         formatted_size = format_size(total_size_bytes)
-        logging.info(f"Directory size: {formatted_size}")
+        logger.info(f"Directory size: {formatted_size}")
 
     if os.path.isdir(path):
-        logging.info(indent + os.path.basename(path) + "/")
+        logger.info(indent + os.path.basename(path) + "/")
         indent += "    "
         for item in os.listdir(path):
             print_directory_tree(
                 os.path.join(path, item), indent=indent, display_metadata=False
             )
     else:
-        logging.info(indent + os.path.basename(path))
+        logger.info(indent + os.path.basename(path))
 
 
 def _index_preserve_data_ptr(index):
     if isinstance(index, tuple):
         return all(_index_preserve_data_ptr(idx) for idx in index)
     # we can't use a list comprehension here because it fails with tensor indices
     if index is None or index is Ellipsis:
@@ -1983,7 +2017,16 @@
                 setattr(mod, name, param)
         for key, val in list(mod._forward_pre_hooks.items()):
             if val is self:
                 del mod._forward_pre_hooks[key]
                 return
         else:
             raise RuntimeError("did not find pre-hook")
+
+
+def is_non_tensor(data):
+    """Checks if an item is a non-tensor."""
+    return getattr(type(data), "_is_non_tensor", False)
+
+
+def _is_non_tensor(cls: type):
+    return getattr(cls, "_is_non_tensor", False)
```

## tensordict/version.py

```diff
@@ -1,2 +1,2 @@
-__version__ = '0.3.1'
-git_version = 'bc7836b2b93c327dad2e9ea7b400ba2ccca83fc6'
+__version__ = '0.3.2'
+git_version = 'b4d05e01356712d734f4ec81d247a0d3789e8537'
```

## tensordict/nn/common.py

```diff
@@ -9,26 +9,25 @@
 import inspect
 import warnings
 from textwrap import indent
 from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union
 
 import torch
 from cloudpickle import dumps as cloudpickle_dumps, loads as cloudpickle_loads
+
 from tensordict._td import is_tensor_collection, TensorDictBase
 from tensordict._tensordict import _unravel_key_to_tuple, unravel_key_list
 from tensordict.functional import make_tensordict
-
 from tensordict.nn.functional_modules import (
     _swap_state,
     extract_weights_and_buffers,
     is_functional,
     make_functional,
     repopulate_module,
 )
-
 from tensordict.nn.utils import (
     _auto_make_functional,
     _dispatch_td_nn_modules,
     set_skip_existing,
 )
 from tensordict.utils import implement_for, NestedKey
 from torch import nn, Tensor
@@ -244,15 +243,14 @@
             break
         # if the env variable was used, we can skip the wrapper altogether
         if not _dispatch_td_nn_modules():
             return func
 
         @functools.wraps(func)
         def wrapper(_self, *args: Any, **kwargs: Any) -> Any:
-
             if not _dispatch_td_nn_modules():
                 return func(_self, *args, **kwargs)
 
             source = self.source
             if isinstance(source, str):
                 source = getattr(_self, source)
             tensordict = None
@@ -826,15 +824,19 @@
             >>> params = TensorDict.from_module(module)
             >>> old_params = params.clone(recurse=True)
             >>> module.reset_parameters(params)
             >>> (old_params == params).any()
             False
         """
         if parameters is None:
-            self._reset_parameters(self)
+            any_reset = self._reset_parameters(self)
+            if not any_reset:
+                warnings.warn(
+                    "reset_parameters_recursive was called without the parameters argument and did not find any parameters to reset"
+                )
             return
         elif parameters.ndim:
             raise RuntimeError(
                 "reset_parameters_recursive does not support batched TensorDicts, ensure `batch_size` is empty and the parameters shape match their original shape."
             )
 
         sanitized_parameters = parameters.apply(
@@ -864,21 +866,24 @@
                 )
             return new_parameters
         else:
             with sanitized_parameters.to_module(self):
                 self._reset_parameters(self)
             return sanitized_parameters
 
-    def _reset_parameters(self, module: nn.Module) -> None:
+    def _reset_parameters(self, module: nn.Module) -> bool:
+        any_reset = False
         for child in module.children():
             if isinstance(child, nn.Module):
-                self._reset_parameters(child)
+                any_reset |= self._reset_parameters(child)
 
             if hasattr(child, "reset_parameters"):
                 child.reset_parameters()
+                any_reset |= True
+        return any_reset
 
 
 class TensorDictModule(TensorDictModuleBase):
     """A TensorDictModule, is a python wrapper around a :obj:`nn.Module` that reads and writes to a TensorDict.
 
     Args:
         module (Callable): a callable, typically a :class:`torch.nn.Module`,
```

## tensordict/nn/params.py

```diff
@@ -42,15 +42,19 @@
 
 
 def _apply_leaves(data, fn):
     if isinstance(data, TensorDict):
         with data.unlock_():
             for key, val in list(data.items()):
                 data._set_str(
-                    key, _apply_leaves(val, fn), validated=True, inplace=False
+                    key,
+                    _apply_leaves(val, fn),
+                    validated=True,
+                    inplace=False,
+                    non_blocking=False,
                 )
         return data
     elif isinstance(data, LazyStackedTensorDict):
         # this is currently not implemented as the registration of params will only work
         # with plain TensorDict. The solution will be using pytree to get each independent
         # leaf
         raise RuntimeError(
@@ -399,14 +403,15 @@
     @lock_blocked
     def update(
         self,
         input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
         clone: bool = False,
         inplace: bool = False,
         *,
+        non_blocking: bool = False,
         keys_to_update: Sequence[NestedKey] | None = None,
     ) -> TensorDictBase:
         if not self.no_convert:
             func = _maybe_make_param
         else:
             func = _maybe_make_param_or_buffer
         if isinstance(input_dict_or_td, TensorDictBase):
@@ -416,14 +421,15 @@
         with self._param_td.unlock_():
             TensorDictBase.update(
                 self,
                 input_dict_or_td,
                 clone=clone,
                 inplace=inplace,
                 keys_to_update=keys_to_update,
+                non_blocking=non_blocking,
             )
             self._reset_params()
         return self
 
     @lock_blocked
     @_unlock_and_set
     def pop(self, key: NestedKey, default: Any = NO_DEFAULT) -> CompatibleType:
@@ -460,30 +466,32 @@
         fn: Callable,
         *others: TensorDictBase,
         batch_size: Sequence[int] | None = None,
         device: torch.device | None = None,
         names: Sequence[str] | None = None,
         inplace: bool = False,
         default: Any = NO_DEFAULT,
+        filter_empty: bool | None = None,
         **constructor_kwargs,
-    ) -> TensorDictBase:
+    ) -> TensorDictBase | None:
         ...
 
     @_unlock_and_set(inplace=True)
     def named_apply(
         self,
         fn: Callable,
         *others: TensorDictBase,
         batch_size: Sequence[int] | None = None,
         device: torch.device | None = None,
         names: Sequence[str] | None = None,
         inplace: bool = False,
         default: Any = NO_DEFAULT,
+        filter_empty: bool | None = None,
         **constructor_kwargs,
-    ) -> TensorDictBase:
+    ) -> TensorDictBase | None:
         ...
 
     @_unlock_and_set(inplace=True)
     def _apply_nest(*args, **kwargs):
         ...
 
     @_get_post_hook
@@ -814,14 +822,20 @@
     @_unlock_and_set(inplace=True)
     def _exclude(
         self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
     ) -> TensorDictBase:
         ...
 
     @_carry_over
+    def from_dict_instance(
+        self, input_dict, batch_size=None, device=None, batch_dims=None
+    ):
+        ...
+
+    @_carry_over
     def _legacy_transpose(self, dim0, dim1):
         ...
 
     @_fallback
     def _transpose(self, dim0, dim1):
         ...
 
@@ -911,14 +925,15 @@
         module,
         *,
         inplace: bool = False,
         return_swap: bool = True,
         swap_dest=None,
         memo=None,
         use_state_dict: bool = False,
+        non_blocking: bool = False,
     ):
         ...
 
     @_fallback
     def _view(self, *args, **kwargs):
         ...
 
@@ -1059,31 +1074,33 @@
 
     @_apply_on_data
     def update_(
         self,
         input_dict_or_td: dict[str, CompatibleType] | T,
         clone: bool = False,
         *,
+        non_blocking: bool = False,
         keys_to_update: Sequence[NestedKey] | None = None,
     ) -> T:
         ...
 
     @_apply_on_data
     def update_at_(
         self,
         input_dict_or_td: dict[str, CompatibleType] | T,
         idx: IndexType,
         clone: bool = False,
         *,
+        non_blocking: bool = False,
         keys_to_update: Sequence[NestedKey] | None = None,
     ) -> T:
         ...
 
     @_apply_on_data
-    def apply_(self, fn: Callable, *others) -> T:
+    def apply_(self, fn: Callable, *others, **kwargs) -> T:
         ...
 
     def _apply(self, fn, recurse=True):
         """Modifies torch.nn.Module._apply to work with Buffer class."""
         if recurse:
             for module in self.children():
                 module._apply(fn)
@@ -1111,30 +1128,26 @@
             with torch.no_grad():
                 param_applied = fn(param)
             should_use_set_data = compute_should_use_set_data(param, param_applied)
             if should_use_set_data:
                 param.data = param_applied
                 out_param = param
             else:
-                assert isinstance(param, nn.Parameter)
-                assert param.is_leaf
                 out_param = nn.Parameter(param_applied, param.requires_grad)
                 self._parameters[key] = out_param
 
             if param.grad is not None:
                 with torch.no_grad():
                     grad_applied = fn(param.grad)
                 should_use_set_data = compute_should_use_set_data(
                     param.grad, grad_applied
                 )
                 if should_use_set_data:
-                    assert out_param.grad is not None
                     out_param.grad.data = grad_applied
                 else:
-                    assert param.grad.is_leaf
                     out_param.grad = grad_applied.requires_grad_(
                         param.grad.requires_grad
                     )
 
         for key, buffer in self._buffers.items():
             if buffer is None:
                 continue
@@ -1144,30 +1157,26 @@
             with torch.no_grad():
                 buffer_applied = fn(buffer)
             should_use_set_data = compute_should_use_set_data(buffer, buffer_applied)
             if should_use_set_data:
                 buffer.data = buffer_applied
                 out_buffer = buffer
             else:
-                assert isinstance(buffer, Buffer)
-                assert buffer.is_leaf
                 out_buffer = Buffer(buffer_applied, buffer.requires_grad)
                 self._buffers[key] = out_buffer
 
             if buffer.grad is not None:
                 with torch.no_grad():
                     grad_applied = fn(buffer.grad)
                 should_use_set_data = compute_should_use_set_data(
                     buffer.grad, grad_applied
                 )
                 if should_use_set_data:
-                    assert out_buffer.grad is not None
                     out_buffer.grad.data = grad_applied
                 else:
-                    assert buffer.grad.is_leaf
                     out_buffer.grad = grad_applied.requires_grad_(
                         buffer.grad.requires_grad
                     )
 
         return self
```

## tensordict/nn/utils.py

```diff
@@ -10,15 +10,15 @@
 import os
 from distutils.util import strtobool
 from typing import Any, Callable
 
 import torch
 from torch import nn
 
-AUTO_MAKE_FUNCTIONAL = strtobool(os.environ.get("AUTO_MAKE_FUNCTIONAL", "True"))
+AUTO_MAKE_FUNCTIONAL = strtobool(os.environ.get("AUTO_MAKE_FUNCTIONAL", "False"))
 
 
 DISPATCH_TDNN_MODULES = strtobool(os.environ.get("DISPATCH_TDNN_MODULES", "True"))
 
 __all__ = ["mappings", "inv_softplus", "biased_softplus"]
 
 _SKIP_EXISTING = False
```

## tensordict/prototype/fx.py

```diff
@@ -41,15 +41,15 @@
 
         if tensordict_out is None:
             tensordict_out = tensordict
 
         for out_key, output in zip(self.out_keys, outputs):
             if out_key != "_":
                 tensordict_out._set_tuple(
-                    out_key, output, inplace=False, validated=True
+                    out_key, output, inplace=False, validated=True, non_blocking=False
                 )
 
         return tensordict_out
 
     def __getattr__(self, name: str) -> Any:
         try:
             return super().__getattr__(name)
```

## Comparing `tensordict-0.3.1.dist-info/LICENSE` & `tensordict-0.3.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tensordict-0.3.1.dist-info/METADATA` & `tensordict-0.3.2.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 Metadata-Version: 2.1
 Name: tensordict
-Version: 0.3.1
+Version: 0.3.2
 Summary: UNKNOWN
 Home-page: https://github.com/pytorch/tensordict
 Author: tensordict contributors
 Author-email: vmoens@fb.com
 License: BSD
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Classifier: Development Status :: 4 - Beta
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: torch >=2.2.1
+Requires-Dist: torch ==2.2.2
 Requires-Dist: numpy
 Requires-Dist: cloudpickle
 Provides-Extra: checkpointing
 Requires-Dist: torchsnapshot-nightly ; extra == 'checkpointing'
 Provides-Extra: h5
 Requires-Dist: h5py >=3.8 ; extra == 'h5'
 Provides-Extra: tests
```

### html2text {}

```diff
@@ -1,15 +1,15 @@
-Metadata-Version: 2.1 Name: tensordict Version: 0.3.1 Summary: UNKNOWN Home-
+Metadata-Version: 2.1 Name: tensordict Version: 0.3.2 Summary: UNKNOWN Home-
 page: https://github.com/pytorch/tensordict Author: tensordict contributors
 Author-email: vmoens@fb.com License: BSD Platform: UNKNOWN Classifier:
 Programming Language :: Python :: 3.8 Classifier: Programming Language ::
 Python :: 3.9 Classifier: Programming Language :: Python :: 3.10 Classifier:
 Programming Language :: Python :: 3.11 Classifier: Development Status :: 4 -
 Beta Description-Content-Type: text/markdown License-File: LICENSE Requires-
-Dist: torch >=2.2.1 Requires-Dist: numpy Requires-Dist: cloudpickle Provides-
+Dist: torch ==2.2.2 Requires-Dist: numpy Requires-Dist: cloudpickle Provides-
 Extra: checkpointing Requires-Dist: torchsnapshot-nightly ; extra ==
 'checkpointing' Provides-Extra: h5 Requires-Dist: h5py >=3.8 ; extra == 'h5'
 Provides-Extra: tests Requires-Dist: pytest ; extra == 'tests' Requires-Dist:
 pyyaml ; extra == 'tests' Requires-Dist: pytest-instafail ; extra == 'tests'
 Requires-Dist: pytest-rerunfailures ; extra == 'tests' Requires-Dist: pytest-
 benchmark ; extra == 'tests' [![Docs - GitHub.io](https://img.shields.io/
 static/v1?logo=github&style=flat&color=pink&label=docs&message=tensordict)]
```

## Comparing `tensordict-0.3.1.dist-info/RECORD` & `tensordict-0.3.2.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,38 +1,38 @@
-tensordict/__init__.py,sha256=0vzt63ICPsSQEoEQhDulEefx85UvNY-J5paO_xbq5xk,1613
+tensordict/__init__.py,sha256=wHCnoagVm3PGPXxdlS54wZgtYm5JS5mde8MqHkhwhaQ,1652
 tensordict/_contextlib.py,sha256=yao_SZSgKUJt6dXHtAc5ZFJzAmm_NQQFYgR1rjDg0k8,6156
-tensordict/_lazy.py,sha256=EcS2xmfc7ZeYDoUwUSYXvXzF--cTVbmQGw7b5KspCiE,126860
+tensordict/_lazy.py,sha256=MjG4ouGqNDoIosoQqJLviJ_TNHf7KotS5d_H0axj_Bs,133618
 tensordict/_pytree.py,sha256=2TujGhpwSB2JesznHaaLG4yYnxr_bUliOk_rNnnvjZg,3368
-tensordict/_td.py,sha256=F6YvSmp_VeXnqwVgHsa0cnWRCLkWWi7sdqysYuijULc,118127
-tensordict/_tensordict.pyd,sha256=mw8LgURMlMF99kK9DU9dVA3s4DBTjvRHQw0H1ZrUVuQ,113152
-tensordict/_torch_func.py,sha256=gXxB8dRiM551H47WVLw75u9v3uF_I8OmiDPYHJW70fY,19584
-tensordict/base.py,sha256=OkBtzl0D4Pwd522jGNh1ECoOyGunPEAKR1acRmZo02U,207885
+tensordict/_td.py,sha256=M51192n31VZMzMa2TZxWblAi1DFok4KE2g9eRvQ0BuE,127020
+tensordict/_tensordict.pyd,sha256=wSia0vwzzMkPhV7HOuRhnQTJsuoZNafD769_cvhBewY,113152
+tensordict/_torch_func.py,sha256=TIidBXLZZppFvuunXZXOTXrBGGhHnpo3l66G1MSQDYQ,21200
+tensordict/base.py,sha256=CmXcSXOFlIfxCLq0d8yCG2cRrUrczEB8D_PXuo2dqww,217075
 tensordict/functional.py,sha256=dEDwr-tNEVYIoNpOQ1FoSM1f37rMzipxIxN_Z8JfQh0,16915
 tensordict/memmap.py,sha256=8HEbz9o7FCJoV6nXpxIlZSMffYaVN4wl4H2NDB5BevE,27011
 tensordict/memmap_deprec.py,sha256=nmPXn2GQ3Krg1GAXLoYPpFZNTpZoaBl8DpkwS-H32dk,33049
-tensordict/persistent.py,sha256=ujw0qy67dB19hfIXNuV3JFdDe2gy74vTGnXSE7ck-uU,42659
-tensordict/tensorclass.py,sha256=eCnDS7vVWCm0xn2VSVSO1QbDABgW7eZmpHBYcFcP4cA,50547
+tensordict/persistent.py,sha256=cFodwSgsbUOycJ6Hbq_SebNdtjQj_KMI3GebQuQ9i-8,43972
+tensordict/tensorclass.py,sha256=tuW1KmSYEs_03VEBHZxOtcbvqIdEdttCDLdVAeWhUl0,93206
 tensordict/tensordict.py,sha256=1OhkuiahFu9Ctz4X5GpjXscKNR-uje5CFLYvlko-TC0,1093
-tensordict/utils.py,sha256=l2vaJ1GsPXDpKosYdsR-XmoF0WpJvYS2PerCbHPmk4Q,66595
-tensordict/version.py,sha256=9JKx3O_OEwMOU0IE9BEbOJUGlicRUUyYx4cudpQbGU4,81
+tensordict/utils.py,sha256=TQTBgiADYw6T8Ibp4Lcnx8sxrrclSqEqHUZ0gcNGqTA,68097
+tensordict/version.py,sha256=J1UlCLf5Px5pOzCD1itbKEAD_YXFd7S7NyMC7oDSLw0,81
 tensordict/nn/__init__.py,sha256=nWPo4TqDb1hYILbEc73EHVyv3iy2kumYcoUp1MaXS1g,1634
-tensordict/nn/common.py,sha256=BEtRlDouy8FhkcoiEDeudnjAV7Sh0er_NAs0jWkjknw,54557
+tensordict/nn/common.py,sha256=RqjFAF4bm5_B9BB0m7xxgAVWjIXdxrUq-Y3imiNkBYk,54882
 tensordict/nn/ensemble.py,sha256=WzVLvyLm_D2Ln9QxX2zrZwtP2Jl3N7Oj9UIJbUS1U7I,6020
 tensordict/nn/functional_modules.py,sha256=PVZO3Emvzu_XRjNrjdVVTvgj97GF7eQdZoEgyvlQJO4,25890
-tensordict/nn/params.py,sha256=UsVJ7J1H6HFZAu87AvMRyVDVcjpGw7u7hoi1zO2EfoQ,36547
+tensordict/nn/params.py,sha256=x66aO0K5EDaFx_G82LYSsDIYNY9n3tas1CtI-1aEt7Q,36707
 tensordict/nn/probabilistic.py,sha256=HBiibBQPBBWnV9MWb1k2dJ7WiWaddTZ6CBq3zT4Uv-8,25553
 tensordict/nn/sequence.py,sha256=mD0oJplRLlKflEGSopeSjc9MzdOfH00FM8Tm17_T7Sw,19947
-tensordict/nn/utils.py,sha256=vZDl4WtW9J71Nm3XJA3Wsj6oaSSQaR3IuvnDleAV2VQ,13230
+tensordict/nn/utils.py,sha256=UCK2w6QoOCouDoxP08CvokffxfnwTtWv3Wl5Ls8rgxs,13231
 tensordict/nn/distributions/__init__.py,sha256=kfuBq-yJHh9OkaQHaOJUpvam4KjKLIirE8UbpYF3BuM,795
 tensordict/nn/distributions/composite.py,sha256=bsvnghcdoj2Ak3r3eNgs2GpBxWIV9-z8IIwdZiOM7oM,6629
 tensordict/nn/distributions/continuous.py,sha256=Ge1qivY1uB3sWBVTFD6wKQbBrvU3tAa5GV28aqCs9l4,9924
 tensordict/nn/distributions/discrete.py,sha256=VrcrSPr9YyBDKagOphYTAgpiQqlwfpf4-8t3TsrJyPQ,2667
 tensordict/nn/distributions/truncated_normal.py,sha256=f--2ISj15TTUlLcUzMh1e50yADuh-vKMFbrzLqwUv7I,6694
 tensordict/nn/distributions/utils.py,sha256=3vEDATr12hUk9OYYKf4dzPmNMmLzKHQuZPsdWcPVfi4,1266
 tensordict/prototype/__init__.py,sha256=XBECOVFLLCiUsvNwwByWkz-U87nt7oJXPp2iIIA5Ysk,393
-tensordict/prototype/fx.py,sha256=UnwTvF1nvxXEfjLNc1WWs5I9r0Hyu9hielLfB9H7SLM,7869
+tensordict/prototype/fx.py,sha256=AD6zDHD2g24iuP9v-yzWN_NX1xInK5E8RHvkend0amc,7889
 tensordict/prototype/tensorclass.py,sha256=bzbSJ7uaQVxfUS6uzwyh5SOBrhufxsW_dLzjMbQJLvw,796
-tensordict-0.3.1.dist-info/LICENSE,sha256=PGO-oZsq4EzhE1-WQS2xGiEF3UCVb9YawfQ09cIMV_8,1119
-tensordict-0.3.1.dist-info/METADATA,sha256=0Q_rVqwU4Y5HIjpi-6Eq4wmGZ3dqxBZhFDLwR-8DpRk,19303
-tensordict-0.3.1.dist-info/WHEEL,sha256=GZFS91_ufm4WrNPBaFVPB9MvOXR6bMZQhPcZRRTN5YM,100
-tensordict-0.3.1.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
-tensordict-0.3.1.dist-info/RECORD,,
+tensordict-0.3.2.dist-info/LICENSE,sha256=PGO-oZsq4EzhE1-WQS2xGiEF3UCVb9YawfQ09cIMV_8,1119
+tensordict-0.3.2.dist-info/METADATA,sha256=VFUQYKUWrQ1-PPEG4D7gXOO1ltaXBlK-UrAOAOupHT4,19303
+tensordict-0.3.2.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
+tensordict-0.3.2.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
+tensordict-0.3.2.dist-info/RECORD,,
```

